Advanced Textbooks in Control and Signal Processing

Series Editors
Professor Michael J. Grimble, Professor of Industrial Systems and Director
Professor Michael A. Johnson, Professor Emeritus of Control Systems and Deputy Director
Industrial Control Centre, Department of Electronic and Electrical Engineering,
University of Strathclyde, Graham Hills Building, 50 George Street, Glasgow G1 1QE, UK

Other titles published in this series:
Genetic Algorithms
K.F. Man, K.S. Tang and S. Kwong
Introduction to Optimal Estimation
E.W. Kamen and J.K. Su
Discrete-time Signal Processing
D. Williamson
Neural Networks for Modelling and
Control of Dynamic Systems
M. Nørgaard, O. Ravn, N.K. Poulsen
and L.K. Hansen
Fault Detection and Diagnosis in
Industrial Systems
L.H. Chiang, E.L. Russell and R.D. Braatz
Soft Computing
L. Fortuna, G. Rizzotto, M. Lavorgna,
G. Nunnari, M.G. Xibilia and R. Caponetto
Statistical Signal Processing
T. Chonavel
Discrete-time Stochastic Processes
(2nd Edition)
T. Söderström
Parallel Computing for Real-time Signal
Processing and Control
M.O. Tokhi, M.A. Hossain and
M.H. Shaheed
Multivariable Control Systems
P. Albertos and A. Sala

Control Systems with Input and Output
Constraints
A.H. Glattfelder and W. Schaufelberger
Analysis and Control of Non-linear
Process Systems
K.M. Hangos, J. Bokor and
G. Szederkényi
Model Predictive Control (2nd Edition)
E.F. Camacho and C. Bordons
Principles of Adaptive Filters and Selflearning Systems
A. Zaknich
Digital Self-tuning Controllers
V. Bobál, J. Böhm, J. Fessl and
J. Macháček
Control of Robot Manipulators in
Joint Space
R. Kelly, V. Santibáñez and A. Loría
Receding Horizon Control
W.H. Kwon and S. Han
Robust Control Design with MATLAB®
D.-W. Gu, P.H. Petkov and
M.M. Konstantinov
Control of Dead-time Processes
J.E. Normey-Rico and E.F. Camacho
Modeling and Control of Discrete-event
Dynamic Systems
B. Hrúz and M.C. Zhou

Bruno Siciliano • Lorenzo Sciavicco
Luigi Villani • Giuseppe Oriolo

Robotics
Modelling, Planning and Control

123

Bruno Siciliano, PhD
Dipartimento di Informatica e Sistemistica
Università di Napoli Federico II
Via Claudio 21
80125 Napoli
Italy

Lorenzo Sciavicco, DrEng
Dipartimento di Informatica e Automazione
Università di Roma Tre
Via della Vasca Navale 79
00146 Roma
Italy

Luigi Villani, PhD
Dipartimento di Informatica e Sistemistica
Università di Napoli Federico II
Via Claudio 21
80125 Napoli
Italy

Giuseppe Oriolo, PhD
Dipartimento di Informatica e Sistemistica
Università di Roma “La Sapienza”
Via Ariosto 25
00185 Roma
Italy

ISBN 978-1-84628-641-4

e-ISBN 978-1-84628-642-1

DOI 10.1007/978-1-84628-642-1
Advanced Textbooks in Control and Signal Processing series ISSN 1439-2232
A catalogue record for this book is available from the British Library
Library of Congress Control Number: 2008939574
© 2009 Springer-Verlag London Limited
MATLAB® is a registered trademark of The MathWorks, Inc., 3 Apple Hill Drive, Natick, MA 017602098, USA. http://www.mathworks.com
Apart from any fair dealing for the purposes of research or private study, or criticism or review, as
permitted under the Copyright, Designs and Patents Act 1988, this publication may only be
reproduced, stored or transmitted, in any form or by any means, with the prior permission in writing of
the publishers, or in the case of reprographic reproduction in accordance with the terms of licences
issued by the Copyright Licensing Agency. Enquiries concerning reproduction outside those terms
should be sent to the publishers.
The use of registered names, trademarks, etc. in this publication does not imply, even in the absence of
a specific statement, that such names are exempt from the relevant laws and regulations and therefore
free for general use.
The publisher makes no representation, express or implied, with regard to the accuracy of the
information contained in this book and cannot accept any legal responsibility or liability for any errors
or omissions that may be made.
Cover design: eStudio Calamar S.L., Girona, Spain
Printed on acid-free paper
9 8 7 6 5 4 3 2 1
springer.com

to our families

Series Editors’ Foreword

The topics of control engineering and signal processing continue to ﬂourish and
develop. In common with general scientiﬁc investigation, new ideas, concepts
and interpretations emerge quite spontaneously and these are then discussed,
used, discarded or subsumed into the prevailing subject paradigm. Sometimes
these innovative concepts coalesce into a new sub-discipline within the broad
subject tapestry of control and signal processing. This preliminary battle between old and new usually takes place at conferences, through the Internet and
in the journals of the discipline. After a little more maturity has been acquired
by the new concepts then archival publication as a scientiﬁc or engineering
monograph may occur.
A new concept in control and signal processing is known to have arrived
when suﬃcient material has evolved for the topic to be taught as a specialised
tutorial workshop or as a course to undergraduate, graduate or industrial
engineers. Advanced Textbooks in Control and Signal Processing are designed
as a vehicle for the systematic presentation of course material for both popular
and innovative topics in the discipline. It is hoped that prospective authors will
welcome the opportunity to publish a structured and systematic presentation
of some of the newer emerging control and signal processing technologies in
the textbook series.
Robots have appeared extensively in the artistic ﬁeld of science ﬁction
writing. The actual name robot arose from its use by the playwright Karel
Čapek in the play Rossum’s Universal Robots (1920). Not surprisingly, the
artistic focus has been on mechanical bipeds with anthropomorphic personalities often termed androids. This focus has been the theme of such cinematic productions as, I, Robot (based on Isaac Asimov’s stories) and Stanley
Kubrick’s ﬁlm, A.I.; however, this book demonstrates that robot technology
is already widely used in industry and that there is some robot technology
which is at prototype stage rapidly approaching introduction to commercial
use. Currently, robots may be classiﬁed according to their mobility attributes
as shown in the ﬁgure.

viii

Series Editors’ Foreword

The largest class of robots extant today is that of the ﬁxed robot which
does repetitive but often precise mechanical and physical tasks. These robots
pervade many areas of modern industrial automation and are mainly concerned with tasks performed in a structured environment. It seems highly
likely that as the technology develops the number of mobile robots will significantly increase and become far more visible as more applications and tasks
in an unstructured environment are serviced by robotic technology.
What then is robotics? A succinct deﬁnition is given in The Chamber’s Dictionary (2003): the branch of technology dealing with the design, construction
and use of robots. This deﬁnition certainly captures the spirit of this volume
in the Advanced Textbooks in Control and Signal Processing series entitled
Robotics and written by Bruno Siciliano, Lorenzo Sciavicco, Luigi Villani and
Giuseppe Oriolo. This book is a greatly extended and revised version of an
earlier book in the series, Modelling and Control of Robot Manipulators (2000,
ISBN: 978-1-85233-221-1). As can be seen from the ﬁgure above, robots cover
a wide variety of types and the new book seeks to present a uniﬁed approach
to robotics whilst focusing on the two leading classes of robots, the ﬁxed and
the wheeled types. The textbook series publishes volumes in support of new
disciplines that are emerging with their own novel identity, and robotics as
a subject certainly falls into this category. The full scope of robotics lies at
the intersection of mechanics, electronics, signal processing, control engineering, computing and mathematical modelling. However, within this very broad
framework the authors have pursued the themes of modelling, planning and
control . These are, and will remain, fundamental aspects of robot design and
operation for years to come. Some interesting innovations in this text include
material on wheeled robots and on vision as used in the control of robots.
Thus, the book provides a thorough theoretical grounding in an area where
the technologies are evolving and developing in new applications.
The series is one of textbooks for advanced courses, and volumes in the
series have useful pedagogical features. This volume has twelve chapters covering both fundamental and specialist topics, and there is a Problems section
at the end of each chapter. Five appendices have been included to give more
depth to some of the advanced methods used in the text. There are over twelve
pages of references and nine pages of index. The details of the citations and
index should also facilitate the use of the volume as a source of reference as

Series Editors’ Foreword

ix

well as a course study text. We expect that the student, the researcher, the
lecturer and the engineer will ﬁnd this volume of great value for the study of
robotics.

Glasgow
August 2008

Michael J. Grimble
Michael A. Johnson

Preface

In the last 25 years, the ﬁeld of robotics has stimulated an increasing interest
in a wide number of scholars, and thus literature has been conspicuous, both
in terms of textbooks and monographs, and in terms of specialized journals
dedicated to robotics. This strong interest is also to be attributed to the interdisciplinary character of robotics, which is a science having roots in diﬀerent
areas. Cybernetics, mechanics, controls, computers, bioengineering, electronics — to mention the most important ones — are all cultural domains which
undoubtedly have boosted the development of this science.
Despite robotics representing as yet a relatively young discipline, its foundations are to be considered well-assessed in the classical textbook literature.
Among these, modelling, planning and control play a basic role, not only in the
traditional context of industrial robotics, but also for the advanced scenarios
of ﬁeld and service robots, which have attracted an increasing interest from
the research community in the last 15 years.
This book is the natural evolution of the previous text Modelling and Control of Robot Manipulators by the ﬁrst two co-authors, published in 1995, and
in 2000 with its second edition. The cut of the original textbook has been
conﬁrmed with the educational goal of blending the fundamental and technological aspects with those advanced aspects, on a uniform track as regards a
rigorous formalism.
The fundamental and technological aspects are mainly concentrated in the
ﬁrst six chapters of the book and concern the theory of manipulator structures,
including kinematics, statics and trajectory planning, and the technology of
robot actuators, sensors and control units.
The advanced aspects are dealt with in the subsequent six chapters and
concern dynamics and motion control of robot manipulators, interaction with
the environment using exteroceptive sensory data (force and vision), mobile
robots and motion planning.
The book contents are organized in 12 chapters and 5 appendices.
In Chap. 1, the diﬀerences between industrial and advanced applications
are enlightened in the general robotics context. The most common mechanical

xii

Preface

structures of robot manipulators and wheeled mobile robots are presented.
Topics are also introduced which are developed in the subsequent chapters.
In Chap. 2 kinematics is presented with a systematic and general approach
which refers to the Denavit-Hartenberg convention. The direct kinematics
equation is formulated which relates joint space variables to operational space
variables. This equation is utilized to ﬁnd manipulator workspace as well as
to derive a kinematic calibration technique. The inverse kinematics problem
is also analyzed and closed-form solutions are found for typical manipulation
structures.
Diﬀerential kinematics is presented in Chap. 3. The relationship between
joint velocities and end-eﬀector linear and angular velocities is described by
the geometric Jacobian. The diﬀerence between the geometric Jacobian and
the analytical Jacobian is pointed out. The Jacobian constitutes a fundamental tool to characterize a manipulator, since it allows the determination of
singular conﬁgurations, an analysis of redundancy and the expression of the
relationship between forces and moments applied to the end-eﬀector and the
resulting joint torques at equilibrium conﬁgurations (statics). Moreover, the
Jacobian allows the formulation of inverse kinematics algorithms that solve
the inverse kinematics problem even for manipulators not having a closed-form
solution.
In Chap. 4, trajectory planning techniques are illustrated which deal with
the computation of interpolating polynomials through a sequence of desired
points. Both the case of point-to-point motion and that of motion through
a sequence of points are treated. Techniques are developed for generating
trajectories both in the joint space and in the operational space, with a special
concern to orientation for the latter.
Chapter 5 is devoted to the presentation of actuators and sensors. After an
illustration of the general features of an actuating system, methods to control
electric and hydraulic drives are presented. The most common proprioceptive
and exteroceptive sensors in robotics are described.
In Chap. 6, the functional architecture of a robot control system is illustrated. The characteristics of programming environments are presented with
an emphasis on teaching-by-showing and robot-oriented programming. A general model for the hardware architecture of an industrial robot control system
is ﬁnally discussed.
Chapter 7 deals with the derivation of manipulator dynamics, which plays
a fundamental role in motion simulation, manipulation structure analysis and
control algorithm synthesis. The dynamic model is obtained by explicitly taking into account the presence of actuators. Two approaches are considered,
namely, one based on Lagrange formulation, and the other based on Newton–
Euler formulation. The former is conceptually simpler and systematic, whereas
the latter allows computation of a dynamic model in a recursive form. Notable
properties of the dynamic model are presented, including linearity in the parameters which is utilized to develop a model identiﬁcation technique. Finally,

Preface

xiii

the transformations needed to express the dynamic model in the operational
space are illustrated.
In Chap. 8 the problem of motion control in free space is treated. The
distinction between joint space decentralized and centralized control strategies
is pointed out. With reference to the former, the independent joint control
technique is presented which is typically used for industrial robot control.
As a premise to centralized control, the computed torque feedforward control
technique is introduced. Advanced schemes are then introduced including PD
control with gravity compensation, inverse dynamics control, robust control,
and adaptive control. Centralized techniques are extended to operational space
control.
Force control of a manipulator in contact with the working environment
is tackled in Chap. 9. The concepts of mechanical compliance and impedance
are deﬁned as a natural extension of operational space control schemes to the
constrained motion case. Force control schemes are then presented which are
obtained by the addition of an outer force feedback loop to a motion control
scheme. The hybrid force/motion control strategy is ﬁnally presented with
reference to the formulation of natural and artiﬁcial constraints describing an
interaction task.
In Chap. 10, visual control is introduced which allows the use of information on the environment surrounding the robotic system. The problems of
camera position and orientation estimate with respect to the objects in the
scene are solved by resorting to both analytical and numerical techniques.
After presenting the advantages to be gained with stereo vision and a suitable camera calibration, the two main visual control strategies are illustrated,
namely in the operational space and in the image space, whose advantages can
be eﬀectively combined in the hybrid visual control scheme.
Wheeled mobile robots are dealt with in Chap. 11, which extends some
modelling, planning and control aspects of the previous chapters. As far
as modelling is concerned, it is worth distinguishing between the kinematic
model, strongly characterized by the type of constraint imposed by wheel
rolling, and the dynamic model which accounts for the forces acting on the
robot. The peculiar structure of the kinematic model is keenly exploited to
develop both path and trajectory planning techniques. The control problem
is tackled with reference to two main motion tasks: trajectory tracking and
conﬁguration regulation. Further, it is evidenced how the implementation of
the control schemes utilizes odometric localization methods.
Chapter 12 reprises the planning problems treated in Chaps. 4 and 11
for robot manipulators and mobile robots respectively, in the case when obstacles are present in the workspace. In this framework, motion planning is
referred to, which is eﬀectively formulated in the conﬁguration space. Several
planning techniques for mobile robots are then presented: retraction, cell decomposition, probabilistic, artiﬁcial potential. The extension to the case of
robot manipulators is ﬁnally discussed.

xiv

Preface

This chapter concludes the presentation of the topical contents of the textbook; ﬁve appendices follow which have been included to recall background
methodologies.
Appendix A is devoted to linear algebra and presents the fundamental
notions on matrices, vectors and related operations.
Appendix B presents those basic concepts of rigid body mechanics which
are preliminary to the study of manipulator kinematics, statics and dynamics.
Appendix C illustrates the principles of feedback control of linear systems
and presents a general method based on Lyapunov theory for control of nonlinear systems.
Appendix D deals with some concepts of diﬀerential geometry needed for
control of mechanical systems subject to nonholonomic constraints.
Appendix E is focused on graph search algorithms and their complexity in
view of application to motion planning methods.
The organization of the contents according to the above illustrated scheme
allows the adoption of the book as a reference text for a senior undergraduate or graduate course in automation, computer, electrical, electronics, or
mechanical engineering with strong robotics content.
From a pedagogical viewpoint, the various topics are presented in an instrumental manner and are developed with a gradually increasing level of diﬃculty. Problems are raised and proper tools are established to ﬁnd engineeringoriented solutions. Each chapter is introduced by a brief preamble providing
the rationale and the objectives of the subject matter. The topics needed for a
proﬁcient study of the text are presented in the ﬁve appendices, whose purpose
is to provide students of diﬀerent extraction with a homogeneous background.
The book contains more than 310 illustrations and more than 60 workedout examples and case studies spread throughout the text with frequent resort
to simulation. The results of computer implementations of inverse kinematics algorithms, trajectory planning techniques, inverse dynamics computation,
motion, force and visual control algorithms for robot manipulators, and motion control for mobile robots are presented in considerable detail in order to
facilitate the comprehension of the theoretical development, as well as to increase sensitivity of application in practical problems. In addition, nearly 150
end-of-chapter problems are proposed, some of which contain further study
matter of the contents, and the book is accompanied by an electronic solutions manual (downloadable from www.springer.com/978-1-84628-641-4)
R
code for computer problems; this is available free
containing the MATLAB
of charge to those adopting this volume as a text for courses. Special care has
been devoted to the selection of bibliographical references (more than 250)
which are cited at the end of each chapter in relation to the historical development of the ﬁeld.
Finally, the authors wish to acknowledge all those who have been helpful
in the preparation of this book.
With reference to the original work, as the basis of the present textbook,
devoted thanks go to Pasquale Chiacchio and Stefano Chiaverini for their

Preface

xv

contributions to the writing of the chapters on trajectory planning and force
control, respectively. Fabrizio Caccavale and Ciro Natale have been of great
help in the revision of the contents for the second edition.
A special note of thanks goes to Alessandro De Luca for his punctual and
critical reading of large portions of the text, as well as to Vincenzo Lippiello,
Agostino De Santis, Marilena Vendittelli and Luigi Freda for their contributions and comments on some sections.

Naples and Rome
July 2008

Bruno Siciliano
Lorenzo Sciavicco
Luigi Villani
Giuseppe Oriolo

Contents

1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1
Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2
Robot Mechanical Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.1 Robot Manipulators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.2 Mobile Robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3
Industrial Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4
Advanced Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.1 Field Robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.2 Service Robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5
Robot Modelling, Planning and Control . . . . . . . . . . . . . . . . . . .
1.5.1 Modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.2 Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.3 Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1
1
3
4
10
15
25
26
27
29
30
32
32
33

2

Kinematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1
Pose of a Rigid Body . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2
Rotation Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 Elementary Rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 Representation of a Vector . . . . . . . . . . . . . . . . . . . . . . . .
2.2.3 Rotation of a Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3
Composition of Rotation Matrices . . . . . . . . . . . . . . . . . . . . . . . .
2.4
Euler Angles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.1 ZYZ Angles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.2 RPY Angles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5
Angle and Axis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.6
Unit Quaternion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.7
Homogeneous Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.8
Direct Kinematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.8.1 Open Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.8.2 Denavit–Hartenberg Convention . . . . . . . . . . . . . . . . . . .

39
39
40
41
42
44
45
48
49
51
52
54
56
58
60
61

xviii

Contents

2.9

2.10

2.11
2.12

3

2.8.3 Closed Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
Kinematics of Typical Manipulator Structures . . . . . . . . . . . . . 68
2.9.1 Three-link Planar Arm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
2.9.2 Parallelogram Arm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
2.9.3 Spherical Arm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
2.9.4 Anthropomorphic Arm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
2.9.5 Spherical Wrist . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
2.9.6 Stanford Manipulator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
2.9.7 Anthropomorphic Arm with Spherical Wrist . . . . . . . . . 77
2.9.8 DLR Manipulator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
2.9.9 Humanoid Manipulator . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Joint Space and Operational Space . . . . . . . . . . . . . . . . . . . . . . . 83
2.10.1 Workspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
2.10.2 Kinematic Redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
Kinematic Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Inverse Kinematics Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
2.12.1 Solution of Three-link Planar Arm . . . . . . . . . . . . . . . . . 91
2.12.2 Solution of Manipulators with Spherical Wrist . . . . . . . 94
2.12.3 Solution of Spherical Arm . . . . . . . . . . . . . . . . . . . . . . . . . 95
2.12.4 Solution of Anthropomorphic Arm . . . . . . . . . . . . . . . . . 96
2.12.5 Solution of Spherical Wrist . . . . . . . . . . . . . . . . . . . . . . . . 99
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100

Diﬀerential Kinematics and Statics . . . . . . . . . . . . . . . . . . . . . . . . 105
3.1
Geometric Jacobian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
3.1.1 Derivative of a Rotation Matrix . . . . . . . . . . . . . . . . . . . . 106
3.1.2 Link Velocities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
3.1.3 Jacobian Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
3.2
Jacobian of Typical Manipulator Structures . . . . . . . . . . . . . . . 113
3.2.1 Three-link Planar Arm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
3.2.2 Anthropomorphic Arm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
3.2.3 Stanford Manipulator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
3.3
Kinematic Singularities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
3.3.1 Singularity Decoupling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
3.3.2 Wrist Singularities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
3.3.3 Arm Singularities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
3.4
Analysis of Redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
3.5
Inverse Diﬀerential Kinematics . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
3.5.1 Redundant Manipulators . . . . . . . . . . . . . . . . . . . . . . . . . . 124
3.5.2 Kinematic Singularities . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
3.6
Analytical Jacobian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
3.7
Inverse Kinematics Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
3.7.1 Jacobian (Pseudo-)inverse . . . . . . . . . . . . . . . . . . . . . . . . . 133
3.7.2 Jacobian Transpose . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134

Contents

3.8

3.9

xix

3.7.3 Orientation Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
3.7.4 Second-order Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 141
3.7.5 Comparison Among Inverse Kinematics Algorithms . . . 143
Statics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
3.8.1 Kineto-Statics Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
3.8.2 Velocity and Force Transformation . . . . . . . . . . . . . . . . . 149
3.8.3 Closed Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
Manipulability Ellipsoids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159

4

Trajectory Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
4.1
Path and Trajectory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
4.2
Joint Space Trajectories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
4.2.1 Point-to-Point Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
4.2.2 Motion Through a Sequence of Points . . . . . . . . . . . . . . 168
4.3
Operational Space Trajectories . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
4.3.1 Path Primitives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
4.3.2 Position . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
4.3.3 Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189

5

Actuators and Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
5.1
Joint Actuating System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
5.1.1 Transmissions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
5.1.2 Servomotors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
5.1.3 Power Ampliﬁers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
5.1.4 Power Supply . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
5.2
Drives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
5.2.1 Electric Drives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
5.2.2 Hydraulic Drives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
5.2.3 Transmission Eﬀects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
5.2.4 Position Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
5.3
Proprioceptive Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
5.3.1 Position Transducers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
5.3.2 Velocity Transducers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
5.4
Exteroceptive Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
5.4.1 Force Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
5.4.2 Range Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
5.4.3 Vision Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230

xx

Contents

6

Control Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
6.1
Functional Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
6.2
Programming Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
6.2.1 Teaching-by-Showing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
6.2.2 Robot-oriented Programming . . . . . . . . . . . . . . . . . . . . . . 241
6.3
Hardware Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245

7

Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
7.1
Lagrange Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
7.1.1 Computation of Kinetic Energy . . . . . . . . . . . . . . . . . . . . 249
7.1.2 Computation of Potential Energy . . . . . . . . . . . . . . . . . . 255
7.1.3 Equations of Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
7.2
Notable Properties of Dynamic Model . . . . . . . . . . . . . . . . . . . . 257
7.2.1 Skew-symmetry of Matrix Ḃ − 2C . . . . . . . . . . . . . . . . . 257
7.2.2 Linearity in the Dynamic Parameters . . . . . . . . . . . . . . . 259
7.3
Dynamic Model of Simple Manipulator Structures . . . . . . . . . . 264
7.3.1 Two-link Cartesian Arm . . . . . . . . . . . . . . . . . . . . . . . . . . 264
7.3.2 Two-link Planar Arm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
7.3.3 Parallelogram Arm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
7.4
Dynamic Parameter Identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . 280
7.5
Newton–Euler Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
7.5.1 Link Accelerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
7.5.2 Recursive Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
7.5.3 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
7.6
Direct Dynamics and Inverse Dynamics . . . . . . . . . . . . . . . . . . . 292
7.7
Dynamic Scaling of Trajectories . . . . . . . . . . . . . . . . . . . . . . . . . . 294
7.8
Operational Space Dynamic Model . . . . . . . . . . . . . . . . . . . . . . . 296
7.9
Dynamic Manipulability Ellipsoid . . . . . . . . . . . . . . . . . . . . . . . . 299
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301

8

Motion Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
8.1
The Control Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
8.2
Joint Space Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
8.3
Decentralized Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
8.3.1 Independent Joint Control . . . . . . . . . . . . . . . . . . . . . . . . 311
8.3.2 Decentralized Feedforward Compensation . . . . . . . . . . . 319
8.4
Computed Torque Feedforward Control . . . . . . . . . . . . . . . . . . . 324
8.5
Centralized Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
8.5.1 PD Control with Gravity Compensation . . . . . . . . . . . . 328
8.5.2 Inverse Dynamics Control . . . . . . . . . . . . . . . . . . . . . . . . . 330
8.5.3 Robust Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
8.5.4 Adaptive Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338

Contents

8.6

8.7

9

xxi

Operational Space Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
8.6.1 General Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
8.6.2 PD Control with Gravity Compensation . . . . . . . . . . . . 345
8.6.3 Inverse Dynamics Control . . . . . . . . . . . . . . . . . . . . . . . . . 347
Comparison Among Various Control Schemes . . . . . . . . . . . . . . 349
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360

Force Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
9.1
Manipulator Interaction with Environment . . . . . . . . . . . . . . . . 363
9.2
Compliance Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
9.2.1 Passive Compliance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
9.2.2 Active Compliance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
9.3
Impedance Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
9.4
Force Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
9.4.1 Force Control with Inner Position Loop . . . . . . . . . . . . . 379
9.4.2 Force Control with Inner Velocity Loop . . . . . . . . . . . . . 380
9.4.3 Parallel Force/Position Control . . . . . . . . . . . . . . . . . . . . 381
9.5
Constrained Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
9.5.1 Rigid Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
9.5.2 Compliant Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . 389
9.6
Natural and Artiﬁcial Constraints . . . . . . . . . . . . . . . . . . . . . . . . 391
9.6.1 Analysis of Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
9.7
Hybrid Force/Motion Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
9.7.1 Compliant Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
9.7.2 Rigid Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404

10 Visual Servoing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
10.1 Vision for Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
10.1.1 Conﬁguration of the Visual System . . . . . . . . . . . . . . . . . 409
10.2 Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410
10.2.1 Image Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
10.2.2 Image Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416
10.3 Pose Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
10.3.1 Analytic Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
10.3.2 Interaction Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424
10.3.3 Algorithmic Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
10.4 Stereo Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
10.4.1 Epipolar Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
10.4.2 Triangulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
10.4.3 Absolute Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
10.4.4 3D Reconstruction from Planar Homography . . . . . . . . 438
10.5 Camera Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440

xxii

Contents

10.6
10.7

The Visual Servoing Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
Position-based Visual Servoing . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
10.7.1 PD Control with Gravity Compensation . . . . . . . . . . . . 446
10.7.2 Resolved-velocity Control . . . . . . . . . . . . . . . . . . . . . . . . . 447
10.8 Image-based Visual Servoing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
10.8.1 PD Control with Gravity Compensation . . . . . . . . . . . . 449
10.8.2 Resolved-velocity Control . . . . . . . . . . . . . . . . . . . . . . . . . 451
10.9 Comparison Among Various Control Schemes . . . . . . . . . . . . . . 453
10.10 Hybrid Visual Servoing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
11 Mobile Robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
11.1 Nonholonomic Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
11.1.1 Integrability Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 473
11.2 Kinematic Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476
11.2.1 Unicycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
11.2.2 Bicycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
11.3 Chained Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
11.4 Dynamic Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
11.5 Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
11.5.1 Path and Timing Law . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
11.5.2 Flat Outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
11.5.3 Path Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
11.5.4 Trajectory Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498
11.5.5 Optimal Trajectories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499
11.6 Motion Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
11.6.1 Trajectory Tracking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503
11.6.2 Regulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510
11.7 Odometric Localization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518
12 Motion Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
12.1 The Canonical Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
12.2 Conﬁguration Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525
12.2.1 Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527
12.2.2 Obstacles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527
12.2.3 Examples of Obstacles . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
12.3 Planning via Retraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 532
12.4 Planning via Cell Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . 536
12.4.1 Exact Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
12.4.2 Approximate Decomposition . . . . . . . . . . . . . . . . . . . . . . . 539
12.5 Probabilistic Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541
12.5.1 PRM Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541

Contents

12.6

12.7

xxiii

12.5.2 Bidirectional RRT Method . . . . . . . . . . . . . . . . . . . . . . . . 543
Planning via Artiﬁcial Potentials . . . . . . . . . . . . . . . . . . . . . . . . . 546
12.6.1 Attractive Potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546
12.6.2 Repulsive Potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547
12.6.3 Total Potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549
12.6.4 Planning Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550
12.6.5 The Local Minima Problem . . . . . . . . . . . . . . . . . . . . . . . 551
The Robot Manipulator Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557

Appendices
A

Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563
A.1 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563
A.2 Matrix Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565
A.3 Vector Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 569
A.4 Linear Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 572
A.5 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573
A.6 Bilinear Forms and Quadratic Forms . . . . . . . . . . . . . . . . . . . . . . 574
A.7 Pseudo-inverse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 575
A.8 Singular Value Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . 577
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 578

B

Rigid-body Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
B.1 Kinematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579
B.2 Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581
B.3 Work and Energy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584
B.4 Constrained Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 588

C

Feedback Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 589
C.1 Control of Single-input/Single-output Linear Systems . . . . . . . 589
C.2 Control of Nonlinear Mechanical Systems . . . . . . . . . . . . . . . . . . 594
C.3 Lyapunov Direct Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598

D

Diﬀerential Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599
D.1 Vector Fields and Lie Brackets . . . . . . . . . . . . . . . . . . . . . . . . . . . 599
D.2 Nonlinear Controllability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604

xxiv

E

Contents

Graph Search Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 605
E.1 Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 605
E.2 Breadth-ﬁrst and Depth-ﬁrst Search . . . . . . . . . . . . . . . . . . . . . . 606
E.3 A Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 608

References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623

1
Introduction

Robotics is concerned with the study of those machines that can replace human beings in the execution of a task, as regards both physical activity and
decision making. The goal of the introductory chapter is to point out the
problems related to the use of robots in industrial applications, as well as the
perspectives oﬀered by advanced robotics. A classiﬁcation of the most common
mechanical structures of robot manipulators and mobile robots is presented.
Topics of modelling, planning and control are introduced which will be examined in the following chapters. The chapter ends with a list of references
dealing with subjects both of speciﬁc interest and of related interest to those
covered by this textbook.

1.1 Robotics
Robotics has profound cultural roots. Over the course of centuries, human beings have constantly attempted to seek substitutes that would be able to mimic
their behaviour in the various instances of interaction with the surrounding
environment. Several motivations have inspired this continuous search referring to philosophical, economic, social and scientiﬁc principles.
One of human beings’ greatest ambitions has been to give life to their
artifacts. The legend of the Titan Prometheus, who molded humankind from
clay, as well as that of the giant Talus, the bronze slave forged by Hephaestus,
testify how Greek mythology was inﬂuenced by that ambition, which has been
revisited in the tale of Frankenstein in modern times.
Just as the giant Talus was entrusted with the task of protecting the
island of Crete from invaders, in the Industrial Age a mechanical creature
(automaton) has been entrusted with the task of substituting a human being
in subordinate labor duties. This concept was introduced by the Czech playwright Karel Čapek who wrote the play Rossum’s Universal Robots (R.U.R.)
in 1920. On that occasion he coined the term robot — derived from the term

2

1 Introduction

robota that means executive labour in Slav languages — to denote the automaton built by Rossum who ends up by rising up against humankind in the
science ﬁction tale.
In the subsequent years, in view of the development of science ﬁction, the
behaviour conceived for the robot has often been conditioned by feelings. This
has contributed to rendering the robot more and more similar to its creator.
It is worth noticing how Rossum’s robots were represented as creatures
made with organic material. The image of the robot as a mechanical artifact
starts in the 1940s when the Russian Isaac Asimov, the well-known science
ﬁction writer, conceived the robot as an automaton of human appearance but
devoid of feelings. Its behaviour was dictated by a “positronic” brain programmed by a human being in such a way as to satisfy certain rules of ethical
conduct. The term robotics was then introduced by Asimov as the science
devoted to the study of robots which was based on the three fundamental
laws:
1. A robot may not injure a human being or, through inaction, allow a human
being to come to harm.
2. A robot must obey the orders given by human beings, except when such
orders would conﬂict with the ﬁrst law.
3. A robot must protect its own existence, as long as such protection does
not conﬂict with the ﬁrst or second law.
These laws established rules of behaviour to consider as speciﬁcations for
the design of a robot, which since then has attained the connotation of an
industrial product designed by engineers or specialized technicians.
Science ﬁction has inﬂuenced the man and the woman in the street that
continue to imagine the robot as a humanoid who can speak, walk, see, and
hear, with an appearance very much like that presented by the robots of the
movie Metropolis, a precursor of modern cinematography on robots, with Star
Wars and more recently with I, Robot inspired by Asimov’s novels.
According to a scientiﬁc interpretation of the science-ﬁction scenario, the
robot is seen as a machine that, independently of its exterior, is able to modify
the environment in which it operates. This is accomplished by carrying out
actions that are conditioned by certain rules of behaviour intrinsic in the
machine as well as by some data the robot acquires on its status and on the
environment. In fact, robotics is commonly deﬁned as the science studying the
intelligent connection between perception and action.
With reference to this deﬁnition, a robotic system is in reality a complex
system, functionally represented by multiple subsystems (Fig. 1.1).
The essential component of a robot is the mechanical system endowed, in
general, with a locomotion apparatus (wheels, crawlers, mechanical legs) and
a manipulation apparatus (mechanical arms, end-eﬀectors, artiﬁcial hands).
As an example, the mechanical system in Fig. 1.1 consists of two mechanical
arms (manipulation apparatus), each of which is carried by a mobile vehicle

1.2 Robot Mechanical Structure

3

Fig. 1.1. Components of a robotic system

(locomotion apparatus). The realization of such a system refers to the context
of design of articulated mechanical systems and choice of materials.
The capability to exert an action, both locomotion and manipulation, is
provided by an actuation system which animates the mechanical components
of the robot. The concept of such a system refers to the context of motion
control , dealing with servomotors, drives and transmissions.
The capability for perception is entrusted to a sensory system which can
acquire data on the internal status of the mechanical system (proprioceptive
sensors, such as position transducers) as well as on the external status of
the environment (exteroceptive sensors, such as force sensors and cameras).
The realization of such a system refers to the context of materials properties,
signal conditioning, data processing, and information retrieval.
The capability for connecting action to perception in an intelligent fashion is provided by a control system which can command the execution of the
action in respect to the goals set by a task planning technique, as well as
of the constraints imposed by the robot and the environment. The realization of such a system follows the same feedback principle devoted to control
of human body functions, possibly exploiting the description of the robotic
system’s components (modelling). The context is that of cybernetics, dealing
with control and supervision of robot motions, artiﬁcial intelligence and expert
systems, the computational architecture and programming environment.
Therefore, it can be recognized that robotics is an interdisciplinary subject
concerning the cultural areas of mechanics, control , computers, and electronics.

1.2 Robot Mechanical Structure
The key feature of a robot is its mechanical structure. Robots can be classiﬁed
as those with a ﬁxed base, robot manipulators, and those with a mobile base,

4

1 Introduction

mobile robots. In the following, the geometrical features of the two classes are
presented.
1.2.1 Robot Manipulators
The mechanical structure of a robot manipulator consists of a sequence of rigid
bodies (links) interconnected by means of articulations (joints); a manipulator
is characterized by an arm that ensures mobility, a wrist that confers dexterity,
and an end-eﬀector that performs the task required of the robot.
The fundamental structure of a manipulator is the serial or open kinematic
chain. From a topological viewpoint, a kinematic chain is termed open when
there is only one sequence of links connecting the two ends of the chain. Alternatively, a manipulator contains a closed kinematic chain when a sequence
of links forms a loop.
A manipulator’s mobility is ensured by the presence of joints. The articulation between two consecutive links can be realized by means of either a
prismatic or a revolute joint. In an open kinematic chain, each prismatic or
revolute joint provides the structure with a single degree of freedom (DOF). A
prismatic joint creates a relative translational motion between the two links,
whereas a revolute joint creates a relative rotational motion between the two
links. Revolute joints are usually preferred to prismatic joints in view of their
compactness and reliability. On the other hand, in a closed kinematic chain,
the number of DOFs is less than the number of joints in view of the constraints
imposed by the loop.
The degrees of freedom should be properly distributed along the mechanical structure in order to have a suﬃcient number to execute a given task.
In the most general case of a task consisting of arbitrarily positioning and
orienting an object in three-dimensional (3D) space, six DOFs are required,
three for positioning a point on the object and three for orienting the object
with respect to a reference coordinate frame. If more DOFs than task variables are available, the manipulator is said to be redundant from a kinematic
viewpoint.
The workspace represents that portion of the environment the manipulator’s end-eﬀector can access. Its shape and volume depend on the manipulator
structure as well as on the presence of mechanical joint limits.
The task required of the arm is to position the wrist which then is required
to orient the end-eﬀector. The type and sequence of the arm’s DOFs, starting from the base joint, allows a classiﬁcation of manipulators as Cartesian,
cylindrical , spherical , SCARA, and anthropomorphic.
Cartesian geometry is realized by three prismatic joints whose axes typically are mutually orthogonal (Fig. 1.2). In view of the simple geometry,
each DOF corresponds to a Cartesian space variable and thus it is natural to perform straight motions in space. The Cartesian structure oﬀers very
good mechanical stiﬀness. Wrist positioning accuracy is constant everywhere
in the workspace. This is the volume enclosed by a rectangular parallel-piped

1.2 Robot Mechanical Structure

5

Fig. 1.2. Cartesian manipulator and its workspace

Fig. 1.3. Gantry manipulator

(Fig. 1.2). As opposed to high accuracy, the structure has low dexterity since
all the joints are prismatic. The direction of approach in order to manipulate an object is from the side. On the other hand, if it is desired to approach an object from the top, the Cartesian manipulator can be realized by
a gantry structure as illustrated in Fig. 1.3. Such a structure makes available
a workspace with a large volume and enables the manipulation of objects of
large dimensions and heavy weight. Cartesian manipulators are employed for
material handling and assembly. The motors actuating the joints of a Cartesian manipulator are typically electric and occasionally pneumatic.
Cylindrical geometry diﬀers from Cartesian in that the ﬁrst prismatic joint
is replaced with a revolute joint (Fig. 1.4). If the task is described in cylindri-

6

1 Introduction

Fig. 1.4. Cylindrical manipulator and its workspace

Fig. 1.5. Spherical manipulator and its workspace

cal coordinates, in this case each DOF also corresponds to a Cartesian space
variable. The cylindrical structure oﬀers good mechanical stiﬀness. Wrist positioning accuracy decreases as the horizontal stroke increases. The workspace is
a portion of a hollow cylinder (Fig. 1.4). The horizontal prismatic joint makes
the wrist of a cylindrical manipulator suitable to access horizontal cavities.
Cylindrical manipulators are mainly employed for carrying objects even of
large dimensions; in such a case the use of hydraulic motors is to be preferred
to that of electric motors.
Spherical geometry diﬀers from cylindrical in that the second prismatic
joint is replaced with a revolute joint (Fig. 1.5). Each DOF corresponds to a
Cartesian space variable provided that the task is described in spherical coordinates. Mechanical stiﬀness is lower than the above two geometries and mechanical construction is more complex. Wrist positioning accuracy decreases
as the radial stroke increases. The workspace is a portion of a hollow sphere
(Fig. 1.5); it can also include the supporting base of the manipulator and thus

1.2 Robot Mechanical Structure

7

Fig. 1.6. SCARA manipulator and its workspace

Fig. 1.7. Anthropomorphic manipulator and its workspace

it can allow manipulation of objects on the ﬂoor. Spherical manipulators are
mainly employed for machining. Electric motors are typically used to actuate
the joints.
A special geometry is SCARA geometry that can be realized by disposing
two revolute joints and one prismatic joint in such a way that all the axes
of motion are parallel (Fig. 1.6). The acronym SCARA stands for Selective
Compliance Assembly Robot Arm and characterizes the mechanical features
of a structure oﬀering high stiﬀness to vertical loads and compliance to horizontal loads. As such, the SCARA structure is well-suited to vertical assembly
tasks. The correspondence between the DOFs and Cartesian space variables
is maintained only for the vertical component of a task described in Cartesian coordinates. Wrist positioning accuracy decreases as the distance of the
wrist from the ﬁrst joint axis increases. The typical workspace is illustrated

8

1 Introduction

Fig. 1.8. Manipulator with parallelogram

Fig. 1.9. Parallel manipulator

in Fig. 1.6. The SCARA manipulator is suitable for manipulation of small
objects; joints are actuated by electric motors.
Anthropomorphic geometry is realized by three revolute joints; the revolute
axis of the ﬁrst joint is orthogonal to the axes of the other two which are
parallel (Fig. 1.7). By virtue of its similarity with the human arm, the second
joint is called the shoulder joint and the third joint the elbow joint since
it connects the “arm” with the “forearm.” The anthropomorphic structure
is the most dexterous one, since all the joints are revolute. On the other
hand, the correspondence between the DOFs and the Cartesian space variables
is lost, and wrist positioning accuracy varies inside the workspace. This is
approximately a portion of a sphere (Fig. 1.7) and its volume is large compared
to manipulator encumbrance. Joints are typically actuated by electric motors.
The range of industrial applications of anthropomorphic manipulators is wide.

1.2 Robot Mechanical Structure

9

Fig. 1.10. Hybrid parallel-serial manipulator

According to the latest report by the International Federation of Robotics
(IFR), up to 2005, 59% of installed robot manipulators worldwide has anthropomorphic geometry, 20% has Cartesian geometry, 12% has cylindrical
geometry, and 8% has SCARA geometry.
All the previous manipulators have an open kinematic chain. Whenever
larger payloads are required, the mechanical structure will have higher stiﬀness
to guarantee comparable positioning accuracy. In such a case, resorting to
a closed kinematic chain is advised. For instance, for an anthropomorphic
structure, parallelogram geometry between the shoulder and elbow joints can
be adopted, so as to create a closed kinematic chain (Fig. 1.8).
An interesting closed-chain geometry is parallel geometry (Fig. 1.9) which
has multiple kinematic chains connecting the base to the end-eﬀector. The
fundamental advantage is seen in the high structural stiﬀness, with respect to
open-chain manipulators, and thus the possibility to achieve high operational
speeds; the drawback is that of having a reduced workspace.
The geometry illustrated in Fig. 1.10 is of hybrid type, since it consists
of a parallel arm and a serial kinematic chain. This structure is suitable for
the execution of manipulation tasks requiring large values of force along the
vertical direction.
The manipulator structures presented above are required to position the
wrist which is then required to orient the manipulator’s end-eﬀector. If arbitrary orientation in 3D space is desired, the wrist must possess at least three
DOFs provided by revolute joints. Since the wrist constitutes the terminal
part of the manipulator, it has to be compact; this often complicates its mechanical design. Without entering into construction details, the realization
endowing the wrist with the highest dexterity is one where the three revolute

10

1 Introduction

Fig. 1.11. Spherical wrist

axes intersect at a single point. In such a case, the wrist is called a spherical
wrist, as represented in Fig. 1.11. The key feature of a spherical wrist is the
decoupling between position and orientation of the end-eﬀector; the arm is entrusted with the task of positioning the above point of intersection, whereas
the wrist determines the end-eﬀector orientation. Those realizations where the
wrist is not spherical are simpler from a mechanical viewpoint, but position
and orientation are coupled, and this complicates the coordination between
the motion of the arm and that of the wrist to perform a given task.
The end-eﬀector is speciﬁed according to the task the robot should execute. For material handling tasks, the end-eﬀector consists of a gripper
of proper shape and dimensions determined by the object to be grasped
(Fig. 1.11). For machining and assembly tasks, the end-eﬀector is a tool or
a specialized device, e.g., a welding torch, a spray gun, a mill, a drill, or a
screwdriver.
The versatility and ﬂexibility of a robot manipulator should not induce
the conviction that all mechanical structures are equivalent for the execution
of a given task. The choice of a robot is indeed conditioned by the application
which sets constraints on the workspace dimensions and shape, the maximum
payload, positioning accuracy, and dynamic performance of the manipulator.
1.2.2 Mobile Robots
The main feature of mobile robots is the presence of a mobile base which
allows the robot to move freely in the environment. Unlike manipulators, such
robots are mostly used in service applications, where extensive, autonomous
motion capabilities are required. From a mechanical viewpoint, a mobile robot
consists of one or more rigid bodies equipped with a locomotion system. This
description includes the following two main classes of mobile robots:1
• Wheeled mobile robots typically consist of a rigid body (base or chassis)
and a system of wheels which provide motion with respect to the ground.
1

Other types of mechanical locomotion systems are not considered here. Among
these, it is worth mentioning tracked locomotion, very eﬀective on uneven terrain,
and undulatory locomotion, inspired by snake gaits, which can be achieved without speciﬁc devices. There also exist types of locomotion that are not constrained
to the ground, such as ﬂying and navigation.

1.2 Robot Mechanical Structure

11

Fig. 1.12. The three types of conventional wheels with their respective icons

Other rigid bodies (trailers), also equipped with wheels, may be connected
to the base by means of revolute joints.
• Legged mobile robots are made of multiple rigid bodies, interconnected by
prismatic joints or, more often, by revolute joints. Some of these bodies
form lower limbs, whose extremities (feet) periodically come in contact
with the ground to realize locomotion. There is a large variety of mechanical structures in this class, whose design is often inspired by the study of
living organisms (biomimetic robotics): they range from biped humanoids
to hexapod robots aimed at replicating the biomechanical eﬃciency of
insects.
Only wheeled vehicles are considered in the following, as they represent
the vast majority of mobile robots actually used in applications. The basic
mechanical element of such robots is indeed the wheel. Three types of conventional wheels exist, which are shown in Fig. 1.12 together with the icons
that will be used to represent them:
• The ﬁxed wheel can rotate about an axis that goes through the center
of the wheel and is orthogonal to the wheel plane. The wheel is rigidly
attached to the chassis, whose orientation with respect to the wheel is
therefore constant.
• The steerable wheel has two axes of rotation. The ﬁrst is the same as a
ﬁxed wheel, while the second is vertical and goes through the center of the
wheel. This allows the wheel to change its orientation with respect to the
chassis.
• The caster wheel has two axes of rotation, but the vertical axis does not
pass through the center of the wheel, from which it is displaced by a constant oﬀset. Such an arrangement causes the wheel to swivel automatically,
rapidly aligning with the direction of motion of the chassis. This type of
wheel is therefore introduced to provide a supporting point for static balance without aﬀecting the mobility of the base; for instance, caster wheels
are commonly used in shopping carts as well as in chairs with wheels.

12

1 Introduction

Fig. 1.13. A diﬀerential-drive mobile robot

Fig. 1.14. A synchro-drive mobile robot

The variety of kinematic structures that can be obtained by combining
the three conventional wheels is wide. In the following, the most relevant
arrangements are brieﬂy examined.
In a diﬀerential-drive vehicle there are two ﬁxed wheels with a common
axis of rotation, and one or more caster wheels, typically smaller, whose function is to keep the robot statically balanced (Fig. 1.13). The two ﬁxed wheels
are separately controlled, in that diﬀerent values of angular velocity may be
arbitrarily imposed, while the caster wheel is passive. Such a robot can rotate
on the spot (i.e., without moving the midpoint between the wheels), provided
that the angular velocities of the two wheels are equal and opposite.
A vehicle with similar mobility is obtained using a synchro-drive kinematic
arrangement (Fig. 1.14). This robot has three aligned steerable wheels which
are synchronously driven by only two motors through a mechanical coupling,
e.g., a chain or a transmission belt. The ﬁrst motor controls the rotation of the
wheels around the horizontal axis, thus providing the driving force (traction)
to the vehicle. The second motor controls the rotation of the wheels around
the vertical axis, hence aﬀecting their orientation. Note that the heading of
the chassis does not change during the motion. Often, a third motor is used
in this type of robot to rotate independently the upper part of the chassis (a
turret) with respect to the lower part. This may be useful to orient arbitrarily
a directional sensor (e.g., a camera) or in any case to recover an orientation
error.
In a tricycle vehicle (Fig. 1.15) there are two ﬁxed wheels mounted on a
rear axle and a steerable wheel in front. The ﬁxed wheels are driven by a single

1.2 Robot Mechanical Structure

13

Fig. 1.15. A tricycle mobile robot

Fig. 1.16. A car-like mobile robot

motor which controls their traction,2 while the steerable wheel is driven by
another motor which changes its orientation, acting then as a steering device.
Alternatively, the two rear wheels may be passive and the front wheel may
provide traction as well as steering.
A car-like vehicle has two ﬁxed wheels mounted on a rear axle and two
steerable wheels mounted on a front axle, as shown in Fig. 1.16. As in the
previous case, one motor provides (front or rear) traction while the other
changes the orientation of the front wheels with respect to the vehicle. It is
worth pointing out that, to avoid slippage, the two front wheels must have a
diﬀerent orientation when the vehicle moves along a curve; in particular, the
internal wheel is slightly more steered with respect to the external one. This
is guaranteed by the use of a speciﬁc device called Ackermann steering.
Finally, consider the robot in Fig. 1.17, which has three caster wheels
usually arranged in a symmetric pattern. The traction velocities of the three
wheels are independently driven. Unlike the previous cases, this vehicle is omnidirectional : in fact, it can move instantaneously in any Cartesian direction,
as well as re-orient itself on the spot.
In addition to the above conventional wheels, there exist other special
types of wheels, among which is notably the Mecanum (or Swedish) wheel ,
shown in Fig. 1.18. This is a ﬁxed wheel with passive rollers placed along the
external rim; the axis of rotation of each roller is typically inclined by 45◦ with
respect to the plane of the wheel. A vehicle equipped with four such wheels
mounted in pairs on two parallel axles is also omnidirectional.
2

The distribution of the traction torque on the two wheels must take into account
the fact that in general they move with diﬀerent speeds. The mechanism which
equally distributes traction is the diﬀerential .

14

1 Introduction

Fig. 1.17. An omnidirectional mobile robot with three independently driven caster
wheels

Fig. 1.18. A Mecanum (or Swedish) wheel

In the design of a wheeled robot, the mechanical balance of the structure
does not represent a problem in general. In particular, a three-wheel robot is
statically balanced as long as its center of mass falls inside the support triangle,
which is deﬁned by the contact points between the wheels and ground. Robots
with more than three wheels have a support polygon, and thus it is typically
easier to guarantee the above balance condition. It should be noted, however,
that when the robot moves on uneven terrain a suspension system is needed
to maintain the contact between each wheel and the ground.
Unlike the case of manipulators, the workspace of a mobile robot (deﬁned
as the portion of the surrounding environment that the robot can access) is potentially unlimited. Nevertheless, the local mobility of a non-omnidirectional
mobile robot is always reduced; for instance, the tricycle robot in Fig. 1.15
cannot move instantaneously in a direction parallel to the rear wheel axle.
Despite this fact, the tricycle can be manoeuvered so as to obtain, at the end
of the motion, a net displacement in that direction. In other words, many
mobile robots are subject to constraints on the admissible instantaneous motions, without actually preventing the possibility of attaining any position and
orientation in the workspace. This also implies that the number of DOFs of
the robot (meant as the number of admissible instantaneous motions) is lower
than the number of its conﬁguration variables.
It is obviously possible to merge the mechanical structure of a manipulator
with that of a mobile vehicle by mounting the former on the latter. Such
a robot is called a mobile manipulator and combines the dexterity of the
articulated arm with the unlimited mobility of the base. An example of such
a mechanical structure is shown in Fig. 1.19. However, the design of a mobile
manipulator involves additional diﬃculties related, for instance, to the static

1.3 Industrial Robotics

15

Fig. 1.19. A mobile manipulator obtained by mounting an anthropomorphic arm
on a diﬀerential-drive vehicle

and dynamic mechanical balance of the robot, as well as to the actuation of
the two systems.

1.3 Industrial Robotics
Industrial robotics is the discipline concerning robot design, control and applications in industry, and its products have by now reached the level of a
mature technology. The connotation of a robot for industrial applications is
that of operating in a structured environment whose geometrical or physical
characteristics are mostly known a priori. Hence, limited autonomy is required.
The early industrial robots were developed in the 1960s, at the conﬂuence
of two technologies: numerical control machines for precise manufacturing,
and teleoperators for remote radioactive material handling. Compared to its
precursors, the ﬁrst robot manipulators were characterized by:
• versatility, in view of the employment of diﬀerent end-eﬀectors at the tip
of the manipulator,
• adaptability to a priori unknown situations, in view of the use of sensors,
• positioning accuracy, in view of the adoption of feedback control techniques,
• execution repeatability, in view of the programmability of various operations.
During the subsequent decades, industrial robots have gained a wide popularity as essential components for the realization of automated manufacturing

16

1 Introduction
140,000
127
120,000

112
99

Units

100,000
77

80,000
60,000

69
53

82

79
69

97
81

78
69

55

40,000
20,000
0
1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006

Fig. 1.20. Yearly installations of industrial robots worldwide

systems. The main factors having determined the spread of robotics technology in an increasingly wider range of applications in the manufacturing
industry are reduction of manufacturing costs, increase of productivity, improvement of product quality standards and, last but not least, the possibility
of eliminating harmful or oﬀ-putting tasks for the human operator in a manufacturing system.
By its usual meaning, the term automation denotes a technology aimed at
replacing human beings with machines in a manufacturing process, as regards
not only the execution of physical operations but also the intelligent processing
of information on the status of the process. Automation is then the synthesis
of industrial technologies typical of the manufacturing process and computer
technology allowing information management. The three levels of automation
one may refer to are rigid automation, programmable automation, and ﬂexible
automation.
Rigid automation deals with a factory context oriented to the mass manufacture of products of the same type. The need to manufacture large numbers
of parts with high productivity and quality standards demands the use of
ﬁxed operational sequences to be executed on the workpiece by special purpose machines.
Programmable automation deals with a factory context oriented to the
manufacture of low-to-medium batches of products of diﬀerent types. A programmable automated system permits changing easy the sequence of operations to be executed on the workpieces in order to vary the range of products.
The machines employed are more versatile and are capable of manufacturing
diﬀerent objects belonging to the same group technology. The majority of the
products available on the market today are manufactured by programmable
automated systems.

1.3 Industrial Robotics

17

Automotive parts
Motor vehicles
Chemical, rubber and plastics
Electrical/electronics
Metal products

2005

Machinery
(industrial and consumer)

2006

Food
Communication
Precision and optical products
0

5,000

10,000

15,000

20,000

25,000

30,000

Units

Fig. 1.21. Yearly supply of industrial robots by main industries

Flexible automation represents the evolution of programmable automation.
Its goal is to allow manufacturing of variable batches of diﬀerent products by
minimizing the time lost for reprogramming the sequence of operations and
the machines employed to pass from one batch to the next. The realization of a
ﬂexible manufacturing system (FMS) demands strong integration of computer
technology with industrial technology.
The industrial robot is a machine with signiﬁcant characteristics of versatility and ﬂexibility. According to the widely accepted deﬁnition of the Robot
Institute of America, a robot is a reprogrammable multifunctional manipulator
designed to move materials, parts, tools or specialized devices through variable
programmed motions for the performance of a variety of tasks. Such a deﬁnition, dating back to 1980, reﬂects the current status of robotics technology.
By virtue of its programmability, the industrial robot is a typical component of programmable automated systems. Nonetheless, robots can be entrusted with tasks in both rigid and ﬂexible automated systems.
According to the above-mentioned IFR report, up to 2006 nearly one million industrial robots are in use worldwide, half of which are in Asia, one third
in Europe, and 16% in North America. The four countries with the largest
number of robots are Japan, Germany, United States and Italy. The ﬁgures
for robot installations in the last 15 years are summarized in the graph in
Fig. 1.20; by the end of 2007, an increase of 10% in sales with respect to the
previous year is foreseen, with milder increase rates in the following years,
reaching a worldwide ﬁgure of 1,200,000 units at work by the end of 2010.
In the same report it is shown how the average service life of an industrial
robot is about 12 years, which may increase to 15 in a few years from now.
An interesting statistic is robot density based on the total number of persons
employed: this ranges from 349 robots in operation per 10,000 workers to

18

1 Introduction

Fig. 1.22. Examples of AGVs for material handling (courtesy of E&K Automation
GmbH)

187 in Korea, 186 in Germany, and 13 in Italy. The United States has just
99 robots per 10,000 workers. The average cost of a 6-axis industrial robot,
including the control unit and development software, ranges from 20,000 to
60,000 euros, depending on the size and applications.
The automotive industry is still the predominant user of industrial robots.
The graph in Fig. 1.21 referring to 2005 and 2006, however, reveals how both
the chemical industry and the electrical/electronics industry are gaining in importance, and new industrial applications, such as metal products, constitute
an area with a high potential investment.
Industrial robots present three fundamental capacities that make them
useful for a manufacturing process: material handling, manipulation, and measurement.
In a manufacturing process, each object has to be transferred from one
location in the factory to another in order to be stored, manufactured, assembled, and packed. During transfer, the physical characteristics of the object do
not undergo any alteration. The robot’s capability to pick up an object, move
it in space on predeﬁned paths and release it makes the robot itself an ideal
candidate for material handling operations. Typical applications include:
•
•
•
•
•

palletizing (placing objects on a pallet in an ordered way),
warehouse loading and unloading,
mill and machine tool tending,
part sorting,
packaging.

In these applications, besides robots, Automated Guided Vehicles (AGV)
are utilized which ensure handling of parts and tools around the shop ﬂoor

1.3 Industrial Robotics

19

Handling
Welding
Assembly
2005

Dispensing

2006

Processing
Others
0

2,000

4,000

6,000

8,000

10,000

12,000

14,000

16,000

18,000

Units

Fig. 1.23. Yearly supply of industrial robots in Europe for manufacturing operations

from one manufacturing cell to the next (Fig. 1.22). As compared to the traditional ﬁxed guide paths for vehicles (inductive guide wire, magnetic tape,
or optical visible line), modern AGVs utilize high-tech systems with onboard
microprocessors and sensors (laser, odometry, GPS) which allow their localization within the plant layout, and manage their work ﬂow and functions,
allowing their complete integration in the FMS. The mobile robots employed
in advanced applications can be considered as the natural evolution of the
AGV systems, as far as enhanced autonomy is concerned.
Manufacturing consists of transforming objects from raw material into
ﬁnished products; during this process, the part either changes its own physical
characteristics as a result of machining, or loses its identity as a result of an
assembly of more parts. The robot’s capability to manipulate both objects and
tools make it suitable to be employed in manufacturing. Typical applications
include:
•
•
•
•
•
•
•
•
•
•

arc and spot welding,
painting and coating,
gluing and sealing,
laser and water jet cutting,
milling and drilling,
casting and die spraying,
deburring and grinding,
screwing, wiring and fastening,
assembly of mechanical and electrical groups,
assembly of electronic boards.

20

1 Introduction

Fig. 1.24. The AdeptOne XL robot (courtesy of Adept Technology Inc)

Besides material handling and manipulation, in a manufacturing process
it is necessary to perform measurements to test product quality. The robot’s
capability to explore 3D space together with the availability of measurements
on the manipulator’s status allow a robot to be used as a measuring device.
Typical applications include:
• object inspection,
• contour ﬁnding,
• detection of manufacturing imperfections.
The graph in Fig. 1.23 reports the number of robots employed in Europe
in 2005 and 2006 for various operations, which reveals how material handling
requires twice as many robots employed for welding, whereas a limited number
of robots is still employed for assembly.
In the following some industrial robots are illustrated in terms of their
features and application ﬁelds.
The AdeptOne XL robot in Fig. 1.24 has a four-joint SCARA structure.
Direct drive motors are employed. The maximum reach is 800 mm, with a
repeatability of 0.025 mm horizontally and 0.038 mm vertically. Maximum
speeds are 1200 mm/s for the prismatic joint, while they range from to 650
to 3300 deg/s for the three revolute joints. The maximum payload3 is 12 kg.
Typical industrial applications include small-parts material handling, assembly and packaging.
3

Repeatability and payload are classical parameters found in industrial robot data
sheets. The former gives a measure of the manipulator’s ability to return to a
previously reached position, while the latter indicates the average load to be
carried at the robot’s end-eﬀector.

1.3 Industrial Robotics

21

Fig. 1.25. The COMAU Smart NS robot (courtesy of COMAU SpA Robotica)

Fig. 1.26. The ABB IRB 4400 robot (courtesy of ABB Robotics)

The Comau SMART NS robot in Fig. 1.25 has a six-joint anthropomorphic
structure with spherical wrist. In its four versions, the outreach ranges from
1650 and 1850 mm horizontally, with a repeatability of 0.05 mm. Maximum
speeds range from 155 to 170 deg/s for the inner three joints, and from 350
to 550 deg/s for the outer three joints. The maximum payload is 16 kg. Both
ﬂoor and ceiling mounting positions are allowed. Typical industrial applications include arc welding, light handling, assembly and technological processes.
The ABB IRB 4400 robot in Fig. 1.26 also has a six-joint anthropomorphic structure, but unlike the previous open-chain structure, it possesses a
closed chain of parallelogram type between the shoulder and elbow joints.
The outreach ranges from 1960 to 2550 mm for the various versions, with a

22

1 Introduction

Fig. 1.27. The KUKA KR 60 Jet robot (courtesy of KUKA Roboter GmbH)

repeatability from 0.07 to 0.1 mm. The maximum speed at the end-eﬀector
is 2200 mm/s. The maximum payload is 60 kg. Floor or shelf-mounting is
available. Typical industrial applications include material handling, machine
tending, grinding, gluing, casting, die spraying and assembly.
The KUKA KR 60 Jet robot in Fig. 1.27 is composed of a ﬁve-axis structure, mounted on a sliding track with a gantry-type installation; the upright
installation is also available. The linear unit has a stroke from a minimum
of 400 mm to a maximum of 20 m (depending on customer’s request), and a
maximum speed of 3200 mm/s. On the other hand, the robot has a payload
of 60 kg, an outreach of 820 mm and a repeatability of 0.15 mm. Maximum
speeds are 120 deg/s and 166 deg/s for the ﬁrst two joints, while they range
from 260 to 322 deg/s for the outer three joints. Typical industrial applications
include machine tending, arc welding, deburring, coating, sealing, plasma and
waterjet cutting.
The ABB IRB340 FlexPicker robot in Fig. 1.28 adopts a parallel geometry
with four axes; in view of its reduced weight and ﬂoor mounting, the robot
can transport 150 objects a minute (cycle time of just 0.4 s), reaching record
speeds of 10 m/s and accelerations of 100 m/s2 , for a payload of 1 kg, with
a repeatability of 0.1 mm. In its ‘clean’ aluminum version, it is particularly
suitable for packaging in the food and pharmaceutical industries.
The Fanuc M-16iB robot in Fig. 1.29 has a six-joint anthropomorphic
structure with a spherical wrist. In its two versions, the outreach varies
from 1667 to 1885 mm horizontally, with a repeatability of 0.1 mm. Maximum
speeds range from 165 to 175 deg/s for the inner three joints, and from 340
to 520 deg/s for the outer three joints. Payload varies from 10 to 20 kg. The
peculiarity of this robot consists of the integrated sensors in the control unit,
including a servoing system based on 3D vision and a six-axis force sensor.

1.3 Industrial Robotics

23

Fig. 1.28. The ABB IRB 340 FlexPicker robot (courtesy of ABB Robotics)

Fig. 1.29. The Fanuc M-16iB robot (courtesy of Fanuc Ltd)

The robot is utilized for handling arbitrarily located objects, deburring, sealing and waterjet cutting.
The Light Weight Robot (LWR) in Fig. 1.30 with a seven-axis structure
was introduced in 2006 as the outcome of technology transfer from DLR (the
German Aerospace Agency) to KUKA. In view of the adoption of lightweight
materials, as well as the adoption of torque sensors at the joints, the robot
can manipulate a payload of 7 to 14 kg, in the face of a weight of the structure
of just 15 kg. The horizontal outreach is 868 mm, with joint speeds ranging
from 110 to 210 deg/s. On the other hand, the presence of the seventh axis of
motion confers kinematic redundancy to the robot, which can then be reconﬁgured into more dexterous postures for the execution of given tasks. Such

24

1 Introduction

Fig. 1.30. The KUKA LWR robot (courtesy of KUKA Roboter GmbH)

a manipulator represents one of the most advanced industrial products and,
in view of its lightweight feature, it oﬀers interesting performance for interaction with the environment, ensuring an inherent safety in case of contact with
human beings.
In most industrial applications requiring object manipulation, typical grippers are utilized as end-eﬀectors. Nevertheless, whenever enhanced manipulability and dexterity is desired, multiﬁngered robot hands are available.
The BarrettHand (Fig. 1.31), endowed with a ﬁxed ﬁnger and two mobile
ﬁngers around the base of the palm, allows the manipulation of objects of
diﬀerent dimension, shape and orientation.
The SCHUNK Antropomorphic Hand (SAH) in Fig. 1.32 is the outcome
of technology transfer from DLR and Harbin Institute of Technology (China)
to SCHUNK. Characterized by three independent aligned ﬁngers and an opposing ﬁnger which is analogous to the human thumb. The ﬁnger joints are
endowed with magnetic angular sensors and torque sensors. This hand oﬀers
good dexterity and approaches the characteristics of the human hand.
LWR technology has been employed for the realization of the two arms
of Justin, a humanoid manipulator made by DLR, composed of a three-joint
torso with an anthropomorphic structure, two seven-axis arms and a sensorized head. The robot is illustrated in Fig. 1.33 in the execution of a bimanual manipulation task; the hands employed are previous versions of the SAH
anthropomorphic hand.
The applications listed describe the current employment of robots as components of industrial automation systems. They all refer to strongly structured
working environments and thus do not exhaust all the possible utilizations of
robots for industrial applications. Whenever it is desired to tackle problems
requiring the adaptation of the robot to a changeable working environment,
the fall-out of advanced robotics products are of concern. In this regard, the

1.4 Advanced Robotics

25

Fig. 1.31. The BarrettHand (courtesy of Barrett Technology Inc)

Fig. 1.32. The SCHUNK Anthropomorphic Hand (courtesy of SCHUNK Intec Ltd)

lightweight robot, the hands and the humanoid manipulator presented above
are to be considered at the transition from traditional industrial robotics systems toward those innovative systems of advanced robotics.

1.4 Advanced Robotics
The expression advanced robotics usually refers to the science studying robots
with marked characteristics of autonomy, operating in scarcely structured
or unstructured environments, whose geometrical or physical characteristics
would not be known a priori.
Nowadays, advanced robotics is still in its youth. It has indeed featured
the realization of prototypes only, because the associated technology is not
yet mature. There are many motivations which strongly encourage advances
in knowledge within this ﬁeld. They range from the need for automata whenever human operators are not available or are not safe (ﬁeld robots), to the
opportunity of developing products for potentially wide markets which are
aimed at improving quality of life (service robots).
The graph in Fig. 1.34 reports the number of robots in stock for nonindustrial applications at the end of 2006 and the forecast to 2010. Such
applications are characterized by the complexity level, the uncertainty and
variability of the environment with which the robot interacts, as shown in the
following examples.

26

1 Introduction

Fig. 1.33. The Justin humanoid robot manipulator (courtesy of DLR)

1.4.1 Field Robots
The context is that of deploying robots in areas where human beings could
not survive or be exposed to unsustainable risks. Such robots should carry
out exploration tasks and report useful data on the environment to a remote
operator, using suitable onboard sensors. Typical scenarios are the exploration of a volcano, the intervention in areas contaminated by poisonous gas
or radiation, or the exploration of the deep ocean or space. As is well known,
NASA succeeded in delivering some mobile robots (rovers) to Mars (Fig. 1.35)
which navigated on the Martian soil, across rocks, hills and crevasses. Such
rovers were partially teleoperated from earth and have successfully explored
the environment with suﬃcient autonomy. Some mini-robots were deployed
on September 11, 2001 at Ground Zero after the collapse of the Twin Towers
in New York, to penetrate the debris in the search for survivors.
A similar scenario is that of disasters caused by ﬁres in tunnels or earthquakes; in such occurrences, there is a danger of further explosions, escape of
harmful gases or collapse, and thus human rescue teams may cooperate with
robot rescue teams. Also in the military ﬁeld, unmanned autonomous aircrafts
and missiles are utilized, as well as teleoperated robots with onboard cameras
to explore buildings. The ‘Grand Challenge’ of October 2005 (Fig. 1.36) was
ﬁnancially supported by the US Department of Defense (DARPA) with the
goal of developing autonomous vehicles to carry weapons and sensors, thus
reducing soldier employment.

1.4 Advanced Robotics

27

14,000
12,000

Stock at the end of 2006
New installations 2007í2010

Units

10,000
8,000
6,000
4,000
2,000
0

Defense, rescue,
Medical
Others
Hostile fields
security
Cleaning
Mobile platforms
Logistics
Construction and demolition

Underwater

Fig. 1.34. Robots on stock for non-industrial applications

Fig. 1.35. The Sojourner rover was deployed by the Pathﬁnder lander and explored
250 m2 of Martian soil in 1997 (courtesy of NASA)

1.4.2 Service Robots
Autonomous vehicles are also employed for civil applications, i.e., for mass
transit systems (Fig. 1.37), thus contributing to the reduction of pollution
levels. Such vehicles are part of the so-called Intelligent Transportation Systems (ITS) devoted to traﬃc management in urban areas. Another feasible
application where the adoption of mobile robots oﬀers potential advantages
is museum guided tours (Fig. 1.38).
Many countries are investing in establishing the new market of service
robots which will co-habitat with human beings in everyday life. According
to the above-mentioned IFR report, up to 2005 1.9 million service robots for
domestic applications (Fig. 1.39) and 1 million toy robots have been sold.
Technology is ready to transform into commercial products the prototypes
of robotic aids to enhance elderly and impaired people’s autonomy in everyday
life; autonomous wheelchairs, mobility aid lifters, feeding aids and rehabilitation robots allowing tetraplegics to perform manual labor tasks are examples
of such service devices. In perspective, other than an all-purpose robot waiter,

28

1 Introduction

Fig. 1.36. The unmanned car Stanley autonomously completed a path of 132 miles
in the record time of 6 h and 53 min (courtesy of DARPA)

Fig. 1.37. The Cycab is an electrically-driven vehicle for autonomous transportation
in urban environments (courtesy of INRIA)

assistance, and healthcare systems integrating robotic and telematic modules
will be developed for home service management (domotics).
Several robotic systems are employed for medical applications. Surgery
assistance systems exploit a robot’s high accuracy to position a tool, i.e., for
hip prosthesis implant. Yet, in minimally-invasive surgery, i.e., cardiac surgery,
the surgeon operates while seated comfortably at a console viewing a 3D image
of the surgical ﬁeld, and operating the surgical instruments remotely by means
of a haptic interface (Fig. 1.40).
Further, in diagnostic and endoscopic surgery systems, small teleoperated
robots travels through the cavities of human body, i.e., in the gastrointestinal
system, bringing live images or intervening in situ for biopsy, dispensing drugs
or removing neoplasms.

1.5 Robot Modelling, Planning and Control

29

Fig. 1.38. Rhino, employing the synchro-drive mobile base B21 by Real World
Interface, was one of the ﬁrst robots for museum guided tours (courtesy of Deutsches
Museum Bonn)

Fig. 1.39. The vacuum robot Roomba, employing a diﬀerential-drive kinematics,
autonomously sweeps and cleans ﬂoors (courtesy of I-Robot Corp)

Finally, in motor rehabilitation systems, a hemiplegic patient wears an
exoskeleton, which actively interacts, sustains and corrects the movements
according to the physiotherapist’s programmed plan.
Another wide market segment comes from entertainment, where robots
are used as toy companions for children, and life companions for the elderly,
such as humanoid robots (Fig. 1.41) and the pet robots (Fig. 1.42) being
developed in Japan. It is reasonable to predict that service robots will be
naturally integrated into our society. Tomorrow, robots will be as pervasive
and personal as today’s personal computers, or just as TV sets in the homes
of 20 years ago. Robotics will then become ubiquitous, a challenge under
discussion within the scientiﬁc community.

1.5 Robot Modelling, Planning and Control
In all robot applications, completion of a generic task requires the execution
of a speciﬁc motion prescribed to the robot. The correct execution of such

30

1 Introduction

Fig. 1.40. The da Vinci robotic system for laparoscopic surgery (courtesy of Intuitive Surgical Inc)

motion is entrusted to the control system which should provide the robot’s
actuators with the commands consistent with the desired motion. Motion
control demands an accurate analysis of the characteristics of the mechanical
structure, actuators, and sensors. The goal of such analysis is the derivation
of the mathematical models describing the input/output relationship characterizing the robot components. Modelling a robot manipulator is therefore a
necessary premise to ﬁnding motion control strategies.
Signiﬁcant topics in the study of modelling, planning and control of robots
which constitute the subject of subsequent chapters are illustrated below.
1.5.1 Modelling
Kinematic analysis of the mechanical structure of a robot concerns the description of the motion with respect to a ﬁxed reference Cartesian frame
by ignoring the forces and moments that cause motion of the structure. It
is meaningful to distinguish between kinematics and diﬀerential kinematics.
With reference to a robot manipulator, kinematics describes the analytical
relationship between the joint positions and the end-eﬀector position and orientation. Diﬀerential kinematics describes the analytical relationship between
the joint motion and the end-eﬀector motion in terms of velocities, through
the manipulator Jacobiann.
The formulation of the kinematics relationship allows the study of two
key problems of robotics, namely, the direct kinematics problem and the inverse kinematics problem. The former concerns the determination of a systematic, general method to describe the end-eﬀector motion as a function of
the joint motion by means of linear algebra tools. The latter concerns the

1.5 Robot Modelling, Planning and Control

31

Fig. 1.41. The Asimo humanoid robot, launched in 1996, has been endowed with
even more natural locomotion and human-robot interaction skills (courtesy of Honda
Motor Company Ltd)

Fig. 1.42. The AIBO dog had been the most widely diﬀused entertainment robot
in the recent years (courtesy of Sony Corp)

inverse problem; its solution is of fundamental importance to transform the
desired motion, naturally prescribed to the end-eﬀector in the workspace, into
the corresponding joint motion.
The availability of a manipulator’s kinematic model is also useful to determine the relationship between the forces and torques applied to the joints
and the forces and moments applied to the end-eﬀector in static equilibrium
conﬁgurations.
Chapter 2 is dedicated to the study of kinematics. Chapter 3 is dedicated to
the study of diﬀerential kinematics and statics, whereas Appendix A provides
a useful brush-up on linear algebra.
Kinematics of a manipulator represents the basis of a systematic, general
derivation of its dynamics, i.e., the equations of motion of the manipulator
as a function of the forces and moments acting on it. The availability of the
dynamic model is very useful for mechanical design of the structure, choice
of actuators, determination of control strategies, and computer simulation of

32

1 Introduction

manipulator motion. Chapter 7 is dedicated to the study of dynamics, whereas
Appendix B recalls some fundamentals on rigid body mechanics.
Modelling of mobile robots requires a preliminary analysis of the kinematic
constraints imposed by the presence of wheels. Depending on the mechanical
structure, such constraints can be integrable or not; this has direct consequence on a robot’s mobility. The kinematic model of a mobile robot is essentially the description of the admissible instantaneous motions in respect
of the constraints. On the other hand, the dynamic model accounts for the
reaction forces and describes the relationship between the above motions and
the generalized forces acting on the robot. These models can be expressed
in a canonical form which is convenient for design of planning and control
techniques. Kinematic and dynamic analysis of mobile robots is developed
in Chap. 11, while Appendix D contains some useful concepts of diﬀerential
geometry.
1.5.2 Planning
With reference to the tasks assigned to a manipulator, the issue is whether
to specify the motion at the joints or directly at the end-eﬀector. In material
handling tasks, it is suﬃcient to assign only the pick-up and release locations
of an object (point-to-point motion), whereas, in machining tasks, the endeﬀector has to follow a desired trajectory (path motion). The goal of trajectory
planning is to generate the timing laws for the relevant variables (joint or endeﬀector) starting from a concise description of the desired motion. Chapter 4
is dedicated to trajectory planning for robot manipulators.
The motion planning problem for a mobile robot concerns the generation
of trajectories to take the vehicle from a given initial conﬁguration to a desired
ﬁnal conﬁguration. Such a problem is more complex than that of robot manipulators, since trajectories have to be generated in respect of the kinematic
constraints imposed by the wheels. Some solution techniques are presented in
Chap. 11, which exploit the speciﬁc diﬀerential structure of the mobile robots’
kinematic models.
Whenever obstacles are present in a mobile robot’s workspace, the planned
motions must be safe, so as to avoid collisions. Such a problem, known as
motion planning, can be formulated in an eﬀective fashion for both robot manipulators and mobile robots utilizing the conﬁguration space concept. The
solution techniques are essentially of algorithmic nature and include exact,
probabilistic and heuristic methods. Chapter 12 is dedicated to motion planning problem, while Appendix E provides some basic concepts on graph search
algorithms.
1.5.3 Control
Realization of the motion speciﬁed by the control law requires the employment
of actuators and sensors. The functional characteristics of the most commonly
used actuators and sensors for robots are described in Chap. 5.

Bibliography

33

Chapter 6 is concerned with the hardware/software architecture of a
robot’s control system which is in charge of implementation of control laws as
well as of interface with the operator.
The trajectories generated constitute the reference inputs to the motion
control system of the mechanical structure. The problem of robot manipulator
control is to ﬁnd the time behaviour of the forces and torques to be delivered
by the joint actuators so as to ensure the execution of the reference trajectories. This problem is quite complex, since a manipulator is an articulated
system and, as such, the motion of one link inﬂuences the motion of the others. Manipulator equations of motion indeed reveal the presence of coupling
dynamic eﬀects among the joints, except in the case of a Cartesian structure
with mutually orthogonal axes. The synthesis of the joint forces and torques
cannot be made on the basis of the sole knowledge of the dynamic model,
since this does not completely describe the real structure. Therefore, manipulator control is entrusted to the closure of feedback loops; by computing the
deviation between the reference inputs and the data provided by the proprioceptive sensors, a feedback control system is capable of satisfying accuracy
requirements on the execution of the prescribed trajectories.
Chapter 8 is dedicated to the presentation of motion control techniques,
whereas Appendix C illustrates the basic principles of feedback control .
Control of a mobile robot substantially diﬀers from the analogous problem
for robot manipulators. This is due, in turn, to the availability of fewer control
inputs than the robot has conﬁguration variables. An important consequence
is that the structure of a controller allowing a robot to follow a trajectory
(tracking problem) is unavoidably diﬀerent from that of a controller aimed at
taking the robot to a given conﬁguration (regulation problem). Further, since
a mobile robot’s proprioceptive sensors do not yield any data on the vehicle’s
conﬁguration, it is necessary to develop localization methods for the robot
in the environment. The control design problem for wheeled mobile robots is
treated in Chap. 11.
If a manipulation task requires interaction between the robot and the environment, the control problem should account for the data provided by the
exteroceptive sensors; the forces exchanged at the contact with the environment, and the objects’ position as detected by suitable cameras. Chapter 9 is
dedicated to force control techniques for robot manipulators, while Chap. 10
presents visual control techniques.

Bibliography
In the last 30 years, the robotics ﬁeld has stimulated the interest of an increasing number of scholars. A truly respectable international research community
has been established. Literature production has been conspicuous, both in
terms of textbooks and scientiﬁc monographs and in terms of journals dedicated to robotics. Therefore, it seems appropriate to close this introduction

34

1 Introduction

by oﬀering a selection of bibliographical reference sources to those readers who
wish to make a thorough study of robotics.
Besides indicating those basic textbooks sharing an aﬃnity of contents
with this one, the following lists include specialized books on related subjects, collections of contributions on the state of the art of research, scientiﬁc
journals, and series of international conferences.
Basic textbooks
• J. Angeles, Fundamentals of Robotic Mechanical Systems: Theory, Methods, and Algorithms, Springer-Verlag, New York, 1997.
• H. Asada, J.-J.E. Slotine, Robot Analysis and Control , Wiley, New York,
1986.
• G.A. Bekey, Autonomous Robots, MIT Press, Cambridge, MA, 2005.
• C. Canudas de Wit, B. Siciliano, G. Bastin, (Eds.), Theory of Robot Control , Springer-Verlag, London, 1996.
• J.J. Craig, Introduction to Robotics: Mechanics and Control , 3rd ed., Pearson Prentice Hall, Upper Saddle River, NJ, 2004.
• A.J. Critchlow, Introduction to Robotics, Macmillan, New York, 1985.
• J.F. Engelberger, Robotics in Practice, Amacom, New York, 1980.
• J.F. Engelberger, Robotics in Service, MIT Press, Cambridge, MA, 1989.
• K.S. Fu, R.C. Gonzalez, C.S.G. Lee, Robotics: Control, Sensing, Vision,
and Intelligence, McGraw-Hill, New York, 1987.
• W. Khalil, E. Dombre, Modeling, Identiﬁcation and Control of Robots,
Hermes Penton Ltd, London, 2002.
• A.J. Koivo, Fundamentals for Control of Robotic Manipulators, Wiley,
New York, 1989.
• Y. Koren, Robotics for Engineers, McGraw-Hill, New York, 1985.
• F.L. Lewis, C.T. Abdallah, D.M. Dawson, Control of Robot Manipulators,
Macmillan, New York, 1993.
• P.J. McKerrow, Introduction to Robotics, Addison-Wesley, Sydney, Australia, 1991.
• R.M. Murray, Z. Li, S.S. Sastry, A Mathematical Introduction to Robotic
Manipulation, CRC Press, Boca Raton, FL, 1994.
• S.B. Niku, Introduction to Robotics: Analysis, Systems, Applications,
Prentice-Hall, Upper Saddle River, NJ, 2001.
• R.P. Paul, Robot Manipulators: Mathematics, Programming, and Control
MIT Press, Cambridge, MA, 1981.
• R.J. Schilling, Fundamentals of Robotics: Analysis and Control , PrenticeHall, Englewood Cliﬀs, NJ, 1990.
• L. Sciavicco, B. Siciliano, Modelling and Control of Robot Manipulators,
2nd ed., Springer, London, UK, 2000.
• W.E. Snyder, Industrial Robots: Computer Interfacing and Control , Prentice-Hall, Englewood Cliﬀs, NJ, 1985.

Bibliography

35

• M.W. Spong, S. Hutchinson, M. Vidyasagar, Robot Modeling and Control ,
Wiley, New York, 2006.
• M. Vukobratović, Introduction to Robotics, Springer-Verlag, Berlin, Germany, 1989.
• T. Yoshikawa, Foundations of Robotics, MIT Press, Boston, MA, 1990.
Specialized books
Topics of related interest to robot modelling, planning and control are:
•
•
•
•
•
•
•
•
•
•
•
•
•

manipulator mechanical design,
manipulation tools,
manipulators with elastic members,
parallel robots,
locomotion apparatus,
mobile robots,
underwater and space robots,
control architectures
motion and force control,
robot vision,
multisensory data fusion,
telerobotics,
human-robot interaction.

The following books are dedicated to these topics:
• G. Antonelli, Underwater Robots: Motion and Force Control of VehicleManipulator Systems, 2nd ed., Springer, Heidelberg, Germany, 2006.
• R.C. Arkin, Behavior-Based Robotics, MIT Press, Cambridge, MA, 1998.
• J. Baeten, J. De Schutter, Integrated Visual Servoing and Force Control:
The Task Frame Approach, Springer, Heidelberg, Germany, 2003.
• M. Buehler, K. Iagnemma, S. Singh, (Eds.), The 2005 DARPA Grand
Challenge: The Great Robot Race, Springer, Heidelberg, Germany, 2007.
• J.F. Canny, The Complexity of Robot Motion Planning, MIT Press, Cambridge, MA, 1988.
• H. Choset, K.M. Lynch, S. Hutchinson, G. Kantor, W. Burgard, L.E.
Kavraki, S. Thrun, Principles of Robot Motion: Theory, Algorithms, and
Implementations, MIT Press, Cambridge, MA, 2005.
• P.I. Corke, Visual Control of Robots: High-Performance Visual Servoing,
Research Studies Press, Taunton, UK, 1996.
• M.R. Cutkosky, Robotic Grasping and Fine Manipulation, Kluwer, Boston,
MA, 1985.
• H.F. Durrant-Whyte, Integration, Coordination and Control of MultiSensor Robot Systems, Kluwer, Boston, MA, 1988.
• A. Ellery, An Introduction to Space Robotics, Springer-Verlag, London,
UK, 2000.

36

1 Introduction

• A.R. Fraser, R.W. Daniel, Perturbation Techniques for Flexible Manipulators, Kluwer, Boston, MA, 1991.
• B.K. Ghosh, N. Xi, T.-J. Tarn, (Eds.), Control in Robotics and Automation: Sensor-Based Integration, Academic Press, San Diego, CA, 1999.
• K. Goldberg, (Ed.), The Robot in the Garden: Telerobotics and Telepistemology in the Age of the Internet, MIT Press, Cambridge, MA, 2000.
• S. Hirose, Biologically Inspired Robots, Oxford University Press, Oxford,
UK, 1993.
• B.K.P. Horn, Robot Vision, McGraw-Hill, New York, 1986.
• K. Iagnemma, S. Dubowsky, Mobile Robots in Rough Terrain Estimation:
Motion Planning, and Control with Application to Planetary Rovers Series, Springer, Heidelberg, Germany, 2004.
• R. Kelly, V. Santibañez, A. Lorı́a, Control of Robot Manipulators in Joint
Space, Springer-Verlag, London, UK, 2005.
• J.-C. Latombe, Robot Motion Planning, Kluwer, Boston, MA, 1991.
• M.T. Mason, Mechanics of Robotic Manipulation, MIT Press, Cambridge,
MA, 2001.
• M.T. Mason, J.K. Salisbury, Robot Hands and the Mechanics of Manipulation, MIT Press, Cambridge, MA, 1985.
• J.-P. Merlet, Parallel Robots, 2nd ed., Springer, Dordrecht, The Netherlands, 2006.
• R.R. Murphy, Introduction to AI Robotics, MIT Press, Cambridge, MA,
2000.
• C. Natale, Interaction Control of Robot Manipulators: Six-degrees-offreedom Tasks, Springer, Heidelberg, Germany, 2003.
• M. Raibert, Legged Robots that Balance, MIT Press, Cambridge, MA, 1985.
• E.I. Rivin, Mechanical Design of Robots, McGraw-Hill, New York, 1987.
• B. Siciliano, L. Villani, Robot Force Control , Kluwer, Boston, MA, 2000.
• R. Siegwart, Introduction to Autonomous Mobile Robots, MIT Press, Cambridge, MA, 2004.
• S. Thrun, W. Burgard, D. Fox, Probabilistic Robotics, MIT Press, Cambridge, MA, 2005.
• D.J. Todd, Walking Machines, an Introduction to Legged Robots, Chapman
Hall, London, UK, 1985.
• L.-W. Tsai, Robot Analysis: The Mechanics of Serial and Parallel Manipulators, Wiley, New York, 1999.
Edited collections on the state of the art of research
• M. Brady, (Ed.), Robotics Science, MIT Press, Cambridge, MA, 1989.
• M. Brady, J.M. Hollerbach, T.L. Johnson, T. Lozano-Pérez, M.T. Mason,
(Eds.), Robot Motion: Planning and Control , MIT Press, Cambridge, MA,
1982.
• R.C. Dorf, International Encyclopedia of Robotics, Wiley, New York, 1988.

Bibliography

37

• V.D. Hunt, Industrial Robotics Handbook , Industrial Press, New York,
1983.
• O. Khatib, J.J. Craig, T. Lozano-Pérez, (Eds.), The Robotics Review 1 ,
MIT Press, Cambridge, MA, 1989.
• O. Khatib, J.J. Craig, T. Lozano-Pérez, (Eds.), The Robotics Review 2 ,
MIT Press, Cambridge, MA., 1992.
• T.R. Kurfess, (Ed.), Robotics and Automation Handbook , CRC Press, Boca
Raton, FL, 2005.
• B. Siciliano, O. Khatib, (Eds.), Springer Handbook of Robotics, Springer,
Heidelberg, Germany, 2008.
• C.S.G. Lee, R.C. Gonzalez, K.S. Fu, (Eds.), Tutorial on Robotics, 2nd ed.,
IEEE Computer Society Press, Silver Spring, MD, 1986.
• M.W. Spong, F.L. Lewis, C.T. Abdallah, (Eds.), Robot Control: Dynamics,
Motion Planning, and Analysis, IEEE Press, New York, 1993.
Scientiﬁc journals
•
•
•
•
•
•
•
•
•

Advanced Robotics
Autonomous Robots
IEEE Robotics and Automation Magazine
IEEE Transactions on Robotics
International Journal of Robotics Research
Journal of Field Robotics
Journal of Intelligent and Robotic Systems
Robotica
Robotics and Autonomous Systems

Series of international scientiﬁc conferences
•
•
•
•
•
•

IEEE International Conference on Robotics and Automation
IEEE/RSJ International Conference on Intelligent Robots and Systems
International Conference on Advanced Robotics
International Symposium of Robotics Research
International Symposium on Experimental Robotics
Robotics: Science and Systems

The above journals and conferences represent the reference sources for the
international scientiﬁc community. Many other robotics journals and conferences exist which are devoted to speciﬁc topics, such as kinematics, control, vision, algorithms, haptics, industrial applications, space and underwater exploration, humanoid robotics, and human-robot interaction. On the other hand,
several journals and prestigious conferences in other ﬁelds, such as mechanics, control, sensors, and artiﬁcial intelligence, oﬀer generous space to robotics
topics.

2
Kinematics

A manipulator can be schematically represented from a mechanical viewpoint
as a kinematic chain of rigid bodies (links) connected by means of revolute
or prismatic joints. One end of the chain is constrained to a base, while an
end-eﬀector is mounted to the other end. The resulting motion of the structure is obtained by composition of the elementary motions of each link with
respect to the previous one. Therefore, in order to manipulate an object in
space, it is necessary to describe the end-eﬀector position and orientation.
This chapter is dedicated to the derivation of the direct kinematics equation
through a systematic, general approach based on linear algebra. This allows
the end-eﬀector position and orientation (pose) to be expressed as a function
of the joint variables of the mechanical structure with respect to a reference
frame. Both open-chain and closed-chain kinematic structures are considered.
With reference to a minimal representation of orientation, the concept of
operational space is introduced and its relationship with the joint space is established. Furthermore, a calibration technique of the manipulator kinematic
parameters is presented. The chapter ends with the derivation of solutions to
the inverse kinematics problem, which consists of the determination of the
joint variables corresponding to a given end-eﬀector pose.

2.1 Pose of a Rigid Body
A rigid body is completely described in space by its position and orientation
(in brief pose) with respect to a reference frame. As shown in Fig. 2.1, let
O–xyz be the orthonormal reference frame and x, y, z be the unit vectors of
the frame axes.
The position of a point O on the rigid body with respect to the coordinate
frame O–xyz is expressed by the relation
o = ox x + oy y + oz z,

40

2 Kinematics

Fig. 2.1. Position and orientation of a rigid body

where ox , oy , oz denote the components of the vector o ∈ IR3 along the frame
axes; the position of O can be compactly written as the (3 × 1) vector
⎡  ⎤
ox
o = ⎣ oy ⎦ .
(2.1)

oz
Vector o is a bound vector since its line of application and point of application
are both prescribed, in addition to its direction and norm.
In order to describe the rigid body orientation, it is convenient to consider
an orthonormal frame attached to the body and express its unit vectors with
respect to the reference frame. Let then O –x y  z  be such a frame with origin
in O and x , y  , z  be the unit vectors of the frame axes. These vectors are
expressed with respect to the reference frame O–xyz by the equations:
x = xx x + xy y + xz z
y  = yx x + yy y + yz z


z =

zx x

+

zy y

+

(2.2)

zz z.

The components of each unit vector are the direction cosines of the axes of
frame O –x y  z  with respect to the reference frame O–xyz.

2.2 Rotation Matrix
By adopting a compact notation, the three unit vectors in (2.2) describing the
body orientation with respect to the reference frame can be combined in the
(3 × 3) matrix
⎡
⎤ ⎡ 
⎤ ⎡ T
⎤
xx yx zx
x x y T x z T x
R = ⎣ x y  z  ⎦ = ⎣ xy yy zy ⎦ = ⎣ xT y y T y z T y ⎦ ,
(2.3)



T
T
T
x z y z z z
xz yz zz

2.2 Rotation Matrix

41

which is termed rotation matrix .
It is worth noting that the column vectors of matrix R are mutually orthogonal since they represent the unit vectors of an orthonormal frame, i.e.,
xT y  = 0

y T z  = 0

z T x = 0.

y T y  = 1

z T z  = 1.

Also, they have unit norm
xT x = 1

As a consequence, R is an orthogonal matrix meaning that
RT R = I 3

(2.4)

where I 3 denotes the (3 × 3) identity matrix.
If both sides of (2.4) are postmultiplied by the inverse matrix R−1 , the
useful result is obtained:
(2.5)
RT = R−1 ,
that is, the transpose of the rotation matrix is equal to its inverse. Further,
observe that det(R) = 1 if the frame is right-handed, while det(R) = −1 if
the frame is left-handed.
The above-deﬁned rotation matrix belongs to the special orthonormal
group SO(m) of the real (m × m) matrices with othonormal columns and
determinant equal to 1; in the case of spatial rotations it is m = 3, whereas
in the case of planar rotations it is m = 2.
2.2.1 Elementary Rotations
Consider the frames that can be obtained via elementary rotations of the
reference frame about one of the coordinate axes. These rotations are positive
if they are made counter-clockwise about the relative axis.
Suppose that the reference frame O–xyz is rotated by an angle α about
axis z (Fig. 2.2), and let O–x y  z  be the rotated frame. The unit vectors of
the new frame can be described in terms of their components with respect
to the reference frame. Consider the frames that can be obtained via elementary rotations of the reference frame about one of the coordinate axes. These
rotations are positive if they are made counter-clockwise about the relative
axis.
Suppose that the reference frame O–xyz is rotated by an angle α about
axis z (Fig. 2.2), and let O–x y  z  be the rotated frame. The unit vectors of
the new frame can be described in terms of their components with respect to
the reference frame, i.e.,
⎡
⎤
⎡
⎤
⎡ ⎤
cos α
−sin α
0
y  = ⎣ cos α ⎦
z = ⎣ 0 ⎦ .
x = ⎣ sin α ⎦
0
0
1

42

2 Kinematics

Fig. 2.2. Rotation of frame O–xyz by an angle α about axis z

Hence, the rotation matrix of frame O–x y  z  with respect to frame O–xyz is
⎡
⎤
cos α −sin α 0
Rz (α) = ⎣ sin α cos α 0 ⎦ .
(2.6)
0
0
1
In a similar manner, it can be shown that the rotations by an angle β
about axis y and by an angle γ about axis x are respectively given by
⎡
⎤
cos β 0 sin β
Ry (β) = ⎣ 0
1
0 ⎦
(2.7)
−sin β 0 cos β
⎡
⎤
1
0
0
Rx (γ) = ⎣ 0 cos γ −sin γ ⎦ .
(2.8)
0 sin γ cos γ
These matrices will be useful to describe rotations about an arbitrary axis in
space.
It is easy to verify that for the elementary rotation matrices in (2.6)–(2.8)
the following property holds:
Rk (−ϑ) = RTk (ϑ)

k = x, y, z.

(2.9)

In view of (2.6)–(2.8), the rotation matrix can be attributed a geometrical
meaning; namely, the matrix R describes the rotation about an axis in space
needed to align the axes of the reference frame with the corresponding axes
of the body frame.
2.2.2 Representation of a Vector
In order to understand a further geometrical meaning of a rotation matrix,
consider the case when the origin of the body frame coincides with the origin

2.2 Rotation Matrix

43

Fig. 2.3. Representation of a point P in two diﬀerent coordinate frames

of the reference frame (Fig. 2.3); it follows that o = 0, where 0 denotes the
(3 × 1) null vector. A point P in space can be represented either as
⎡ ⎤
px
p = ⎣ py ⎦
pz
with respect to frame O–xyz, or as
⎤
px
p = ⎣ py ⎦
pz
⎡

with respect to frame O–x y  z  .
Since p and p are representations of the same point P , it is
⎤
⎡
p = px x + py y  + pz z  = ⎣ x

y

z  ⎦ p

and, accounting for (2.3), it is
p = Rp .

(2.10)

The rotation matrix R represents the transformation matrix of the vector
coordinates in frame O–x y  z  into the coordinates of the same vector in frame
O–xyz. In view of the orthogonality property (2.4), the inverse transformation
is simply given by
(2.11)
p = RT p.

44

2 Kinematics

Fig. 2.4. Representation of a point P in rotated frames

Example 2.1
Consider two frames with common origin mutually rotated by an angle α about
the axis z. Let p and p be the vectors of the coordinates of a point P , expressed
in the frames O–xyz and O–x y  z  , respectively (Fig. 2.4). On the basis of simple
geometry, the relationship between the coordinates of P in the two frames is
px = px cos α − py sin α
py = px sin α + py cos α
pz = pz .
Therefore, the matrix (2.6) represents not only the orientation of a frame with
respect to another frame, but it also describes the transformation of a vector from
a frame to another frame with the same origin.

2.2.3 Rotation of a Vector
A rotation matrix can be also interpreted as the matrix operator allowing
rotation of a vector by a given angle about an arbitrary axis in space. In fact,
let p be a vector in the reference frame O–xyz; in view of orthogonality of the
matrix R, the product Rp yields a vector p with the same norm as that of p
but rotated with respect to p according to the matrix R. The norm equality
can be proved by observing that pT p = pT RT Rp and applying (2.4). This
interpretation of the rotation matrix will be revisited later.

2.3 Composition of Rotation Matrices

45

Fig. 2.5. Rotation of a vector

Example 2.2
Consider the vector p which is obtained by rotating a vector p in the plane xy by
an angle α about axis z of the reference frame (Fig. 2.5). Let (px , py , pz ) be the
coordinates of the vector p . The vector p has components
px = px cos α − py sin α
py = px sin α + py cos α
pz = pz .
It is easy to recognize that p can be expressed as
p = Rz (α)p ,
where Rz (α) is the same rotation matrix as in (2.6).

In sum, a rotation matrix attains three equivalent geometrical meanings:
• It describes the mutual orientation between two coordinate frames; its
column vectors are the direction cosines of the axes of the rotated frame
with respect to the original frame.
• It represents the coordinate transformation between the coordinates of a
point expressed in two diﬀerent frames (with common origin).
• It is the operator that allows the rotation of a vector in the same coordinate
frame.

2.3 Composition of Rotation Matrices
In order to derive composition rules of rotation matrices, it is useful to consider
the expression of a vector in two diﬀerent reference frames. Let then O–x0 y0 z0 ,

46

2 Kinematics

O–x1 y1 z1 , O–x2 y2 z2 be three frames with common origin O. The vector p
describing the position of a generic point in space can be expressed in each
of the above frames; let p0 , p1 , p2 denote the expressions of p in the three
frames.1
At ﬁrst, consider the relationship between the expression p2 of the vector
p in Frame 2 and the expression p1 of the same vector in Frame 1. If Rji
denotes the rotation matrix of Frame i with respect to Frame j, it is
p1 = R12 p2 .

(2.12)

p0 = R01 p1

(2.13)

Similarly, it turns out that
0

p =

R02 p2 .

(2.14)

On the other hand, substituting (2.12) in (2.13) and using (2.14) gives
R02 = R01 R12 .

(2.15)

The relationship in (2.15) can be interpreted as the composition of successive
rotations. Consider a frame initially aligned with the frame O–x0 y0 z0 . The
rotation expressed by matrix R02 can be regarded as obtained in two steps:
• First rotate the given frame according to R01 , so as to align it with frame
O–x1 y1 z1 .
• Then rotate the frame, now aligned with frame O–x1 y1 z1 , according to
R12 , so as to align it with frame O–x2 y2 z2 .
Notice that the overall rotation can be expressed as a sequence of partial
rotations; each rotation is deﬁned with respect to the preceding one. The
frame with respect to which the rotation occurs is termed current frame.
Composition of successive rotations is then obtained by postmultiplication of
the rotation matrices following the given order of rotations, as in (2.15). With
the adopted notation, in view of (2.5), it is
Rji = (Rij )−1 = (Rij )T .

(2.16)

Successive rotations can be also speciﬁed by constantly referring them
to the initial frame; in this case, the rotations are made with respect to a
ﬁxed frame. Let R01 be the rotation matrix of frame O–x1 y1 z1 with respect
0
to the ﬁxed frame O–x0 y0 z0 . Let then R̄2 denote the matrix characterizing
frame O–x2 y2 z2 with respect to Frame 0, which is obtained as a rotation of
1
Frame 1 according to the matrix R̄2 . Since (2.15) gives a composition rule of
successive rotations about the axes of the current frame, the overall rotation
can be regarded as obtained in the following steps:
1

Hereafter, the superscript of a vector or a matrix denotes the frame in which its
components are expressed.

2.3 Composition of Rotation Matrices

47

• First realign Frame 1 with Frame 0 by means of rotation R10 .
1
• Then make the rotation expressed by R̄2 with respect to the current frame.
• Finally compensate for the rotation made for the realignment by means of
the inverse rotation R01 .
Since the above rotations are described with respect to the current frame, the
application of the composition rule (2.15) yields
0

1

R̄2 = R01 R10 R̄2 R01 .
In view of (2.16), it is
0

1

R̄2 = R̄2 R01

(2.17)

0

where the resulting R̄2 is diﬀerent from the matrix R02 in (2.15). Hence, it
can be stated that composition of successive rotations with respect to a ﬁxed
frame is obtained by premultiplication of the single rotation matrices in the
order of the given sequence of rotations.
By recalling the meaning of a rotation matrix in terms of the orientation
of a current frame with respect to a ﬁxed frame, it can be recognized that its
columns are the direction cosines of the axes of the current frame with respect
to the ﬁxed frame, while its rows (columns of its transpose and inverse) are
the direction cosines of the axes of the ﬁxed frame with respect to the current
frame.
An important issue of composition of rotations is that the matrix product
is not commutative. In view of this, it can be concluded that two rotations
in general do not commute and its composition depends on the order of the
single rotations.

Example 2.3
Consider an object and a frame attached to it. Figure 2.6 shows the eﬀects of two
successive rotations of the object with respect to the current frame by changing the
order of rotations. It is evident that the ﬁnal object orientation is diﬀerent in the two
cases. Also in the case of rotations made with respect to the current frame, the ﬁnal
orientations diﬀer (Fig. 2.7). It is interesting to note that the eﬀects of the sequence
of rotations with respect to the ﬁxed frame are interchanged with the eﬀects of the
sequence of rotations with respect to the current frame. This can be explained by
observing that the order of rotations in the ﬁxed frame commutes with respect to
the order of rotations in the current frame.

48

2 Kinematics

Fig. 2.6. Successive rotations of an object about axes of current frame

Fig. 2.7. Successive rotations of an object about axes of ﬁxed frame

2.4 Euler Angles
Rotation matrices give a redundant description of frame orientation; in fact,
they are characterized by nine elements which are not independent but related
by six constraints due to the orthogonality conditions given in (2.4). This implies that three parameters are suﬃcient to describe orientation of a rigid body

2.4 Euler Angles

49

Fig. 2.8. Representation of Euler angles ZYZ

in space. A representation of orientation in terms of three independent parameters constitutes a minimal representation. In fact, a minimal representation
of the special orthonormal group SO(m) requires m(m − 1)/2 parameters;
thus, three parameters are needed to parameterize SO(3), whereas only one
parameter is needed for a planar rotation SO(2).
A minimal representation of orientation can be obtained by using a set
T
of three angles φ = [ ϕ ϑ ψ ] . Consider the rotation matrix expressing
the elementary rotation about one of the coordinate axes as a function of a
single angle. Then, a generic rotation matrix can be obtained by composing a
suitable sequence of three elementary rotations while guaranteeing that two
successive rotations are not made about parallel axes. This implies that 12
distinct sets of angles are allowed out of all 27 possible combinations; each
set represents a triplet of Euler angles. In the following, two sets of Euler
angles are analyzed; namely, the ZYZ angles and the ZYX (or Roll–Pitch–
Yaw) angles.
2.4.1 ZYZ Angles
The rotation described by ZYZ angles is obtained as composition of the following elementary rotations (Fig. 2.8):
• Rotate the reference frame by the angle ϕ about axis z; this rotation is
described by the matrix Rz (ϕ) which is formally deﬁned in (2.6).
• Rotate the current frame by the angle ϑ about axis y  ; this rotation is
described by the matrix Ry (ϑ) which is formally deﬁned in (2.7).
• Rotate the current frame by the angle ψ about axis z  ; this rotation is
described by the matrix Rz (ψ) which is again formally deﬁned in (2.6).

50

2 Kinematics

The resulting frame orientation is obtained by composition of rotations
with respect to current frames, and then it can be computed via postmultiplication of the matrices of elementary rotation, i.e.,2
R(φ) = Rz (ϕ)Ry (ϑ)Rz (ψ)
⎡
cϕ cϑ cψ − sϕ sψ −cϕ cϑ sψ − sϕ cψ
= ⎣ sϕ cϑ cψ + cϕ sψ −sϕ cϑ sψ + cϕ cψ
−sϑ cψ
sϑ sψ

⎤

(2.18)

cϕ sϑ
sϕ sϑ ⎦ .
cϑ

It is useful to solve the inverse problem, that is to determine the set of
Euler angles corresponding to a given rotation matrix
⎤
⎡
r11 r12 r13
R = ⎣ r21 r22 r23 ⎦ .
r31 r32 r33
Compare this expression with that of R(φ) in (2.18). By considering the
elements [1, 3] and [2, 3], under the assumption that r13 = 0 and r23 = 0, it
follows that
ϕ = Atan2(r23 , r13 )
where Atan2(y, x) is the arctangent function of two arguments3 . Then, squaring and summing the elements [1, 3] and [2, 3] and using the element [3, 3]
yields


2 + r2 , r
ϑ = Atan2
r13
23 33 .
2 + r 2 limits the range of
The choice of the positive sign for the term r13
23
feasible values of ϑ to (0, π). On this assumption, considering the elements
[3, 1] and [3, 2] gives
ψ = Atan2(r32 , −r31 ).

In sum, the requested solution is
ϕ = Atan2(r23 , r13 )


2 + r2 , r
ϑ = Atan2
r13
23 33

(2.19)

ψ = Atan2(r32 , −r31 ).
It is possible to derive another solution which produces the same eﬀects as
solution (2.19). Choosing ϑ in the range (−π, 0) leads to
ϕ = Atan2(−r23 , −r13 )
2
3

The notations cφ and sφ are the abbreviations for cos φ and sin φ, respectively;
short-hand notations of this kind will be adopted often throughout the text.
The function Atan2(y, x) computes the arctangent of the ratio y/x but utilizes the
sign of each argument to determine which quadrant the resulting angle belongs
to; this allows the correct determination of an angle in a range of 2π.

2.4 Euler Angles

51

Fig. 2.9. Representation of Roll–Pitch–Yaw angles

 

2 + r2 , r
ϑ = Atan2 − r13
23 33

(2.20)

ψ = Atan2(−r32 , r31 ).
Solutions (2.19), (2.20) degenerate when sϑ = 0; in this case, it is possible
to determine only the sum or diﬀerence of ϕ and ψ. In fact, if ϑ = 0, π,
the successive rotations of ϕ and ψ are made about axes of current frames
which are parallel, thus giving equivalent contributions to the rotation; see
Problem 2.2.4
2.4.2 RPY Angles
Another set of Euler angles originates from a representation of orientation in
the (aero)nautical ﬁeld. These are the ZYX angles, also called Roll–Pitch–
Yaw angles, to denote the typical changes of attitude of an (air)craft. In this
T
case, the angles φ = [ ϕ ϑ ψ ] represent rotations deﬁned with respect to
a ﬁxed frame attached to the centre of mass of the craft (Fig. 2.9).
The rotation resulting from Roll–Pitch–Yaw angles can be obtained as
follows:
• Rotate the reference frame by the angle ψ about axis x (yaw); this rotation
is described by the matrix Rx (ψ) which is formally deﬁned in (2.8).
• Rotate the reference frame by the angle ϑ about axis y (pitch); this rotation
is described by the matrix Ry (ϑ) which is formally deﬁned in (2.7).
• Rotate the reference frame by the angle ϕ about axis z (roll); this rotation
is described by the matrix Rz (ϕ) which is formally deﬁned in (2.6).
4

In the following chapter, it will be seen that these conﬁgurations characterize the
so-called representation singularities of the Euler angles.

52

2 Kinematics

The resulting frame orientation is obtained by composition of rotations with
respect to the ﬁxed frame, and then it can be computed via premultiplication
of the matrices of elementary rotation, i.e.,5
R(φ) = Rz (ϕ)Ry (ϑ)Rx (ψ)
⎡
cϕ cϑ cϕ sϑ sψ − sϕ cψ
= ⎣ sϕ cϑ sϕ sϑ sψ + cϕ cψ
−sϑ
cϑ sψ

⎤
cϕ sϑ cψ + sϕ sψ
sϕ sϑ cψ − cϕ sψ ⎦ .
cϑ cψ

(2.21)

As for the Euler angles ZYZ, the inverse solution to a given rotation matrix
⎡
⎤
r11 r12 r13
R = ⎣ r21 r22 r23 ⎦ ,
r31 r32 r33
can be obtained by comparing it with the expression of R(φ) in (2.21). The
solution for ϑ in the range (−π/2, π/2) is
ϕ = Atan2(r21 , r11 )



2
2
ϑ = Atan2 −r31 , r32 + r33

(2.22)

ψ = Atan2(r32 , r33 ).
The other equivalent solution for ϑ in the range (π/2, 3π/2) is
ϕ = Atan2(−r21 , −r11 )



2 + r2
ϑ = Atan2 −r31 , − r32
33

(2.23)

ψ = Atan2(−r32 , −r33 ).
Solutions (2.22), (2.23) degenerate when cϑ = 0; in this case, it is possible to
determine only the sum or diﬀerence of ϕ and ψ.

2.5 Angle and Axis
A nonminimal representation of orientation can be obtained by resorting to
four parameters expressing a rotation of a given angle about an axis in space.
This can be advantageous in the problem of trajectory planning for a manipulator’s end-eﬀector orientation.
Let r = [ rx ry rz ]T be the unit vector of a rotation axis with respect
to the reference frame O–xyz. In order to derive the rotation matrix R(ϑ, r)
expressing the rotation of an angle ϑ about axis r, it is convenient to compose
5

The ordered sequence of rotations XYZ about axes of the ﬁxed frame is equivalent
to the sequence ZYX about axes of the current frame.

2.5 Angle and Axis

53

Fig. 2.10. Rotation of an angle about an axis

elementary rotations about the coordinate axes of the reference frame. The
angle is taken to be positive if the rotation is made counter-clockwise about
axis r.
As shown in Fig. 2.10, a possible solution is to rotate ﬁrst r by the angles
necessary to align it with axis z, then to rotate by ϑ about z and ﬁnally
to rotate by the angles necessary to align the unit vector with the initial
direction. In detail, the sequence of rotations, to be made always with respect
to axes of ﬁxed frame, is the following:
• Align r with z, which is obtained as the sequence of a rotation by −α
about z and a rotation by −β about y.
• Rotate by ϑ about z.
• Realign with the initial direction of r, which is obtained as the sequence
of a rotation by β about y and a rotation by α about z.
In sum, the resulting rotation matrix is
R(ϑ, r) = Rz (α)Ry (β)Rz (ϑ)Ry (−β)Rz (−α).

(2.24)

From the components of the unit vector r it is possible to extract the transcendental functions needed to compute the rotation matrix in (2.24), so as
to eliminate the dependence from α and β; in fact, it is
sin α = 

ry
rx2

+


sin β =

ry2

rx2 + ry2

cos α = 

rx
rx2

+ ry2

cos β = rz .

54

2 Kinematics

Then, it can be found that the rotation matrix corresponding to a given angle
and axis is — see Problem 2.4 —
⎤
⎡
rx ry (1 − cϑ ) − rz sϑ rx rz (1 − cϑ ) + ry sϑ
rx2 (1 − cϑ ) + cϑ
⎥
⎢
R(ϑ, r) = ⎣ rx ry (1 − cϑ ) + rz sϑ
ry2 (1 − cϑ ) + cϑ
ry rz (1 − cϑ ) − rx sϑ ⎦.
rx rz (1 − cϑ ) − ry sϑ ry rz (1 − cϑ ) + rx sϑ
rz2 (1 − cϑ ) + cϑ
(2.25)
For this matrix, the following property holds:
R(−ϑ, −r) = R(ϑ, r),

(2.26)

i.e., a rotation by −ϑ about −r cannot be distinguished from a rotation by ϑ
about r; hence, such representation is not unique.
If it is desired to solve the inverse problem to compute the axis and angle
corresponding to a given rotation matrix
⎤
⎡
r11 r12 r13
R = ⎣ r21 r22 r23 ⎦ ,
r31 r32 r33
the following result is useful:



r11 + r22 + r33 − 1
ϑ = cos
2
⎡
⎤
r32 − r23
1 ⎣
r13 − r31 ⎦ ,
r=
2 sin ϑ
r21 − r12
−1


(2.27)
(2.28)

for sin ϑ = 0. Notice that the expressions (2.27), (2.28) describe the rotation
in terms of four parameters; namely, the angle and the three components of
the axis unit vector. However, it can be observed that the three components
of r are not independent but are constrained by the condition
rx2 + ry2 + rz2 = 1.

(2.29)

If sin ϑ = 0, the expressions (2.27), (2.28) become meaningless. To solve the
inverse problem, it is necessary to directly refer to the particular expressions
attained by the rotation matrix R and ﬁnd the solving formulae in the two
cases ϑ = 0 and ϑ = π. Notice that, when ϑ = 0 (null rotation), the unit
vector r is arbitrary (singularity). See also Problem 2.5.

2.6 Unit Quaternion
The drawbacks of the angle/axis representation can be overcome by a different four-parameter representation; namely, the unit quaternion, viz. Euler
parameters, deﬁned as Q = {η, } where:
η = cos

ϑ
2

(2.30)

2.6 Unit Quaternion

55

ϑ
r;
(2.31)
2
η is called the scalar part of the quaternion while  = [ x y z ]T is called
the vector part of the quaternion. They are constrained by the condition
 = sin

η2 +

2
x

+

2
y

+

2
z

= 1,

(2.32)

hence, the name unit quaternion. It is worth remarking that, unlike the angle/axis representation, a rotation by −ϑ about −r gives the same quaternion as that associated with a rotation by ϑ about r; this solves the above
nonuniqueness problem. In view of (2.25), (2.30), (2.31), (2.32), the rotation
matrix corresponding to a given quaternion takes on the form — see Problem 2.6 —
⎤
⎡
2(η 2 + 2x ) − 1 2( x y − η z ) 2( x z + η y )
⎥
⎢
R(η, ) = ⎣ 2( x y + η z ) 2(η 2 + 2y ) − 1 2( y z − η x ) ⎦ .
(2.33)
2(

x z

− η y)

2(

y z

+ η x)

2(η 2 +

2
z)

−1

If it is desired to solve the inverse problem to compute the quaternion
corresponding to a given rotation matrix
⎤
⎡
r11 r12 r13
R = ⎣ r21 r22 r23 ⎦ ,
r31 r32 r33
the following result is useful:
1√
η=
r11 + r22 + r33 + 1
2⎡
√
⎤
sgn (r32 − r23 ) r11 − r22 − r33 + 1
√
1⎢
⎥
 = ⎣ sgn (r13 − r31 ) r22 − r33 − r11 + 1 ⎦ ,
2
√
sgn (r21 − r12 ) r33 − r11 − r22 + 1

(2.34)

(2.35)

where conventionally sgn (x) = 1 for x ≥ 0 and sgn (x) = −1 for x < 0. Notice
that in (2.34) it has been implicitly assumed η ≥ 0; this corresponds to an
angle ϑ ∈ [−π, π], and thus any rotation can be described. Also, compared to
the inverse solution in (2.27), (2.28) for the angle and axis representation, no
singularity occurs for (2.34), (2.35). See also Problem 2.8.
The quaternion extracted from R−1 = RT is denoted as Q−1 , and can be
computed as
(2.36)
Q−1 = {η, −}.
Let Q1 = {η1 , 1 } and Q2 = {η2 , 2 } denote the quaternions corresponding
to the rotation matrices R1 and R2 , respectively. The quaternion corresponding to the product R1 R2 is given by
Q1 ∗ Q2 = {η1 η2 − T1 2 , η1 2 + η2 1 + 1 × 2 }

(2.37)

where the quaternion product operator “∗” has been formally introduced. It is
easy to see that if Q2 = Q−1
1 then the quaternion {1, 0} is obtained from (2.37)
which is the identity element for the product. See also Problem 2.9.

56

2 Kinematics

Fig. 2.11. Representation of a point P in diﬀerent coordinate frames

2.7 Homogeneous Transformations
As illustrated at the beginning of the chapter, the position of a rigid body in
space is expressed in terms of the position of a suitable point on the body with
respect to a reference frame (translation), while its orientation is expressed in
terms of the components of the unit vectors of a frame attached to the body
— with origin in the above point — with respect to the same reference frame
(rotation).
As shown in Fig. 2.11, consider an arbitrary point P in space. Let p0
be the vector of coordinates of P with respect to the reference frame O0 –
x0 y0 z0 . Consider then another frame in space O1 –x1 y1 z1 . Let o01 be the vector
describing the origin of Frame 1 with respect to Frame 0, and R01 be the
rotation matrix of Frame 1 with respect to Frame 0. Let also p1 be the vector
of coordinates of P with respect to Frame 1. On the basis of simple geometry,
the position of point P with respect to the reference frame can be expressed
as
(2.38)
p0 = o01 + R01 p1 .
Hence, (2.38) represents the coordinate transformation (translation + rotation) of a bound vector between two frames.
The inverse transformation can be obtained by premultiplying both sides
of (2.38) by R01 T ; in view of(2.4), it follows that
p1 = −R01 T o01 + R01 T p0

(2.39)

which, via (2.16), can be written as
p1 = −R10 o01 + R10 p0 .

(2.40)

In order to achieve a compact representation of the relationship between
the coordinates of the same point in two diﬀerent frames, the homogeneous
representation of a generic vector p can be introduced as the vector p̃ formed
by adding a fourth unit component, i.e.,

2.7 Homogeneous Transformations

57

⎡ ⎤
⎢p⎥
p = ⎣ ⎦.

(2.41)

1
By adopting this representation for the vectors p0 and p1 in (2.38), the coordinate transformation can be written in terms of the (4 × 4) matrix
⎡
⎤
⎢
A01 = ⎣

R01
T

0

o01 ⎥
⎦

(2.42)

1

which, according to (2.41), is termed homogeneous transformation matrix .
Since o01 ∈ IR3 e R01 ∈ SO(3), this matrix belongs to the special Euclidean
group SE(3) = IR3 × SO(3).
As can be easily seen from (2.42), the transformation of a vector from
Frame 1 to Frame 0 is expressed by a single matrix containing the rotation
matrix of Frame 1 with respect to Frame 0 and the translation vector from
the origin of Frame 0 to the origin of Frame 1.6 Therefore, the coordinate
transformation (2.38) can be compactly rewritten as
p0 = A01 p1 .

(2.43)

The coordinate transformation between Frame 0 and Frame 1 is described
by the homogeneous transformation matrix A10 which satisﬁes the equation
−1 0
(2.44)
p .
p1 = A10 p0 = A01
This matrix is expressed in a block-partitioned form as
⎤ ⎡
⎡
⎢
A10 = ⎣

R01 T
T

0

−R01 T o01

⎥ ⎢
⎦=⎣

1

R10
0

T

⎤

−R10 o01

⎥
⎦,

(2.45)

1

which gives the homogeneous representation form of the result already established by (2.39), (2.40) — see Problem 2.10.
Notice that for the homogeneous transformation matrix the orthogonality
property does not hold; hence, in general,
A−1 = AT .

(2.46)

In sum, a homogeneous transformation matrix expresses the coordinate
transformation between two frames in a compact form. If the frames have the
6

It can be shown that in (2.42) non-null values of the ﬁrst three elements of the
fourth row of A produce a perspective eﬀect, while values other than unity for
the fourth element give a scaling eﬀect.

58

2 Kinematics

Fig. 2.12. Conventional representations of joints

same origin, it reduces to the rotation matrix previously deﬁned. Instead, if
the frames have distinct origins, it allows the notation with superscripts and
subscripts to be kept which directly characterize the current frame and the
ﬁxed frame.
Analogously to what presented for the rotation matrices, it is easy to
verify that a sequence of coordinate transformations can be composed by the
product
pn
p0 = A01 A12 . . . An−1
(2.47)
n
where Ai−1
denotes the homogeneous transformation relating the description
i
of a point in Frame i to the description of the same point in Frame i − 1.

2.8 Direct Kinematics
A manipulator consists of a series of rigid bodies (links) connected by means of
kinematic pairs or joints. Joints can be essentially of two types: revolute and
prismatic; conventional representations of the two types of joints are sketched
in Fig. 2.12. The whole structure forms a kinematic chain. One end of the
chain is constrained to a base. An end-eﬀector (gripper, tool) is connected to
the other end allowing manipulation of objects in space.
From a topological viewpoint, the kinematic chain is termed open when
there is only one sequence of links connecting the two ends of the chain. Alternatively, a manipulator contains a closed kinematic chain when a sequence
of links forms a loop.
The mechanical structure of a manipulator is characterized by a number of
degrees of freedom (DOFs) which uniquely determine its posture.7 Each DOF
is typically associated with a joint articulation and constitutes a joint variable.
The aim of direct kinematics is to compute the pose of the end-eﬀector as a
function of the joint variables.
7

The term posture of a kinematic chain denotes the pose of all the rigid bodies
composing the chain. Whenever the kinematic chain reduces to a single rigid
body, then the posture coincides with the pose of the body.

2.8 Direct Kinematics

59

Fig. 2.13. Description of the position and orientation of the end-eﬀector frame

It was previously illustrated that the pose of a body with respect to a
reference frame is described by the position vector of the origin and the unit
vectors of a frame attached to the body. Hence, with respect to a reference
frame Ob –xb yb zb , the direct kinematics function is expressed by the homogeneous transformation matrix
⎤

⎡
⎢ nbe (q)

T be (q) = ⎣

0

sbe (q)

abe (q)

0

0

pbe (q) ⎥

⎦,

(2.48)

1

where q is the (n × 1) vector of joint variables, ne , se , ae are the unit vectors
of a frame attached to the end-eﬀector, and pe is the position vector of the
origin of such a frame with respect to the origin of the base frame Ob –xb yb zb
(Fig. 2.13). Note that ne , se , ae and pe are a function of q.
The frame Ob –xb yb zb is termed base frame. The frame attached to the endeﬀector is termed end-eﬀector frame and is conveniently chosen according to
the particular task geometry. If the end-eﬀector is a gripper, the origin of the
end-eﬀector frame is located at the centre of the gripper, the unit vector ae
is chosen in the approach direction to the object, the unit vector se is chosen
normal to ae in the sliding plane of the jaws, and the unit vector ne is chosen
normal to the other two so that the frame (ne , se , ae ) is right-handed.
A ﬁrst way to compute direct kinematics is oﬀered by a geometric analysis
of the structure of the given manipulator.

60

2 Kinematics

Fig. 2.14. Two-link planar arm

Example 2.4
Consider the two-link planar arm in Fig. 2.14. On the basis of simple trigonometry,
the choice of the joint variables, the base frame, and the end-eﬀector frame leads
to8

⎡
⎢

T be (q) = ⎣

⎤

nbe

sbe

abe

0

0

0

⎡

0
⎥ ⎢0
⎦ = ⎣1
1
0

pbe

s12
−c12
0
0

c12
s12
0
0

⎤

a1 c1 + a2 c12
a1 s1 + a2 s12 ⎥
⎦.
0
1

(2.49)

It is not diﬃcult to infer that the eﬀectiveness of a geometric approach
to the direct kinematics problem is based ﬁrst on a convenient choice of the
relevant quantities and then on the ability and geometric intuition of the problem solver. Whenever the manipulator structure is complex and the number of
joints increases, it is preferable to adopt a less direct solution, which, though,
is based on a systematic, general procedure. The problem becomes even more
complex when the manipulator contains one or more closed kinematic chains.
In such a case, as it will be discussed later, there is no guarantee to obtain an
analytical expression for the direct kinematics function in (2.48).
2.8.1 Open Chain
Consider an open-chain manipulator constituted by n + 1 links connected by
n joints, where Link 0 is conventionally ﬁxed to the ground. It is assumed that
each joint provides the mechanical structure with a single DOF, corresponding
to the joint variable.
The construction of an operating procedure for the computation of direct kinematics is naturally derived from the typical open kinematic chain of
the manipulator structure. In fact, since each joint connects two consecutive
8

The notations si...j , ci...j denote respectively sin (qi + . . . + qj ), cos (qi + . . . + qj ).

2.8 Direct Kinematics

61

Fig. 2.15. Coordinate transformations in an open kinematic chain

links, it is reasonable to consider ﬁrst the description of kinematic relationship
between consecutive links and then to obtain the overall description of manipulator kinematics in a recursive fashion. To this purpose, it is worth deﬁning
a coordinate frame attached to each link, from Link 0 to Link n. Then, the
coordinate transformation describing the position and orientation of Frame n
with respect to Frame 0 (Fig. 2.15) is given by
(qn ).
T 0n (q) = A01 (q1 )A12 (q2 ) . . . An−1
n

(2.50)

As requested, the computation of the direct kinematics function is recursive
and is obtained in a systematic manner by simple products of the homogeneous
transformation matrices Ai−1
(qi ) (for i = 1, . . . , n), each of which is a function
i
of a single joint variable.
With reference to the direct kinematics equation in (2.49), the actual coordinate transformation describing the position and orientation of the endeﬀector frame with respect to the base frame can be obtained as
T be (q) = T b0 T 0n (q)T ne

(2.51)

where T b0 and T ne are two (typically) constant homogeneous transformations
describing the position and orientation of Frame 0 with respect to the base
frame, and of the end-eﬀector frame with respect to Frame n, respectively.
2.8.2 Denavit–Hartenberg Convention
In order to compute the direct kinematics equation for an open-chain manipulator according to the recursive expression in (2.50), a systematic, general

62

2 Kinematics

Fig. 2.16. Denavit–Hartenberg kinematic parameters

method is to be derived to deﬁne the relative position and orientation of two
consecutive links; the problem is that to determine two frames attached to
the two links and compute the coordinate transformations between them. In
general, the frames can be arbitrarily chosen as long as they are attached to
the link they are referred to. Nevertheless, it is convenient to set some rules
also for the deﬁnition of the link frames.
With reference to Fig. 2.16, let Axis i denote the axis of the joint connecting Link i − 1 to Link i; the so-called Denavit–Hartenberg convention (DH) is
adopted to deﬁne link Frame i:
• Choose axis zi along the axis of Joint i + 1.
• Locate the origin Oi at the intersection of axis zi with the common normal9
to axes zi−1 and zi . Also, locate Oi at the intersection of the common
normal with axis zi−1 .
• Choose axis xi along the common normal to axes zi−1 and zi with direction
from Joint i to Joint i + 1.
• Choose axis yi so as to complete a right-handed frame.
The Denavit–Hartenberg convention gives a nonunique deﬁnition of the link
frame in the following cases:
• For Frame 0, only the direction of axis z0 is speciﬁed; then O0 and x0 can
be arbitrarily chosen.
• For Frame n, since there is no Joint n + 1, zn is not uniquely deﬁned while
xn has to be normal to axis zn−1 . Typically, Joint n is revolute, and thus
zn is to be aligned with the direction of zn−1 .
9

The common normal between two lines is the line containing the minimum distance segment between the two lines.

2.8 Direct Kinematics

63

• When two consecutive axes are parallel, the common normal between them
is not uniquely deﬁned.
• When two consecutive axes intersect, the direction of xi is arbitrary.
• When Joint i is prismatic, the direction of zi−1 is arbitrary.
In all such cases, the indeterminacy can be exploited to simplify the procedure;
for instance, the axes of consecutive frames can be made parallel.
Once the link frames have been established, the position and orientation of
Frame i with respect to Frame i − 1 are completely speciﬁed by the following
parameters:
ai distance between Oi and Oi ,
di coordinate of Oi along zi−1 ,
αi angle between axes zi−1 and zi about axis xi to be taken positive when
rotation is made counter-clockwise,
ϑi angle between axes xi−1 and xi about axis zi−1 to be taken positive when
rotation is made counter-clockwise.
Two of the four parameters (ai and αi ) are always constant and depend
only on the geometry of connection between consecutive joints established
by Link i. Of the remaining two parameters, only one is variable depending
on the type of joint that connects Link i − 1 to Link i. In particular:
• if Joint i is revolute the variable is ϑi ,
• if Joint i is prismatic the variable is di .
At this point, it is possible to express the coordinate transformation between
Frame i and Frame i − 1 according to the following steps:
• Choose a frame aligned with Frame i − 1.
• Translate the chosen frame by di along axis zi−1 and rotate it by ϑi about
axis zi−1 ; this sequence aligns the current frame with Frame i and is
described by the homogeneous transformation matrix
⎡

Ai−1
i

cϑi
⎢ sϑi
=⎣
0
0

−sϑi
cϑi
0
0

0
0
1
0

⎤
0
0⎥
⎦.
di
1

• Translate the frame aligned with Frame i by ai along axis xi and rotate
it by αi about axis xi ; this sequence aligns the current frame with Frame i
and is described by the homogeneous transformation matrix
⎡

1

0
⎢
Aii = ⎣
0
0

0
cαi
sαi
0

0
−sαi
cαi
0

⎤
ai
0⎥
⎦.
0
1

64

2 Kinematics

• The resulting coordinate transformation is obtained by postmultiplication
of the single transformations as
⎡

cϑi

s
⎢
ϑi
i
(qi ) = Ai−1
Ai−1
i
i Ai = ⎣ 0
0

−sϑi cαi
cϑi cαi
sαi
0

sϑi sαi
−cϑi sαi
cαi
0

⎤
ai cϑi
ai sϑi ⎥
⎦.
di
1

(2.52)

Notice that the transformation matrix from Frame i to Frame i−1 is a function
only of the joint variable qi , that is, ϑi for a revolute joint or di for a prismatic
joint.
To summarize, the Denavit–Hartenberg convention allows the construction
of the direct kinematics function by composition of the individual coordinate
transformations expressed by (2.52) into one homogeneous transformation
matrix as in (2.50). The procedure can be applied to any open kinematic
chain and can be easily rewritten in an operating form as follows.
1. Find and number consecutively the joint axes; set the directions of axes
z0 , . . . , zn−1 .
2. Choose Frame 0 by locating the origin on axis z0 ; axes x0 and y0 are
chosen so as to obtain a right-handed frame. If feasible, it is worth choosing
Frame 0 to coincide with the base frame.
Execute steps from 3 to 5 for i = 1, . . . , n − 1:
3. Locate the origin Oi at the intersection of zi with the common normal to
axes zi−1 and zi . If axes zi−1 and zi are parallel and Joint i is revolute,
then locate Oi so that di = 0; if Joint i is prismatic, locate Oi at a reference
position for the joint range, e.g., a mechanical limit.
4. Choose axis xi along the common normal to axes zi−1 and zi with direction
from Joint i to Joint i + 1.
5. Choose axis yi so as to obtain a right-handed frame.
To complete:
6. Choose Frame n; if Joint n is revolute, then align zn with zn−1 , otherwise,
if Joint n is prismatic, then choose zn arbitrarily. Axis xn is set according
to step 4.
7. For i = 1, . . . , n, form the table of parameters ai , di , αi , ϑi .
8. On the basis of the parameters in 7, compute the homogeneous transfor(qi ) for i = 1, . . . , n.
mation matrices Ai−1
i
that
9. Compute the homogeneous transformation T 0n (q) = A01 . . . An−1
n
yields the position and orientation of Frame n with respect to Frame 0.
10.Given T b0 and T ne , compute the direct kinematics function as T be (q) =
T b0 T 0n T ne that yields the position and orientation of the end-eﬀector frame
with respect to the base frame.

2.8 Direct Kinematics

65

Fig. 2.17. Connection of a single link in the chain with two links

For what concerns the computational aspects of direct kinematics, it can be
recognized that the heaviest load derives from the evaluation of transcendental functions. On the other hand, by suitably factorizing the transformation
equations and introducing local variables, the number of ﬂops (additions +
multiplications) can be reduced. Finally, for computation of orientation it is
convenient to evaluate the two unit vectors of the end-eﬀector frame of simplest expression and derive the third one by vector product of the ﬁrst two.
2.8.3 Closed Chain
The above direct kinematics method based on the DH convention exploits
the inherently recursive feature of an open-chain manipulator. Nevertheless,
the method can be extended to the case of manipulators containing closed
kinematic chains according to the technique illustrated below.
Consider a closed-chain manipulator constituted by n + 1 links. Because
of the presence of a loop, the number of joints l must be greater than n; in
particular, it can be understood that the number of closed loops is equal to
l − n.
With reference to Fig. 2.17, Links 0 through i are connected successively
through the ﬁrst i joints as in an open kinematic chain. Then, Joint i + 1
connects Link i with Link i + 1 while Joint i + 1 connects Link i with
Link i + 1 ; the axes of Joints i + 1 and i + 1 are assumed to be aligned.
Although not represented in the ﬁgure, Links i + 1 and i + 1 are members
of the closed kinematic chain. In particular, Link i + 1 is further connected
to Link i + 2 via Joint i + 2 and so forth, until Link j via Joint j. Likewise,
Link i + 1 is further connected to Link i + 2 via Joint i + 2 and so forth,
until Link k via Joint k. Finally, Links j and k are connected together at
Joint j + 1 to form a closed chain. In general, j = k.
In order to attach frames to the various links and apply DH convention,
one closed kinematic chain is taken into account. The closed chain can be
virtually cut open at Joint j + 1, i.e., the joint between Link j and Link k.
An equivalent tree-structured open kinematic chain is obtained, and thus link

66

2 Kinematics

Fig. 2.18. Coordinate transformations in a closed kinematic chain

frames can be deﬁned as in Fig. 2.18. Since Links 0 through i occur before
the two branches of the tree, they are left out of the analysis. For the same
reason, Links j + 1 through n are left out as well. Notice that Frame i is to
be chosen with axis zi aligned with the axes of Joints i + 1 and i + 1 .
It follows that the position and orientation of Frame j with respect to
Frame i can be expressed by composing the homogeneous transformations as
(qj )
Aij (q  ) = Aii+1 (qi+1 ) . . . Aj−1
j

(2.53)

where q  = [ qi+1 . . . qj ] . Likewise, the position and orientation of
Frame k with respect to Frame i is given by
T

(qk )
Aik (q  ) = Aii+1 (qi+1 ) . . . Ak−1
k

(2.54)

where q  = [ qi+1 . . . qk ] .
Since Links j and k are connected to each other through Joint j + 1,
it is worth analyzing the mutual position and orientation between Frames j
and k, as illustrated in Fig. 2.19. Notice that, since Links j and k are connected
to form a closed chain, axes zj and zk are aligned. Therefore, the following
orientation constraint has to be imposed between Frames j and k:
T

z ij (q  ) = z ik (q  ),

(2.55)

where the unit vectors of the two axes have been conveniently referred to
Frame i.
Moreover, if Joint j + 1 is prismatic, the angle ϑjk between axes xj and xk
is ﬁxed; hence, in addition to (2.55), the following constraint is obtained:

i

xiT
j (q )xk (q ) = cos ϑjk .

(2.56)

Obviously, there is no need to impose a similar constraint on axes yj and yk
since that would be redundant.

2.8 Direct Kinematics

67

Fig. 2.19. Coordinate transformation at the cut joint

Regarding the position constraint between Frames j and k, let pij and
respectively denote the positions of the origins of Frames j and k, when
referred to Frame i. By projecting on Frame j the distance vector of the origin
of Frame k from Frame j, the following constraint has to be imposed:

T
(2.57)
Rji (q  ) pij (q  ) − pik (q  ) = [ 0 0 djk ]
pik

where Rji = RiT
j denotes the orientation of Frame i with respect to Frame j.
At this point, if Joint j + 1 is revolute, then djk is a ﬁxed oﬀset along axis zj ;
hence, the three equalities of (2.57) fully describe the position constraint. If,
however, Joint j + 1 is prismatic, then djk varies. Consequently, only the ﬁrst
two equalities of (2.57) describe the position constraint, i.e.,
 
 iT  

xj (q )
0
i

i

pj (q ) − pk (q ) =
(2.58)

0
y iT
(q
)
j
where Rij = [ xij y ij z ij ].
In summary, if Joint j + 1 is revolute the constraints are
 j

T
Ri (q  ) pij (q  ) − pik (q  ) = [ 0 0 djk ]
z ij (q  ) = z ik (q  ),
whereas if Joint j + 1 is prismatic the constraints are
 
⎧  iT  

xj (q )
0
⎪
i

i

⎪
pj (q ) − pk (q ) =
⎪

⎨ y iT
0
j (q )
z ij (q  ) = z ik (q  )
⎪
⎪
⎪
⎩ iT  i 
xj (q )xk (q ) = cos ϑjk .

(2.59)

(2.60)

68

2 Kinematics

In either case, there are six equalities that must be satisﬁed. Those should be
solved for a reduced number of independent joint variables to be keenly chosen
among the components of q  and q  which characterize the DOFs of the closed
chain. These are the natural candidates to be the actuated joints, while the
other joints in the chain (including the cut joint) are typically not actuated.
Such independent variables, together with the remaining joint variables not
involved in the above analysis, constitute the joint vector q that allows the
direct kinematics equation to be computed as
T 0n (q) = A0i Aij Ajn ,

(2.61)

where the sequence of successive transformations after the closure of the chain
has been conventionally resumed from Frame j.
In general, there is no guarantee to solve the constraints in closed form
unless the manipulator has a simple kinematic structure. In other words, for
a given manipulator with a speciﬁc geometry, e.g., a planar structure, some of
the above equalities may become dependent. Hence, the number of independent equalities is less than six and it should likely be easier to solve them.
To conclude, it is worth sketching the operating form of the procedure to
compute the direct kinematics function for a closed-chain manipulator using
the Denavit–Hartenberg convention.
1. In the closed chain, select one joint that is not actuated. Assume that the
joint is cut open so as to obtain an open chain in a tree structure.
2. Compute the homogeneous transformations according to DH convention.
3. Find the equality constraints for the two frames connected by the cut joint.
4. Solve the constraints for a reduced number of joint variables.
5. Express the homogeneous transformations in terms of the above joint variables and compute the direct kinematics function by composing the various
transformations from the base frame to the end-eﬀector frame.

2.9 Kinematics of Typical Manipulator Structures
This section contains several examples of computation of the direct kinematics function for typical manipulator structures that are often encountered in
industrial robots.
With reference to the schematic representation of the kinematic chain,
manipulators are usually illustrated in postures where the joint variables, deﬁned according to the DH convention, are diﬀerent from zero; such values
might diﬀer from the null references utilized for robot manipulator programming. Hence, it will be necessary to sum constant contributions (oﬀsets) to
the values of the joint variables measured by the robot sensory system, so as
to match the references.

2.9 Kinematics of Typical Manipulator Structures

69

Fig. 2.20. Three-link planar arm

2.9.1 Three-link Planar Arm
Consider the three-link planar arm in Fig. 2.20, where the link frames have
been illustrated. Since the revolute axes are all parallel, the simplest choice
was made for all axes xi along the direction of the relative links (the direction
of x0 is arbitrary) and all lying in the plane (x0 , y0 ). In this way, all the
parameters di are null and the angles between the axes xi directly provide the
joint variables. The DH parameters are speciﬁed in Table 2.1.
Table 2.1. DH parameters for the three-link planar arm
Link
1
2
3

ai
a1
a2
a3

αi
0
0
0

di
0
0
0

ϑi
ϑ1
ϑ2
ϑ3

Since all joints are revolute, the homogeneous transformation matrix deﬁned in (2.52) has the same structure for each joint, i.e.,
⎡

ci
s
⎢
(ϑi ) = ⎣ i
Ai−1
i
0
0

−si
ci
0
0

0
0
1
0

⎤
ai ci
ai si ⎥
⎦
0
1

i = 1, 2, 3.

(2.62)

70

2 Kinematics

Fig. 2.21. Parallelogram arm

Computation of the direct kinematics function as in (2.50) yields
⎤
⎡
c123 −s123 0 a1 c1 + a2 c12 + a3 c123
c123 0 a1 s1 + a2 s12 + a3 s123 ⎥
⎢s
T 03 (q) = A01 A12 A23 = ⎣ 123
⎦
0
0
1
0
0
0
0
1

(2.63)

where q = [ ϑ1 ϑ2 ϑ3 ]T . Notice that the unit vector z 03 of Frame 3 is aligned
with z 0 = [ 0 0 1 ]T , in view of the fact that all revolute joints are parallel
to axis z0 . Obviously, pz = 0 and all three joints concur to determine the
end-eﬀector position in the plane of the structure. It is worth pointing out
that Frame 3 does not coincide with the end-eﬀector frame (Fig. 2.13), since
the resulting approach unit vector is aligned with x03 and not with z 03 . Thus,
assuming that the two frames have the same origin, the constant transformation
⎡
⎤
0 0 1 0
⎢ 0 1 0 0⎥
T 3e = ⎣
⎦.
−1 0 0 0
0 0 0 1
is needed, having taken n aligned with z 0 .
2.9.2 Parallelogram Arm
Consider the parallelogram arm in Fig. 2.21. A closed chain occurs where the
ﬁrst two joints connect Link 1 and Link 1 to Link 0, respectively. Joint 4 was
selected as the cut joint, and the link frames have been established accordingly.
The DH parameters are speciﬁed in Table 2.2, where a1 = a3 and a2 = a1
in view of the parallelogram structure.
Notice that the parameters for Link 4 are all constant. Since the joints
are revolute, the homogeneous transformation matrix deﬁned in (2.52) has

2.9 Kinematics of Typical Manipulator Structures

71

Table 2.2. DH parameters for the parallelogram arm
Link
1
2
3
1
4

ai
a1 
a2 
a3 
a1
a4

αi
0
0
0
0
0

di
0
0
0
0
0

ϑi
ϑ 1
ϑ 2
ϑ 3
ϑ1
0

the same structure for each joint, i.e., as in (2.62) for Joints 1 , 2 , 3 and 1 .
Therefore, the coordinate transformations for the two branches of the tree are
respectively:
⎤
⎡   
c1 2 3 −s1 2 3 0 a1 c1 + a2 c1 2 + a3 c1 2 3


c1 2 3
0 a1 s1 + a2 s1 2 + a3 s1 2 3 ⎥
⎢s   
A03 (q  ) = A01 A12 A23 = ⎣ 1 2 3
⎦
0
0
1
0
0
0
0
1
where q  = [ ϑ1

ϑ3 ]T , and
⎡ 
c1
⎢ s1
0

A1 (q ) = ⎣
0
0

ϑ 2

−s1
c1
0
0

0
0
1
0

⎤
a1 c1
a1 s1 ⎥
⎦
0
1

where q  = ϑ1 . To complete, the constant homogeneous transformation for
the last link is
⎤
⎡
1 0 0 a4

⎢0 1 0 0 ⎥
A34 = ⎣
⎦.
0 0 1 0
0 0 0 1
With reference to (2.59), the position constraints are (d3 1 = 0)
⎡ ⎤
0


R30 (q  ) p03 (q  ) − p01 (q  ) = ⎣ 0 ⎦
0
while the orientation constraints are satisﬁed independently of q  and q  . Since
a1 = a3 and a2 = a1 , two independent constraints can be extracted, i.e.,
a1 (c1 + c1 2 3 ) + a1 (c1 2 − c1 ) = 0
a1 (s1 + s1 2 3 ) + a1 (s1 2 − s1 ) = 0.
In order to satisfy them for any choice of a1 and a1 , it must be
ϑ2 = ϑ1 − ϑ1
ϑ3 = π − ϑ2 = π − ϑ1 + ϑ1

72

2 Kinematics
T

Therefore, the vector of joint variables is q = [ ϑ1 ϑ1 ] . These joints are
natural candidates to be the actuated joints.10 Substituting the expressions
of ϑ2 and ϑ3 into the homogeneous transformation A03 and computing the
direct kinematics function as in (2.61) yields
⎡

−c1


−s
⎢
T 04 (q) = A03 (q)A34 = ⎣ 1
0
0

s1
−c1
0
0

⎤
0 a1 c1 − a4 c1
0 a1 s1 − a4 s1 ⎥
⎦.
1
0
0
1

(2.64)

A comparison between (2.64) and (2.49) reveals that the parallelogram arm is
kinematically equivalent to a two-link planar arm. The noticeable diﬀerence,
though, is that the two actuated joints — providing the DOFs of the structure
— are located at the base. This will greatly simplify the dynamic model of
the structure, as will be seen in Sect. 7.3.3.
2.9.3 Spherical Arm
Consider the spherical arm in Fig. 2.22, where the link frames have been
illustrated. Notice that the origin of Frame 0 was located at the intersection
of z0 with z1 so that d1 = 0; analogously, the origin of Frame 2 was located
at the intersection between z1 and z2 . The DH parameters are speciﬁed in
Table 2.3.
Table 2.3. DH parameters for the spherical arm
Link
1
2
3

ai
0
0
0

αi
−π/2
π/2
0

di
0
d2
d3

ϑi
ϑ1
ϑ2
0

The homogeneous transformation matrices deﬁned in (2.52) are for the
single joints:
⎡

c1
⎢ s1
0
A1 (ϑ1 ) = ⎣
0
0

0 −s1
0
c1
−1
0
0
0

⎤
0
0⎥
⎦
0
1
⎡

1
⎢0
2
A3 (d3 ) = ⎣
0
0
10

⎡

c2
⎢ s2
1
A2 (ϑ2 ) = ⎣
0
0
0
1
0
0

0 s2
0 −c2
1
0
0
0

⎤
0
0 ⎥
⎦
d2
1

⎤
0 0
0 0 ⎥
⎦.
1 d3
0 1

Notice that it is not possible to solve (2.64) for ϑ2 and ϑ3 since they are constrained by the condition ϑ2 + ϑ3 = π.

2.9 Kinematics of Typical Manipulator Structures

73

Fig. 2.22. Spherical arm

Computation of the direct kinematics function as in (2.50) yields
⎡

c1 c2
⎢ s1 c2
0
0 1 2
T 3 (q) = A1 A2 A3 = ⎣
−s2
0

−s1
c1
0
0

c1 s2
s1 s2
c2
0

⎤
c1 s2 d3 − s1 d2
s1 s2 d3 + c1 d2 ⎥
⎦
c2 d3
1

(2.65)

where q = [ ϑ1 ϑ2 d3 ]T . Notice that the third joint does not obviously
inﬂuence the rotation matrix. Further, the orientation of the unit vector y 03
is uniquely determined by the ﬁrst joint, since the revolute axis of the second
joint z1 is parallel to axis y3 . Diﬀerent from the previous structures, in this
case Frame 3 can represent an end-eﬀector frame of unit vectors (ne , se , ae ),
i.e., T 3e = I 4 .
2.9.4 Anthropomorphic Arm
Consider the anthropomorphic arm in Fig. 2.23. Notice how this arm corresponds to a two-link planar arm with an additional rotation about an axis
of the plane. In this respect, the parallelogram arm could be used in lieu of
the two-link planar arm, as found in some industrial robots with an anthropomorphic structure.
The link frames have been illustrated in the ﬁgure. As for the previous
structure, the origin of Frame 0 was chosen at the intersection of z0 with z1
(d1 = 0); further, z1 and z2 are parallel and the choice of axes x1 and x2
was made as for the two-link planar arm. The DH parameters are speciﬁed in
Table 2.4.

74

2 Kinematics

Fig. 2.23. Anthropomorphic arm
Table 2.4. DH parameters for the anthropomorphic arm
Link
1
2
3

ai
0
a2
a3

αi
π/2
0
0

di
0
0
0

ϑi
ϑ1
ϑ2
ϑ3

The homogeneous transformation matrices deﬁned in (2.52) are for the
single joints:
⎤
⎡
c1 0 s1 0
⎢ s 0 −c1 0 ⎥
A01 (ϑ1 ) = ⎣ 1
⎦
0 1
0
0
0 0
0
1
⎤
⎡
ci −si 0 ai ci
s
ci 0 ai si ⎥
⎢
(ϑi ) = ⎣ i
Ai−1
i = 2, 3.
⎦
i
0
0
1
0
0
0
0
1
Computation of the direct kinematics function as in (2.50) yields
⎡

c1 c23
⎢ s1 c23
0
0 1 2
T 3 (q) = A1 A2 A3 = ⎣
s23
0

−c1 s23
−s1 s23
c23
0

s1
−c1
0
0

⎤
c1 (a2 c2 + a3 c23 )
s1 (a2 c2 + a3 c23 ) ⎥
⎦
a2 s2 + a3 s23
1

(2.66)

where q = [ ϑ1 ϑ2 ϑ3 ]T . Since z3 is aligned with z2 , Frame 3 does not coincide with a possible end-eﬀector frame as in Fig. 2.13, and a proper constant
transformation would be needed.

2.9 Kinematics of Typical Manipulator Structures

75

Fig. 2.24. Spherical wrist

2.9.5 Spherical Wrist
Consider a particular type of structure consisting just of the wrist of Fig. 2.24.
Joint variables were numbered progressively starting from 4, since such a
wrist is typically thought of as mounted on a three-DOF arm of a six-DOF
manipulator. It is worth noticing that the wrist is spherical since all revolute
axes intersect at a single point. Once z3 , z4 , z5 have been established, and x3
has been chosen, there is an indeterminacy on the directions of x4 and x5 .
With reference to the frames indicated in Fig. 2.24, the DH parameters are
speciﬁed in Table 2.5.
Table 2.5. DH parameters for the spherical wrist
Link
4
5
6

ai
0
0
0

αi
−π/2
π/2
0

di
0
0
d6

ϑi
ϑ4
ϑ5
ϑ6

The homogeneous transformation matrices deﬁned in (2.52) are for the
single joints:
⎡

c4
s
⎢
A34 (ϑ4 ) = ⎣ 4
0
0

0
0
−1
0

−s4
c4
0
0

⎤
0
0⎥
⎦
0
1
⎡

c6
s
⎢
A56 (ϑ6 ) = ⎣ 6
0
0

⎡

c5
s
⎢
A45 (ϑ5 ) = ⎣ 5
0
0
−s6
c6
0
0

⎤
0 0
0 0 ⎥
⎦.
1 d6
0 1

0 s5
0 −c5
1
0
0
0

⎤
0
0⎥
⎦
0
1

76

2 Kinematics

Fig. 2.25. Stanford manipulator

Computation of the direct kinematics function as in (2.50) yields
⎤
c4 s5 d6
s4 s5 d6 ⎥
⎦
c5 d6
1
(2.67)
where q = [ ϑ4 ϑ5 ϑ6 ]T . Notice that, as a consequence of the choice made
for the coordinate frames, the block matrix R36 that can be extracted from T 36
coincides with the rotation matrix of Euler angles (2.18) previously derived,
that is, ϑ4 , ϑ5 , ϑ6 constitute the set of ZYZ angles with respect to the reference
frame O3 –x3 y3 z3 . Moreover, the unit vectors of Frame 6 coincide with the unit
vectors of a possible end-eﬀector frame according to Fig. 2.13.
⎡

c4 c5 c6 − s4 s6
⎢ s4 c5 c6 + c4 s6
3
3 4 5
T 6 (q) = A4 A5 A6 = ⎣
−s5 c6
0

−c4 c5 s6 − s4 c6
−s4 c5 s6 + c4 c6
s5 s6
0

c4 s5
s4 s5
c5
0

2.9.6 Stanford Manipulator
The so-called Stanford manipulator is composed of a spherical arm and a
spherical wrist (Fig. 2.25). Since Frame 3 of the spherical arm coincides with
Frame 3 of the spherical wrist, the direct kinematics function can be obtained
via simple composition of the transformation matrices (2.65), (2.67) of the
previous examples, i.e.,
⎤
⎡
⎢ n0
T 06 = T 03 T 36 = ⎣

s0

a0

p0 ⎥
⎦.

0

0

0

1

2.9 Kinematics of Typical Manipulator Structures

77

Carrying out the products yields
 ⎤
c1 s2 d3 − s1 d2 + c1 (c2 c4 s5 + s2 c5 ) − s1 s4 s5 d6
p06 = ⎣ s1 s2 d3 + c1 d2 + s1 (c2 c4 s5 + s2 c5 ) + c1 s4 s5 d6 ⎦
c2 d3 + (−s2 c4 s5 + c2 c5 )d6
⎡

(2.68)

for the end-eﬀector position, and

⎤
⎡
c1 c2 (c4 c5 c6 − s4 s6 ) − s2 s5 c6  − s1 (s4 c5 c6 + c4 s6 )
n06 = ⎣ s1 c2 (c4 c5 c6 − s4 s6 ) − s2 s5 c6 + c1 (s4 c5 c6 + c4 s6 ) ⎦
−s2 (c4 c5 c6 − s4 s6 ) − c2 s5 c6

⎤
⎡
c1 −c2 (c4 c5 s6 + s4 c6 ) + s2 s5 s6  − s1 (−s4 c5 s6 + c4 c6 )
s06 = ⎣ s1 −c2 (c4 c5 s6 + s4 c6 ) + s2 s5 s6 + c1 (−s4 c5 s6 + c4 c6 ) ⎦ (2.69)
s2 (c4 c5 s6 + s4 c6 ) + c2 s5 s6
⎤
⎡
c1 (c2 c4 s5 + s2 c5 ) − s1 s4 s5
a06 = ⎣ s1 (c2 c4 s5 + s2 c5 ) + c1 s4 s5 ⎦
−s2 c4 s5 + c2 c5
for the end-eﬀector orientation.
A comparison of the vector p06 in (2.68) with the vector p03 in (2.65) relative
to the sole spherical arm reveals the presence of additional contributions due
to the choice of the origin of the end-eﬀector frame at a distance d6 from
the origin of Frame 3 along the direction of a06 . In other words, if it were
d6 = 0, the position vector would be the same. This feature is of fundamental
importance for the solution of the inverse kinematics for this manipulator, as
will be seen later.
2.9.7 Anthropomorphic Arm with Spherical Wrist
A comparison between Fig. 2.23 and Fig. 2.24 reveals that the direct kinematics function cannot be obtained by multiplying the transformation matrices
T 03 and T 36 , since Frame 3 of the anthropomorphic arm cannot coincide with
Frame 3 of the spherical wrist.
Direct kinematics of the entire structure can be obtained in two ways.
One consists of interposing a constant transformation matrix between T 03 and
T 36 which allows the alignment of the two frames. The other refers to the
Denavit–Hartenberg operating procedure with the frame assignment for the
entire structure illustrated in Fig. 2.26. The DH parameters are speciﬁed in
Table 2.6.
Since Rows 3 and 4 diﬀer from the corresponding rows of the tables for
the two single structures, the relative homogeneous transformation matrices
A23 and A34 have to be modiﬁed into
⎤
⎤
⎡
⎡
c3 0 s3 0
c4 0 −s4 0
0
c4
0 ⎥
⎢ s 0 −c3 0 ⎥
⎢s
A23 (ϑ3 ) = ⎣ 3
A34 (ϑ4 ) = ⎣ 4
⎦
⎦
0 1
0
0
0 −1
0
d4
0
0
0
1
0 0
0
1

78

2 Kinematics

Fig. 2.26. Anthropomorphic arm with spherical wrist
Table 2.6. DH parameters for the anthropomorphic arm with spherical wrist
Link
1
2
3
4
5
6

ai
0
a2
0
0
0
0

αi
π/2
0
π/2
−π/2
π/2
0

di
0
0
0
d4
0
d6

ϑi
ϑ1
ϑ2
ϑ3
ϑ4
ϑ5
ϑ6

while the other transformation matrices remain the same. Computation of the
direct kinematics function leads to expressing the position and orientation of
the end-eﬀector frame as:
⎤
⎡
a2 c1 c2 + d4 c1 s23 + d6 c1 (c23 c4 s5 + s23 c5 ) + s1 s4 s5 
(2.70)
p06 = ⎣ a2 s1 c2 + d4 s1 s23 + d6 s1 (c23 c4 s5 + s23 c5 ) − c1 s4 s5 ⎦
a2 s2 − d4 c23 + d6 (s23 c4 s5 − c23 c5 )
and


⎤
c1 c23 (c4 c5 c6 − s4 s6 ) − s23 s5 c6  + s1 (s4 c5 c6 + c4 s6 )
n06 = ⎣ s1 c23 (c4 c5 c6 − s4 s6 ) − s23 s5 c6 − c1 (s4 c5 c6 + c4 s6 ) ⎦
s23 (c4 c5 c6 − s4 s6 ) + c23 s5 c6

⎤
⎡
c1 −c23 (c4 c5 s6 + s4 c6 ) + s23 s5 s6  + s1 (−s4 c5 s6 + c4 c6 )
s06 = ⎣ s1 −c23 (c4 c5 s6 + s4 c6 ) + s23 s5 s6 − c1 (−s4 c5 s6 + c4 c6 ) ⎦ (2.71)
−s23 (c4 c5 s6 + s4 c6 ) − c23 s5 s6
⎤
⎡
c1 (c23 c4 s5 + s23 c5 ) + s1 s4 s5
a06 = ⎣ s1 (c23 c4 s5 + s23 c5 ) − c1 s4 s5 ⎦ .
s23 c4 s5 − c23 c5
⎡

2.9 Kinematics of Typical Manipulator Structures

79

Fig. 2.27. DLR manipulator

By setting d6 = 0, the position of the wrist axes intersection is obtained. In
that case, the vector p0 in (2.70) corresponds to the vector p03 for the sole
anthropomorphic arm in (2.66), because d4 gives the length of the forearm
(a3 ) and axis x3 in Fig. 2.26 is rotated by π/2 with respect to axis x3 in
Fig. 2.23.
2.9.8 DLR Manipulator
Consider the DLR manipulator, whose development is at the basis of the realization of the robot in Fig. 1.30; it is characterized by seven DOFs and as such
it is inherently redundant. This manipulator has two possible conﬁgurations
for the outer three joints (wrist). With reference to a spherical wrist similar to
that introduced in Sect. 2.9.5, the resulting kinematic structure is illustrated
in Fig. 2.27, where the frames attached to the links are evidenced.
As in the case of the spherical arm, notice that the origin of Frame 0 has
been chosen so as to zero d1 . The DH parameters are speciﬁed in Table 2.7.
Table 2.7. DH parameters for the DLR manipulator
Link
1
2
3
4
5
6
7

ai
0
0
0
0
0
0
0

αi
π/2
π/2
π/2
π/2
π/2
π/2
0

di
0
0
d3
0
d5
0
d7

ϑi
ϑ1
ϑ2
ϑ3
ϑ4
ϑ5
ϑ6
ϑ7

80

2 Kinematics

The generic homogeneous transformation matrix deﬁned in (2.52) is (αi =
π/2)
⎤
⎡
0
ci 0 si
⎢ si 0 −ci 0 ⎥
⎥
i = 1, . . . , 6
(2.72)
=⎢
Ai−1
i
⎣0 1
0
di ⎦
0

0

0

1

while, since α7 = 0, it is
⎡

c7
⎢
s
7
A67 = ⎢
⎣0
0

−s7
c7
0
0

⎤
0 0
0 0 ⎥
⎥.
1 d7 ⎦
0 1

(2.73)

The direct kinematics function, computed as in (2.50), leads to the following
expressions for the end-eﬀector frame
⎤
⎡
d3 xd3 + d5 xd5 + d7 xd7
p07 = ⎣ d3 yd3 + d5 yd5 + d7 yd7 ⎦
(2.74)
d3 zd3 + d5 zd5 + d7 zd7
with
xd3 = c1 s2
xd5 = c1 (c2 c3 s4 − s2 c4 ) + s1 s3 s4
xd7 = c1 (c2 k1 + s2 k2 ) + s1 k3
yd3 = s1 s2
yd5 = s1 (c2 c3 s4 − s2 c4 ) − c1 s3 s4
yd7 = s1 (c2 k1 + s2 k2 ) − c1 k3
zd3 = −c2
zd5 = c2 c4 + s2 c3 s4
zd7 = s2 (c3 (c4 c5 s6 − s4 c6 ) + s3 s5 s6 ) − c2 k2 ,
where
k1 = c3 (c4 c5 s6 − s4 c6 ) + s3 s5 s6
k2 = s4 c5 s6 + c4 c6
k3 = s3 (c4 c5 s6 − s4 c6 ) − c3 s5 s6 .
Furthermore, the end-eﬀector frame orientation can be derived as
⎤
⎡
((xa c5 + xc s5 )c6 + xb s6 )c7 + (xa s5 − xc c5 )s7
n07 = ⎣ ((ya c5 + yc s5 )c6 + yb s6 )c7 + (ya s5 − yc c5 )s7 ⎦
(za c6 + zc s6 )c7 + zb s7

2.9 Kinematics of Typical Manipulator Structures

81

⎤

⎡

−((xa c5 + xc s5 )c6 + xb s6 )s7 + (xa s5 − xc c5 )c7
s07 = ⎣ −((ya c5 + yc s5 )c6 + yb s6 )s7 + (ya s5 − yc c5 )c7 ⎦
−(za c6 + zc s6 )s7 + zb c7
⎤
⎡
(xa c5 + xc s5 )s6 − xb c6
a07 = ⎣ (ya c5 + yc s5 )s6 − yb c6 ⎦ ,
za s6 − zc c6

(2.75)

where
xa = (c1 c2 c3 + s1 s3 )c4 + c1 s2 s4
xb = (c1 c2 c3 + s1 s3 )s4 − c1 s2 c4
xc = c1 c2 s3 − s1 c3
ya = (s1 c2 c3 − c1 s3 )c4 + s1 s2 s4
yb = (s1 c2 c3 − c1 s3 )s4 − s1 s2 c4
yc = s1 c2 s3 + c1 c3
za = (s2 c3 c4 − c2 s4 )c5 + s2 s3 s5
zb = (s2 c3 s4 + c2 c4 )s5 − s2 s3 c5
zc = s2 c3 s4 + c2 c4 .
(2.76)
As in the case of the anthropomorphic arm with spherical wrist, it occurs
that Frame 4 cannot coincide with the base frame of the wrist.
Finally, consider the possibility to mount a diﬀerent type of spherical wrist,
where Joint 7 is so that α7 = π/2. In such a case, the computation of the
direct kinematics function changes, since the seventh row of the kinematic
parameters table changes. In particular, notice that, since d7 = 0, a7 = 0,
then
⎤
⎡
c7 0 s7 a7 c7
⎢ s7 0 −c7 a7 s7 ⎥
⎥.
(2.77)
A67 = ⎢
⎣0 0
1
0 ⎦
0

0

0

1

It follows, however, that Frame 7 does not coincide with the end-eﬀector
frame, as already discussed for the three-link planar arm, since the approach
unit vector a07 is aligned with x7 .
2.9.9 Humanoid Manipulator
The term humanoid refers to a robot showing a kinematic structure similar to
that of the human body. It is commonly thought that the most relevant feature of humanoid robots is biped locomotion. However, in detail, a humanoid
manipulator refers to an articulated structure with a kinematics analogous to

82

2 Kinematics

Fig. 2.28. Humanoid manipulator

that of the human body upper part: torso, arms, end-eﬀectors similar to human hands and a ‘head’ which, eventually, includes an artiﬁcial vision system
— see Chap. 10.
For the humanoid manipulator in Fig. 1.33, it is worth noticing the presence of two end-eﬀectors (where the ‘hands’ are mounted), while the arms
consist of two DLR manipulators, introduced in the previous section, each
with seven DOFs. In particular, consider the conﬁguration where the last
joint is so that α7 = π/2.
To simplify, the kinematic structure allowing the articulation of the robot’s
head in Fig. 1.33. The torso can be modelled as an anthropomorphic arm
(three DOFs), for a total of seventeen DOFs.
Further, a connecting device exists between the end-eﬀector of the anthropomorphic torso and the base frames of the two manipulators. Such device
permits keeping the ‘chest’ of the humanoid manipulator always orthogonal to
the ground. With reference to Fig. 2.28, this device is represented by a further
joint, located at the end of the torso. Hence, the corresponding parameter ϑ4
does not constitute a DOF, yet it varies so as to compensate Joints 2 and 3
rotations of the anthropomorphic torso.
To compute the direct kinematics function, it is possible to resort to a DH
parameters table for each of the two tree kinematic structures, which can be
identiﬁed from the base of the manipulator to each of the two end-eﬀectors.
Similarly to the case of mounting a spherical wrist onto an anthropomorphic
arm, this implies the change of some rows of the transformation matrices of

2.10 Joint Space and Operational Space

83

those manipulators, described in the previous sections, constituting the torso
and the arms.
Alternatively, it is possible to consider intermediate transformation matrices between the relevant structures. In detail, as illustrated in Fig. 2.28, if t
denotes the frame attached to the torso, r and l the base frames, respectively,
of the right arm and the left arm, and rh and lh the frames attached to the
two hands (end-eﬀectors), it is possible to compute for the right arm and the
left arm, respectively:
T 0rh = T 03 T 3t T tr T rrh

(2.78)

T 0lh = T 03 T 3t T tl T llh

(2.79)

where the matrix T 3t describes the transformation imposed by the motion of
Joint 4 (dashed line in Fig. 2.28), located at the end-eﬀector of the torso.
Frame 4 coincides with Frame t in Fig. 2.27. In view of the property of parameter ϑ4 , it is ϑ4 = −ϑ2 − ϑ3 , and thus
⎡

c23
⎢ −s23
3
Tt = ⎣
0
0

s23
c23
0
0

0
0
1
0

⎤
0
0⎥
⎦.
0
1

The matrix T 03 is given by (2.66), whereas the matrices T tr and T tl relating
the torso end-eﬀector frame to the base frames of the two manipulators have
constant values. With reference to Fig. 2.28, the elements of these matrices
depend on the angle β and on the distances between the origin of Frame t
and the origins of Frames r and l. Finally, the expressions of the matrices T rrh
and T llh must be computed by considering the change in the seventh row of
the DH parameters table of the DLR manipulator, so as to account for the
diﬀerent kinematic structure of the wrist (see Problem 2.14).

2.10 Joint Space and Operational Space
As described in the previous sections, the direct kinematics equation of a
manipulator allows the position and orientation of the end-eﬀector frame to
be expressed as a function of the joint variables with respect to the base frame.
If a task is to be speciﬁed for the end-eﬀector, it is necessary to assign the
end-eﬀector position and orientation, eventually as a function of time (trajectory). This is quite easy for the position. On the other hand, specifying
the orientation through the unit vector triplet (ne , se , ae )11 is quite diﬃcult,
since their nine components must be guaranteed to satisfy the orthonormality constraints imposed by (2.4) at each time instant. This problem will be
resumed in Chap. 4.
11

To simplify, the indication of the reference frame in the superscript is omitted.

84

2 Kinematics

The problem of describing end-eﬀector orientation admits a natural solution if one of the above minimal representations is adopted. In this case,
indeed, a motion trajectory can be assigned to the set of angles chosen to
represent orientation.
Therefore, the position can be given by a minimal number of coordinates
with regard to the geometry of the structure, and the orientation can be
speciﬁed in terms of a minimal representation (Euler angles) describing the
rotation of the end-eﬀector frame with respect to the base frame. In this way,
it is possible to describe the end-eﬀector pose by means of the (m × 1) vector,
with m ≤ n,
 
pe
(2.80)
xe =
φe
where pe describes the end-eﬀector position and φe its orientation.
This representation of position and orientation allows the description of an
end-eﬀector task in terms of a number of inherently independent parameters.
The vector xe is deﬁned in the space in which the manipulator task is speciﬁed;
hence, this space is typically called operational space. On the other hand, the
joint space (conﬁguration space) denotes the space in which the (n × 1) vector
of joint variables
⎡ ⎤
q1
⎢ .. ⎥
q = ⎣ . ⎦,
(2.81)
qn
is deﬁned; it is qi = ϑi for a revolute joint and qi = di for a prismatic
joint. Accounting for the dependence of position and orientation from the
joint variables, the direct kinematics equation can be written in a form other
than (2.50), i.e.,
(2.82)
xe = k(q).
The (m × 1) vector function k(·) — nonlinear in general — allows computation of the operational space variables from the knowledge of the joint space
variables.
It is worth noticing that the dependence of the orientation components
of the function k(q) in (2.82) on the joint variables is not easy to express
except for simple cases. In fact, in the most general case of a six-dimensional
operational space (m = 6), the computation of the three components of the
function φe (q) cannot be performed in closed form but goes through the
computation of the elements of the rotation matrix, i.e., ne (q), se (q), ae (q).
The equations that allow the determination of the Euler angles from the triplet
of unit vectors ne , se , ae were given in Sect. 2.4.

2.10 Joint Space and Operational Space

85

Example 2.5
Consider again the three-link planar arm in Fig. 2.20. The geometry of the structure
suggests that the end-eﬀector position is determined by the two coordinates px and
py , while its orientation is determined by the angle φ formed by the end-eﬀector with
the axis x0 . Expressing these operational variables as a function of the joint variables,
the two position coordinates are given by the ﬁrst two elements of the fourth column
of the homogeneous transformation matrix (2.63), while the orientation angle is
simply given by the sum of joint variables. In sum, the direct kinematics equation
can be written in the form



xe =

px
py
φ





= k(q) =

a1 c1 + a2 c12 + a3 c123
a1 s1 + a2 s12 + a3 s123
ϑ1 + ϑ2 + ϑ3



.

(2.83)

This expression shows that three joint space variables allow speciﬁcation of at most
three independent operational space variables. On the other hand, if orientation is
of no concern, it is xe = [ px py ]T and there is kinematic redundancy of DOFs
with respect to a pure positioning end-eﬀector task; this concept will be dealt with
in detail afterwards.

2.10.1 Workspace
With reference to the operational space, an index of robot performance is
the so-called workspace; this is the region described by the origin of the endeﬀector frame when all the manipulator joints execute all possible motions. It
is often customary to distinguish between reachable workspace and dexterous
workspace. The latter is the region that the origin of the end-eﬀector frame
can describe while attaining diﬀerent orientations, while the former is the
region that the origin of the end-eﬀector frame can reach with at least one
orientation. Obviously, the dexterous workspace is a subspace of the reachable
workspace. A manipulator with less than six DOFs cannot take any arbitrary
position and orientation in space.
The workspace is characterized by the manipulator geometry and the mechanical joint limits. For an n-DOF manipulator, the reachable workspace is
the geometric locus of the points that can be achieved by considering the
direct kinematics equation for the sole position part, i.e.,
pe = pe (q)

qim ≤ qi ≤ qiM

i = 1, . . . , n,

where qim (qiM ) denotes the minimum (maximum) limit at Joint i. This volume is ﬁnite, closed, connected — pe (q) is a continuous function — and thus
is deﬁned by its bordering surface. Since the joints are revolute or prismatic,
it is easy to recognize that this surface is constituted by surface elements of
planar, spherical, toroidal and cylindrical type. The manipulator workspace

86

2 Kinematics

Fig. 2.29. Region of admissible conﬁgurations for a two-link arm

(without end-eﬀector) is reported in the data sheet given by the robot manufacturer in terms of a top view and a side view. It represents a basic element
to evaluate robot performance for a desired application.

Example 2.6
Consider the simple two-link planar arm. If the mechanical joint limits are known,
the arm can attain all the joint space conﬁgurations corresponding to the points in
the rectangle in Fig. 2.29.
The reachable workspace can be derived via a graphical construction of the
image of the rectangle perimeter in the plane of the arm. To this purpose, it is
worth considering the images of the segments ab, bc, cd, da, ae, ef , f d. Along the
segments ab, bc, cd, ae, ef , f d a loss of mobility occurs due to a joint limit; a
loss of mobility occurs also along the segment ad because the arm and forearm are
aligned.12 Further, a change of the arm posture occurs at points a and d: for q2 > 0
the elbow-down posture is obtained, while for q2 < 0 the arm is in the elbow-up
posture.
In the plane of the arm, start drawing the arm in conﬁguration A corresponding
to q1m and q2 = 0 (a); then, the segment ab describing motion from q2 = 0 to
q2M generates the arc AB; the subsequent arcs BC, CD, DA, AE, EF , F D are
generated in a similar way (Fig. 2.30). The external contour of the area CDAEF HC
delimits the requested workspace. Further, the area BCDAB is relative to elbowdown postures while the area DAEF D is relative to elbow-up postures; hence, the
points in the area BADHB are reachable by the end-eﬀector with both postures.
12

In the following chapter, it will be seen that this conﬁguration characterizes a
kinematic singularity of the arm.

2.10 Joint Space and Operational Space

87

Fig. 2.30. Workspace of a two-link planar arm

In a real manipulator, for a given set of joint variables, the actual values of the operational space variables deviate from those computed via direct
kinematics. The direct kinematics equation has indeed a dependence from the
DH parameters which is not explicit in (2.82). If the mechanical dimensions
of the structure diﬀer from the corresponding parameter of the table because
of mechanical tolerances, a deviation arises between the position reached in
the assigned posture and the position computed via direct kinematics. Such a
deviation is deﬁned accuracy; this parameter attains typical values below one
millimeter and depends on the structure as well as on manipulator dimensions. Accuracy varies with the end-eﬀector position in the workspace and it
is a relevant parameter when robot programming oriented environments are
adopted, as will be seen in the last chapter.
Another parameter that is usually listed in the performance data sheet of
an industrial robot is repeatability which gives a measure of the manipulator’s
ability to return to a previously reached position; this parameter is relevant
for programming an industrial robot by the teaching–by–showing technique
which will be presented in Chap. 6. Repeatability depends not only on the
characteristics of the mechanical structure but also on the transducers and
controller; it is expressed in metric units and is typically smaller than accuracy.
For instance, for a manipulator with a maximum reach of 1.5 m, accuracy
varies from 0.2 to 1 mm in the workspace, while repeatability varies from 0.02
to 0.2 mm.
2.10.2 Kinematic Redundancy
A manipulator is termed kinematically redundant when it has a number of
DOFs which is greater than the number of variables that are necessary to

88

2 Kinematics

describe a given task. With reference to the above-deﬁned spaces, a manipulator is intrinsically redundant when the dimension of the operational space is
smaller than the dimension of the joint space (m < n). Redundancy is, anyhow, a concept relative to the task assigned to the manipulator; a manipulator
can be redundant with respect to a task and nonredundant with respect to
another. Even in the case of m = n, a manipulator can be functionally redundant when only a number of r components of operational space are of concern
for the speciﬁc task, with r < m.
Consider again the three-DOF planar arm of Sect. 2.9.1. If only the endeﬀector position (in the plane) is speciﬁed, that structure presents a functional
redundancy (n = m = 3, r = 2); this is lost when also the end-eﬀector
orientation in the plane is speciﬁed (n = m = r = 3). On the other hand, a
four-DOF planar arm is intrinsically redundant (n = 4, m = 3).
Yet, take the typical industrial robot with six DOFs; such manipulator
is not intrinsically redundant (n = m = 6), but it can become functionally
redundant with regard to the task to execute. Thus, for instance, in a lasercutting task a functional redundancy will occur since the end-eﬀector rotation
about the approach direction is irrelevant to completion of the task (r = 5).
At this point, a question should arise spontaneously: Why to intentionally
utilize a redundant manipulator? The answer is to recognize that redundancy
can provide the manipulator with dexterity and versatility in its motion. The
typical example is constituted by the human arm that has seven DOFs: three
in the shoulder, one in the elbow and three in the wrist, without considering
the DOFs in the ﬁngers. This manipulator is intrinsically redundant; in fact,
if the base and the hand position and orientation are both ﬁxed — requiring
six DOFs — the elbow can be moved, thanks to the additional available DOF.
Then, for instance, it is possible to avoid obstacles in the workspace. Further,
if a joint of a redundant manipulator reaches its mechanical limit, there might
be other joints that allow execution of the prescribed end-eﬀector motion.
A formal treatment of redundancy will be presented in the following chapter.

2.11 Kinematic Calibration
The Denavit–Hartenberg parameters for direct kinematics need to be computed as precisely as possible in order to improve manipulator accuracy. Kinematic calibration techniques are devoted to ﬁnding accurate estimates of DH
parameters from a series of measurements on the manipulator’s end-eﬀector
pose. Hence, they do not allow direct measurement of the geometric parameters of the structure.
Consider the direct kinematics equation in (2.82) which can be rewritten
by emphasizing the dependence of the operational space variables on the ﬁxed
DH parameters, besides the joint variables. Let a = [ a1 . . . an ]T , α =

2.11 Kinematic Calibration

89

[ α1 . . . αn ]T , d = [ d1 . . . dn ]T , and ϑ = [ θ1 . . . θn ]T denote the
vectors of DH parameters for the whole structure; then (2.82) becomes
xe = k(a, α, d, ϑ).

(2.84)

The manipulator’s end-eﬀector pose should be measured with high precision
for the eﬀectiveness of the kinematic calibration procedure. To this purpose
a mechanical apparatus can be used that allows the end-eﬀector to be constrained at given poses with a priori known precision. Alternatively, direct
measurement systems of object position and orientation in the Cartesian space
can be used which employ triangulation techniques.
Let xm be the measured pose and xn the nominal pose that can be computed via (2.84) with the nominal values of the parameters a, α, d, ϑ. The
nominal values of the ﬁxed parameters are set equal to the design data of the
mechanical structure, whereas the nominal values of the joint variables are set
equal to the data provided by the position transducers at the given manipulator posture. The deviation Δx = xm − xn gives a measure of accuracy at the
given posture. On the assumption of small deviations, at ﬁrst approximation,
it is possible to derive the following relation from (2.84):
Δx =

∂k
∂k
∂k
∂k
Δa +
Δα +
Δd +
Δϑ
∂a
∂α
∂d
∂ϑ

(2.85)

where Δa, Δα, Δd, Δϑ denote the deviations between the values of the
parameters of the real structure and the nominal ones. Moreover, ∂k/∂a,
∂k/∂α, ∂k/∂d, ∂k/∂ϑ denote the (m × n) matrices whose elements are the
partial derivatives of the components of the direct kinematics function with
respect to the single parameters.13
Group the parameters in the (4n × 1) vector ζ = [ aT αT dT ϑT ]T .
Let Δζ = ζ m −ζ n denote the parameter variations with respect to the nominal
values, and Φ = [ ∂k/∂a ∂k/∂α ∂k/∂d ∂k/∂ϑ ] the (m × 4n) kinematic
calibration matrix computed for the nominal values of the parameters ζ n .
Then (2.85) can be compactly rewritten as
Δx = Φ(ζ n )Δζ.

(2.86)

It is desired to compute Δζ starting from the knowledge of ζ n , xn and the
measurement of xm . Since (2.86) constitutes a system of m equations into
4n unknowns with m < 4n, a suﬃcient number of end-eﬀector pose measurements has to be performed so as to obtain a system of at least 4n equations.
Therefore, if measurements are made for a number of l poses, (2.86) yields
⎤ ⎡
⎤
⎡
Φ1
Δx1
.
.
(2.87)
Δx̄ = ⎣ .. ⎦ = ⎣ .. ⎦ Δζ = Φ̄Δζ.
Δxl
Φl
13

These matrices are the Jacobians of the transformations between the parameter
space and the operational space.

90

2 Kinematics

As regards the nominal values of the parameters needed for the computation
of the matrices Φi , it should be observed that the geometric parameters are
constant whereas the joint variables depend on the manipulator conﬁguration
at pose i.
In order to avoid ill-conditioning of matrix Φ̄, it is advisable to choose l
so that lm
4n and then solve (2.87) with a least-squares technique; in this
case the solution is of the form
Δζ = (Φ̄ Φ̄)−1 Φ̄ Δx̄
T

T

T

(2.88)

T

where (Φ̄ Φ̄)−1 Φ̄ is the left pseudo-inverse matrix of Φ̄.14 By computing Φ̄
with the nominal values of the parameters ζ n , the ﬁrst parameter estimate is
given by
(2.89)
ζ  = ζ n + Δζ.
This is a nonlinear parameter estimate problem and, as such, the procedure
should be iterated until Δζ converges within a given threshold. At each iteration, the calibration matrix Φ̄ is to be updated with the parameter estimates
ζ  obtained via (2.89) at the previous iteration. In a similar manner, the deviation Δx̄ is to be computed as the diﬀerence between the measured values
for the l end-eﬀector poses and the corresponding poses computed by the direct kinematics function with the values of the parameters at the previous
iteration. As a result of the kinematic calibration procedure, more accurate
estimates of the real manipulator geometric parameters as well as possible
corrections to make on the joint transducers measurements are obtained.
Kinematic calibration is an operation that is performed by the robot manufacturer to guarantee the accuracy reported in the data sheet. There is another
kind of calibration that is performed by the robot user which is needed for the
measurement system start-up to guarantee that the position transducers data
are consistent with the attained manipulator posture. For instance, in the
case of incremental (nonabsolute) position transducers, such calibration consists of taking the mechanical structure into a given reference posture (home)
and initializing the position transducers with the values at that posture.

2.12 Inverse Kinematics Problem
The direct kinematics equation, either in the form (2.50) or in the form (2.82),
establishes the functional relationship between the joint variables and the endeﬀector position and orientation. The inverse kinematics problem consists of
the determination of the joint variables corresponding to a given end-eﬀector
position and orientation. The solution to this problem is of fundamental importance in order to transform the motion speciﬁcations, assigned to the endeﬀector in the operational space, into the corresponding joint space motions
that allow execution of the desired motion.
14

See Sect. A.7 for the deﬁnition of the pseudo-inverse of a matrix.

2.12 Inverse Kinematics Problem

91

As regards the direct kinematics equation in (2.50), the end-eﬀector position and rotation matrix are computed in a unique manner, once the joint
variables are known15 . On the other hand, the inverse kinematics problem is
much more complex for the following reasons:
• The equations to solve are in general nonlinear, and thus it is not always
possible to ﬁnd a closed-form solution.
• Multiple solutions may exist.
• Inﬁnite solutions may exist, e.g., in the case of a kinematically redundant
manipulator.
• There might be no admissible solutions, in view of the manipulator kinematic structure.
The existence of solutions is guaranteed only if the given end-eﬀector position
and orientation belong to the manipulator dexterous workspace.
On the other hand, the problem of multiple solutions depends not only on
the number of DOFs but also on the number of non-null DH parameters; in
general, the greater the number of non-null parameters, the greater the number of admissible solutions. For a six-DOF manipulator without mechanical
joint limits, there are in general up to 16 admissible solutions. Such occurrence demands some criterion to choose among admissible solutions (e.g., the
elbow-up/elbow-down case of Example 2.6). The existence of mechanical joint
limits may eventually reduce the number of admissible multiple solutions for
the real structure.
Computation of closed-form solutions requires either algebraic intuition to
ﬁnd those signiﬁcant equations containing the unknowns or geometric intuition to ﬁnd those signiﬁcant points on the structure with respect to which
it is convenient to express position and/or orientation as a function of a reduced number of unknowns. The following examples will point out the ability
required to an inverse kinematics problem solver. On the other hand, in all
those cases when there are no — or it is diﬃcult to ﬁnd — closed-form solutions, it might be appropriate to resort to numerical solution techniques;
these clearly have the advantage of being applicable to any kinematic structure, but in general they do not allow computation of all admissible solutions.
In the following chapter, it will be shown how suitable algorithms utilizing
the manipulator Jacobian can be employed to solve the inverse kinematics
problem.
2.12.1 Solution of Three-link Planar Arm
Consider the arm shown in Fig. 2.20 whose direct kinematics was given
in (2.63). It is desired to ﬁnd the joint variables ϑ1 , ϑ2 , ϑ3 corresponding
to a given end-eﬀector position and orientation.
15

In general, this cannot be said for (2.82) too, since the Euler angles are not
uniquely deﬁned.

92

2 Kinematics

As already pointed out, it is convenient to specify position and orientation
in terms of a minimal number of parameters: the two coordinates px , py and
the angle φ with axis x0 , in this case. Hence, it is possible to refer to the direct
kinematics equation in the form (2.83).
A ﬁrst algebraic solution technique is illustrated below. Having speciﬁed
the orientation, the relation
φ = ϑ1 + ϑ2 + ϑ3

(2.90)

is one of the equations of the system to solve16 . From (2.63) the following
equations can be obtained:
pW x = px − a3 cφ = a1 c1 + a2 c12
pW y = py − a3 sφ = a1 s1 + a2 s12

(2.91)
(2.92)

which describe the position of point W , i.e., the origin of Frame 2; this depends
only on the ﬁrst two angles ϑ1 and ϑ2 . Squaring and summing (2.91), (2.92)
yields
p2W x + p2W y = a21 + a22 + 2a1 a2 c2
from which

p2W x + p2W y − a21 − a22
.
2a1 a2
The existence of a solution obviously imposes that −1 ≤ c2 ≤ 1, otherwise
the given point would be outside the arm reachable workspace. Then, set

s2 = ± 1 − c22 ,
c2 =

where the positive sign is relative to the elbow-down posture and the negative
sign to the elbow-up posture. Hence, the angle ϑ2 can be computed as
ϑ2 = Atan2(s2 , c2 ).
Having determined ϑ2 , the angle ϑ1 can be found as follows. Substituting
ϑ2 into (2.91), (2.92) yields an algebraic system of two equations in the two
unknowns s1 and c1 , whose solution is
s1 =

(a1 + a2 c2 )pW y − a2 s2 pW x
p2W x + p2W y

c1 =

(a1 + a2 c2 )pW x + a2 s2 pW y
.
p2W x + p2W y

In analogy to the above, it is
ϑ1 = Atan2(s1 , c1 ).
16

If φ is not speciﬁed, then the arm is redundant and there exist inﬁnite solutions
to the inverse kinematics problem.

2.12 Inverse Kinematics Problem

93

Fig. 2.31. Admissible postures for a two-link planar arm

In the case when s2 = 0, it is obviously ϑ2 = 0, π; as will be shown in the
following, in such a posture the manipulator is at a kinematic singularity. Yet,
the angle ϑ1 can be determined uniquely, unless a1 = a2 and it is required
pW x = pW y = 0.
Finally, the angle ϑ3 is found from (2.90) as
ϑ3 = φ − ϑ1 − ϑ2 .
An alternative geometric solution technique is presented below. As above,
the orientation angle is given as in (2.90) and the coordinates of the origin
of Frame 2 are computed as in (2.91), (2.92). The application of the cosine
theorem to the triangle formed by links a1 , a2 and the segment connecting
points W and O gives
p2W x + p2W y = a21 + a22 − 2a1 a2 cos (π − ϑ2 );
the two admissible conﬁgurations of the triangle are shown in Fig. 2.31. Observing that cos (π − ϑ2 ) = −cos ϑ2 leads to
p2W x + p2W y − a21 − a22
.
2a1 a2

For the existence of the triangle, it must be p2W x + p2W y ≤ a1 + a2 . This
condition is not satisﬁed when the given point is outside the arm reachable
workspace. Then, under the assumption of admissible solutions, it is
c2 =

ϑ2 = ±cos −1 (c2 );
the elbow-up posture is obtained for ϑ2 ∈ (−π, 0) while the elbow-down posture is obtained for ϑ2 ∈ (0, π).
To ﬁnd ϑ1 consider the angles α and β in Fig. 2.31. Notice that the determination of α depends on the sign of pW x and pW y ; then, it is necessary to
compute α as
α = Atan2(pW y , pW x ).

94

2 Kinematics

To compute β, applying again the cosine theorem yields

cβ p2W x + p2W y = a1 + a2 c2
and resorting to the expression of c2 given above leads to
⎛
⎞
2
2
2
2
p
+
p
+
a
−
a
1
2⎠
Wx
 Wy
β = cos −1 ⎝
2
2
2a1 pW x + pW y
with β ∈ (0, π) so as to preserve the existence of triangles. Then, it is
ϑ1 = α ± β,
where the positive sign holds for ϑ2 < 0 and the negative sign for ϑ2 > 0.
Finally, ϑ3 is computed from (2.90).
It is worth noticing that, in view of the substantial equivalence between
the two-link planar arm and the parallelogram arm, the above techniques can
be formally applied to solve the inverse kinematics of the arm in Sect. 2.9.2.
2.12.2 Solution of Manipulators with Spherical Wrist
Most of the existing manipulators are kinematically simple, since they are
typically formed by an arm, of the kind presented above, and a spherical wrist;
see the manipulators in Sects. 2.9.6–2.9.8. This choice is partly motivated by
the diﬃculty to ﬁnd solutions to the inverse kinematics problem in the general
case. In particular, a six -DOF kinematic structure has closed-form inverse
kinematics solutions if:
• three consecutive revolute joint axes intersect at a common point, like for
the spherical wrist;
• three consecutive revolute joint axes are parallel.
In any case, algebraic or geometric intuition is required to obtain closed-form
solutions.
Inspired by the previous solution to a three-link planar arm, a suitable
point along the structure can be found whose position can be expressed both as
a function of the given end-eﬀector position and orientation and as a function
of a reduced number of joint variables. This is equivalent to articulating the
inverse kinematics problem into two subproblems, since the solution for the
position is decoupled from that for the orientation.
For a manipulator with spherical wrist, the natural choice is to locate such
point W at the intersection of the three terminal revolute axes (Fig. 2.32). In
fact, once the end-eﬀector position and orientation are speciﬁed in terms of
pe and Re = [ ne se ae ], the wrist position can be found as
pW = pe − d6 ae

(2.93)

2.12 Inverse Kinematics Problem

95

Fig. 2.32. Manipulator with spherical wrist

which is a function of the sole joint variables that determine the arm position17 . Hence, in the case of a (nonredundant) three-DOF arm, the inverse
kinematics can be solved according to the following steps:
•
•
•
•
•

Compute the wrist position pW (q1 , q2 , q3 ) as in (2.93).
Solve inverse kinematics for (q1 , q2 , q3 ).
Compute R03 (q1 , q2 , q3 ).
Compute R36 (ϑ4 , ϑ5 , ϑ6 ) = R03 T R.
Solve inverse kinematics for orientation (ϑ4 , ϑ5 , ϑ6 ).

Therefore, on the basis of this kinematic decoupling, it is possible to solve
the inverse kinematics for the arm separately from the inverse kinematics
for the spherical wrist. Below are presented the solutions for two typical arms
(spherical and anthropomorphic) as well as the solution for the spherical wrist.
2.12.3 Solution of Spherical Arm
Consider the spherical arm shown in Fig. 2.22, whose direct kinematics was
given in (2.65). It is desired to ﬁnd the joint variables ϑ1 , ϑ2 , d3 corresponding
to a given end-eﬀector position pW .
In order to separate the variables on which pW depends, it is convenient to
express the position of pW with respect to Frame 1; then, consider the matrix
equation
(A01 )−1 T 03 = A12 A23 .

17

Note that the same reasoning was implicitly adopted in Sect. 2.12.1 for the threelink planar arm; pW described the one-DOF wrist position for the two-DOF arm
obtained by considering only the ﬁrst two links.

96

2 Kinematics

Equating the ﬁrst three elements of the fourth columns of the matrices on
both sides yields
⎤ ⎡
⎤
⎡
d3 s2
pW x c1 + pW y s1
⎦ = ⎣ −d3 c2 ⎦
(2.94)
−pW z
p1W = ⎣
−pW x s1 + pW y c1
d2
which depends only on ϑ2 and d3 . To solve this equation, set
t = tan

ϑ1
2

so that

1 − t2
2t
s1 =
.
2
1+t
1 + t2
Substituting this equation in the third component on the left-hand side
of (2.94) gives
(d2 + pW y )t2 + 2pW x t + d2 − pW y = 0,
c1 =

whose solution is
−pW x ±
t=



p2W x + p2W y − d22

d 2 + pW y

.

The two solutions correspond to two diﬀerent postures. Hence, it is



ϑ1 = 2Atan2 −pW x ± p2W x + p2W y − d22 , d2 + pW y .
Once ϑ1 is known, squaring and summing the ﬁrst two components of (2.94)
yields

d3 = (pW x c1 + pW y s1 )2 + p2W z ,
where only the solution with d3 ≥ 0 has been considered. Note that the same
value of d3 corresponds to both solutions for ϑ1 . Finally, if d3 = 0, from the
ﬁrst two components of (2.94) it is
pW x c1 + pW y s1
d3 s2
=
,
−pW z
−d3 c2
from which
ϑ2 = Atan2(pW x c1 + pW y s1 , pW z ).
Notice that, if d3 = 0, then ϑ2 cannot be uniquely determined.
2.12.4 Solution of Anthropomorphic Arm
Consider the anthropomorphic arm shown in Fig. 2.23. It is desired to ﬁnd
the joint variables ϑ1 , ϑ2 , ϑ3 corresponding to a given end-eﬀector position
pW . Notice that the direct kinematics for pW is expressed by (2.66) which can

2.12 Inverse Kinematics Problem

97

be obtained from (2.70) by setting d6 = 0, d4 = a3 and replacing ϑ3 with the
angle ϑ3 + π/2 because of the misalignment of the Frames 3 for the structures
in Fig. 2.23 and in Fig. 2.26, respectively. Hence, it follows
pW x = c1 (a2 c2 + a3 c23 )

(2.95)

pW y = s1 (a2 c2 + a3 c23 )
pW z = a2 s2 + a3 s23 .

(2.96)
(2.97)

Proceeding as in the case of the two-link planar arm, it is worth squaring
and summing (2.95)–(2.97) yielding
p2W x + p2W y + p2W z = a22 + a23 + 2a2 a3 c3
from which
c3 =

p2W x + p2W y + p2W z − a22 − a23
2a2 a3

(2.98)

where the admissibility of 
the solution obviously requires that −1 ≤ c3 ≤ 1,
or equivalently |a2 − a3 | ≤ p2W x + p2W y + p2W z ≤ a2 + a3 , otherwise the wrist
point is outside the reachable workspace of the manipulator. Hence it is

s3 = ± 1 − c23
(2.99)
and thus
ϑ3 = Atan2(s3 , c3 )
giving the two solutions, according to the sign of s3 ,
ϑ3,I ∈ [−π, π]
ϑ3,II = −ϑ3,I .

(2.100)
(2.101)

Having determined ϑ3 , it is possible to compute ϑ2 as follows. Squaring
and summing (2.95), (2.96) gives
p2W x + p2W y = (a2 c2 + a3 c23 )2
from which


a2 c2 + a3 c23 = ± p2W x + p2W y .

(2.102)

The system of the two Eqs. (2.102), (2.97), for each of the solutions (2.100),
(2.101), admits the solutions:

± p2W x + p2W y (a2 + a3 c3 ) + pW z a3 s3
c2 =
(2.103)
a22 + a23 + 2a2 a3 c3

pW z (a2 + a3 c3 ) ∓ p2W x + p2W y a3 s3
s2 =
.
(2.104)
a22 + a23 + 2a2 a3 c3

98

2 Kinematics

From (2.103), (2.104) it follows
ϑ2 = Atan2(s2 , c2 )
which gives the four solutions for ϑ2 , according to the sign of s3 in (2.99):


ϑ2,I = Atan2 (a2 + a3 c3 )pW z − a3 s+
p2W x + p2W y ,
3


(2.105)
(a2 + a3 c3 ) p2W x + p2W y + a3 s+
3 pW z


ϑ2,II = Atan2 (a2 + a3 c3 )pW z + a3 s+
p2W x + p2W y ,
3


(2.106)
−(a2 + a3 c3 ) p2W x + p2W y + a3 s+
3 pW z
1 − c23 , and


= Atan2 (a2 + a3 c3 )pW z − a3 s−
p2W x + p2W y ,
3


p
(a2 + a3 c3 ) p2W x + p2W y + a3 s−
W
z
3


= Atan2 (a2 + a3 c3 )pW z + a3 s−
p2W x + p2W y ,
3


−(a2 + a3 c3 ) p2W x + p2W y + a3 s−
3 pW z

corresponding to s+
3 =
ϑ2,III

ϑ2,IV

(2.107)

(2.108)

2
corresponding to s−
3 = − 1 − c3 .
Finally, to compute ϑ1 , it is suﬃcient to rewrite (2.95), (2.96), using
(2.102), as

pW x = ±c1 p2W x + p2W y

pW y = ±s1 p2W x + p2W y

which, once solved, gives the two solutions:
ϑ1,I = Atan2(pW y , pW x )
ϑ1,II = Atan2(−pW y , −pW x ).
Notice that (2.110) gives18

Atan2(pW y , pW x ) − π
ϑ1,II =
Atan2(pW y , pW x ) + π
18

(2.109)
(2.110)

pW y ≥ 0
pW y < 0.

It is easy to show that Atan2(−y, −x) = −Atan2(y, −x) and



Atan2(y, −x) =

π − Atan2(y, x)
−π − Atan2(y, x)

y≥0
y < 0.

2.12 Inverse Kinematics Problem

99

Fig. 2.33. The four conﬁgurations of an anthropomorphic arm compatible with a
given wrist position

As can be recognized, there exist four solutions according to the values of
ϑ3 in (2.100), (2.101), ϑ2 in (2.105)–(2.108) and ϑ1 in (2.109), (2.110):
(ϑ1,I , ϑ2,I , ϑ3,I )

(ϑ1,I , ϑ2,III , ϑ3,II )

(ϑ1,II , ϑ2,II , ϑ3,I )

(ϑ1,II , ϑ2,IV , ϑ3,II ),

which are illustrated in Fig. 2.33: shoulder–right/elbow–up, shoulder–left/elbow–
up, shoulder–right/elbow–down, shoulder–left/elbow–down; obviously, the forearm orientation is diﬀerent for the two pairs of solutions.
Notice ﬁnally how it is possible to ﬁnd the solutions only if at least
pW x = 0

or

pW y = 0.

In the case pW x = pW y = 0, an inﬁnity of solutions is obtained, since it is
possible to determine the joint variables ϑ2 and ϑ3 independently of the value
of ϑ1 ; in the following, it will be seen that the arm in such conﬁguration is
kinematically singular (see Problem 2.18).
2.12.5 Solution of Spherical Wrist
Consider the spherical wrist shown in Fig. 2.24, whose direct kinematics was
given in (2.67). It is desired to ﬁnd the joint variables ϑ4 , ϑ5 , ϑ6 corresponding
to a given end-eﬀector orientation R36 . As previously pointed out, these angles
constitute a set of Euler angles ZYZ with respect to Frame 3. Hence, having
computed the rotation matrix
⎤
⎡ 3
nx s3x a3x
R36 = ⎣ n3y s3y a3y ⎦ ,
n3z s3z a3z

100

2 Kinematics

from its expression in terms of the joint variables in (2.67), it is possible to
compute the solutions directly as in (2.19), (2.20), i.e.,
ϑ4 = Atan2(a3y , a3x )


(a3x )2 + (a3y )2 , a3z
ϑ5 = Atan2
ϑ6 =

(2.111)

Atan2(s3z , −n3z )

for ϑ5 ∈ (0, π), and
ϑ4 = Atan2(−a3y , −a3x )

 
ϑ5 = Atan2 − (a3x )2 + (a3y )2 , a3z

(2.112)

ϑ6 = Atan2(−s3z , n3z )
for ϑ5 ∈ (−π, 0).

Bibliography
The treatment of kinematics of robot manipulators can be found in several
classical robotics texts, such as [180, 10, 200, 217]. Speciﬁc texts are [23, 6,
151].
For the descriptions of the orientation of a rigid body, see [187]. Quaternion
algebra can be found in [46]; see [204] for the extraction of quaternions from
rotation matrices.
The Denavit–Hartenberg convention was ﬁrst introduced in [60]. A modiﬁed version is utilized in [53, 248, 111]. The use of homogeneous transformation
matrices for the computation of open-chain manipulator direct kinematics is
presented in [181], while in [183] suﬃcient conditions are given for the closedform computation of the inverse kinematics problem. For kinematics of closed
chains see [144, 111]. The design of the Stanford manipulator is due to [196].
The problem of kinematic calibration is considered in [188, 98]. Methods
which do not require the use of external sensors for direct measurement of
end-eﬀector position and orientation are proposed in [68].
The kinematic decoupling deriving from the spherical wrist is utilized
in [76, 99, 182]. Numerical methods for the solution of the inverse kinematics
problem based on iterative algorithms are proposed in [232, 86].

Problems
2.1. Find the rotation matrix corresponding to the set of Euler angles ZXZ.
2.2. Discuss the inverse solution for the Euler angles ZYZ in the case sϑ = 0.

Problems

101

Fig. 2.34. Four-link closed-chain planar arm with prismatic joint

2.3. Discuss the inverse solution for the Roll–Pitch–Yaw angles in the case
cϑ = 0.
2.4. Verify that the rotation matrix corresponding to the rotation by an angle
about an arbitrary axis is given by (2.25).
2.5. Prove that the angle and the unit vector of the axis corresponding to a
rotation matrix are given by (2.27), (2.28). Find inverse formulae in the case
of sin ϑ = 0.
2.6. Verify that the rotation matrix corresponding to the unit quaternion is
given by (2.33).
2.7. Prove that the unit quaternion is invariant with respect to the rotation
matrix and its transpose, i.e., R(η, ) = RT (η, ) = .
2.8. Prove that the unit quaternion corresponding to a rotation matrix is
given by (2.34), (2.35).
2.9. Prove that the quaternion product is expressed by (2.37).
2.10. By applying the rules for inverting a block-partitioned matrix, prove
that matrix A10 is given by (2.45).
2.11. Find the direct kinematics equation of the four-link closed-chain planar
arm in Fig. 2.34, where the two links connected by the prismatic joint are
orthogonal to each other
2.12. Find the direct kinematics equation for the cylindrical arm in Fig. 2.35.
2.13. Find the direct kinematics equation for the SCARA manipulator in
Fig. 2.36.
2.14. Find the complete direct kinematics equation for the humanoid manipulator in Fig. 2.28.

102

2 Kinematics

Fig. 2.35. Cylindrical arm

2.15. For the set of minimal representations of orientation φ, deﬁne the sum
operation in terms of the composition of rotations. By means of an example,
show that the commutative property does not hold for that operation.
2.16. Consider the elementary rotations about coordinate axes given by inﬁnitesimal angles. Show that the rotation resulting from any two elementary
rotations does not depend on the order of rotations. [Hint: for an inﬁnitesimal
angle dφ, approximate cos (dφ) ≈ 1 and sin (dφ) ≈ dφ . . . ]. Further, deﬁne
R(dφx , dφy , dφz ) = Rx (dφx )Ry (dφy )Rz (dφz ); show that
R(dφx , dφy , dφz )R(dφx , dφy , dφz ) = R(dφx + dφx , dφy + dφy , dφz + dφz ).
2.17. Draw the workspace of the three-link planar arm in Fig. 2.20 with the
data:
a1 = 0.5
a2 = 0.3
a3 = 0.2
−π/3 ≤ q1 ≤ π/3

− 2π/3 ≤ q2 ≤ 2π/3

− π/2 ≤ q3 ≤ π/2.

2.18. With reference to the inverse kinematics of the anthropomorphic arm
in Sect. 2.12.4, discuss the number of solutions in the singular cases of s3 = 0
and pW x = pW y = 0.
2.19. Solve the inverse kinematics for the cylindrical arm in Fig. 2.35.
2.20. Solve the inverse kinematics for the SCARA manipulator in Fig. 2.36.

Problems

Fig. 2.36. SCARA manipulator

103

3
Diﬀerential Kinematics and Statics

In the previous chapter, direct and inverse kinematics equations establishing
the relationship between the joint variables and the end-eﬀector pose were
derived. In this chapter, diﬀerential kinematics is presented which gives the
relationship between the joint velocities and the corresponding end-eﬀector
linear and angular velocity. This mapping is described by a matrix, termed
geometric Jacobian, which depends on the manipulator conﬁguration. Alternatively, if the end-eﬀector pose is expressed with reference to a minimal
representation in the operational space, it is possible to compute the Jacobian matrix via diﬀerentiation of the direct kinematics function with respect
to the joint variables. The resulting Jacobian, termed analytical Jacobian, in
general diﬀers from the geometric one. The Jacobian constitutes one of the
most important tools for manipulator characterization; in fact, it is useful for
ﬁnding singularities, analyzing redundancy, determining inverse kinematics
algorithms, describing the mapping between forces applied to the end-eﬀector
and resulting torques at the joints (statics) and, as will be seen in the following chapters, deriving dynamic equations of motion and designing operational
space control schemes. Finally, the kineto-statics duality concept is illustrated,
which is at the basis of the deﬁnition of velocity and force manipulability ellipsoids.

3.1 Geometric Jacobian
Consider an n-DOF manipulator. The direct kinematics equation can be written in the form
⎤
⎡
⎢
T e (q) = ⎣

Re (q)
T

0
T

pe (q) ⎥
⎦

(3.1)

1

where q = [ q1 . . . qn ] is the vector of joint variables. Both end-eﬀector
position and orientation vary as q varies.

106

3 Diﬀerential Kinematics and Statics

The goal of the diﬀerential kinematics is to ﬁnd the relationship between
the joint velocities and the end-eﬀector linear and angular velocities. In other
words, it is desired to express the end-eﬀector linear velocity ṗe and angular
velocity ω e as a function of the joint velocities q̇. As will be seen afterwards,
the sought relations are both linear in the joint velocities, i.e.,
ṗe = J P (q)q̇

(3.2)

ω e = J O (q)q̇.

(3.3)

In (3.2) J P is the (3 × n) matrix relating the contribution of the joint velocities q̇ to the end-eﬀector linear velocity ṗe , while in (3.3) J O is the (3 × n)
matrix relating the contribution of the joint velocities q̇ to the end-eﬀector
angular velocity ω e . In compact form, (3.2), (3.3) can be written as


ṗe
= J (q)q̇
(3.4)
ve =
ωe
which represents the manipulator diﬀerential kinematics equation. The (6×n)
matrix J is the manipulator geometric Jacobian


JP
J=
,
(3.5)
JO
which in general is a function of the joint variables.
In order to compute the geometric Jacobian, it is worth recalling a number
of properties of rotation matrices and some important results of rigid body
kinematics.
3.1.1 Derivative of a Rotation Matrix
The manipulator direct kinematics equation in (3.1) describes the end-eﬀector
pose, as a function of the joint variables, in terms of a position vector and a
rotation matrix. Since the aim is to characterize the end-eﬀector linear and
angular velocities, it is worth considering ﬁrst the derivative of a rotation
matrix with respect to time.
Consider a time-varying rotation matrix R = R(t). In view of the orthogonality of R, one has the relation
R(t)RT (t) = I
which, diﬀerentiated with respect to time, gives the identity
T

Ṙ(t)RT (t) + R(t)Ṙ (t) = O.
Set
S(t) = Ṙ(t)RT (t);

(3.6)

3.1 Geometric Jacobian

107

the (3 × 3) matrix S is skew-symmetric since
S(t) + S T (t) = O.

(3.7)

Postmultiplying both sides of (3.6) by R(t) gives
Ṙ(t) = S(t)R(t)

(3.8)

that allows the time derivative of R(t) to be expressed as a function of R(t)
itself.
Equation (3.8) relates the rotation matrix R to its derivative by means
of the skew-symmetric operator S and has a meaningful physical interpretation. Consider a constant vector p and the vector p(t) = R(t)p . The time
derivative of p(t) is
ṗ(t) = Ṙ(t)p ,
which, in view of (3.8), can be written as
ṗ(t) = S(t)R(t)p .
If the vector ω(t) denotes the angular velocity of frame R(t) with respect to
the reference frame at time t, it is known from mechanics that
ṗ(t) = ω(t) × R(t)p .
Therefore, the matrix operator S(t) describes the vector product between the
vector ω and the vector R(t)p . The matrix S(t) is so that its symmetric
elements with respect to the main diagonal represent the components of the
vector ω(t) = [ ωx ωy ωz ]T in the form
⎡
⎤
0
−ωz ωy
S = ⎣ ωz
(3.9)
0
−ωx ⎦ ,
−ωy ωx
0
which justiﬁes the expression S(t) = S(ω(t)). Hence, (3.8) can be rewritten
as
Ṙ = S(ω)R.
(3.10)
Furthermore, if R denotes a rotation matrix, it can be shown that the
following relation holds:
RS(ω)RT = S(Rω)
which will be useful later (see Problem 3.1).

(3.11)

108

3 Diﬀerential Kinematics and Statics

Example 3.1
Consider the elementary rotation matrix about axis z given in (2.6). If α is a function
of time, by computing the time derivative of Rz (α(t)), (3.6) becomes





−α̇ sin α −α̇ cos α 0
S(t) = α̇ cos α −α̇ sin α 0
0
0
0


0 −α̇ 0
= α̇
0
0 = S(ω(t)).
0
0
0

cos α
−sin α
0

sin α
cos α
0

0
0
1



According to (3.9), it is
ω = [0

0

α̇ ]T

that expresses the angular velocity of the frame about axis z.

With reference to Fig. 2.11, consider the coordinate transformation of a
point P from Frame 1 to Frame 0; in view of (2.38), this is given by
p0 = o01 + R01 p1 .

(3.12)

Diﬀerentiating (3.12) with respect to time gives
0

ṗ0 = ȯ01 + R01 ṗ1 + Ṙ1 p1 ;

(3.13)

utilizing the expression of the derivative of a rotation matrix (3.8) and specifying the dependence on the angular velocity gives
ṗ0 = ȯ01 + R01 ṗ1 + S(ω 01 )R01 p1 .
Further, denoting the vector R01 p1 by r 01 , it is
ṗ0 = ȯ01 + R01 ṗ1 + ω 01 × r 01

(3.14)

which is the known form of the velocity composition rule.
Notice that, if p1 is ﬁxed in Frame 1, then it is
ṗ0 = ȯ01 + ω 01 × r 01

(3.15)

since ṗ1 = 0.
3.1.2 Link Velocities
Consider the generic Link i of a manipulator with an open kinematic chain.
According to the Denavit–Hartenberg convention adopted in the previous
chapter, Link i connects Joints i and i + 1; Frame i is attached to Link i

3.1 Geometric Jacobian

109

Fig. 3.1. Characterization of generic Link i of a manipulator

and has origin along Joint i + 1 axis, while Frame i − 1 has origin along Joint i
axis (Fig. 3.1).
Let pi−1 and pi be the position vectors of the origins of Frames i − 1 and i,
respectively. Also, let r i−1
i−1,i denote the position of the origin of Frame i with
respect to Frame i − 1 expressed in Frame i − 1. According to the coordinate
transformation (3.10), one can write1
pi = pi−1 + Ri−1 r i−1
i−1,i .
Then, by virtue of (3.14), it is
i−1
ṗi = ṗi−1 + Ri−1 ṙ i−1
i−1,i + ω i−1 × Ri−1 r i−1,i = ṗi−1 + v i−1,i + ω i−1 × r i−1,i
(3.16)
which gives the expression of the linear velocity of Link i as a function of the
translational and rotational velocities of Link i − 1. Note that v i−1,i denotes
the velocity of the origin of Frame i with respect to the origin of Frame i − 1.
Concerning link angular velocity, it is worth starting from the rotation
composition
;
Ri = Ri−1 Ri−1
i

from (3.8), its time derivative can be written as
i−1
S(ω i )Ri = S(ω i−1 )Ri + Ri−1 S(ω i−1
i−1,i )Ri

(3.17)

where ω i−1
i−1,i denotes the angular velocity of Frame i with respect to Frame
i − 1 expressed in Frame i − 1. From (2.4), the second term on the right-hand
side of (3.17) can be rewritten as
i−1
T
i−1
= Ri−1 S(ω i−1
;
Ri−1 S(ω i−1
i−1,i )Ri
i−1,i )Ri−1 Ri−1 Ri
1

Hereafter, the indication of superscript ‘0’ is omitted for quantities referred to
Frame 0. Also, without loss of generality, Frame 0 and Frame n are taken as the
base frame and the end-eﬀector frame, respectively.

110

3 Diﬀerential Kinematics and Statics

in view of property (3.11), it is
i−1
Ri−1 S(ω i−1
= S(Ri−1 ω i−1
i−1,i )Ri
i−1,i )Ri .

Then, (3.17) becomes
S(ω i )Ri = S(ω i−1 )Ri + S(Ri−1 ω i−1
i−1,i )Ri
leading to the result
ω i = ω i−1 + Ri−1 ω i−1
i−1,i = ω i−1 + ω i−1,i ,

(3.18)

which gives the expression of the angular velocity of Link i as a function of
the angular velocities of Link i − 1 and of Link i with respect to Link i − 1.
The relations (3.16), (3.18) attain diﬀerent expressions depending on the
type of Joint i (prismatic or revolute).
Prismatic joint
Since orientation of Frame i with respect to Frame i − 1 does not vary by
moving Joint i, it is
ω i−1,i = 0.
(3.19)
Further, the linear velocity is
v i−1,i = d˙i z i−1

(3.20)

where z i−1 is the unit vector of Joint i axis. Hence, the expressions of angular
velocity (3.18) and linear velocity (3.16) respectively become
ω i = ω i−1
ṗi = ṗi−1 + d˙i z i−1 + ω i × r i−1,i ,

(3.21)
(3.22)

where the relation ω i = ω i−1 has been exploited to derive (3.22).
Revolute joint
For the angular velocity it is obviously
ω i−1,i = ϑ̇i z i−1 ,

(3.23)

while for the linear velocity it is
v i−1,i = ω i−1,i × r i−1,i

(3.24)

due to the rotation of Frame i with respect to Frame i − 1 induced by the
motion of Joint i. Hence, the expressions of angular velocity (3.18) and linear
velocity (3.16) respectively become
ω i = ω i−1 + ϑ̇i z i−1
ṗi = ṗi−1 + ω i × r i−1,i ,
where (3.18) has been exploited to derive (3.26).

(3.25)
(3.26)

3.1 Geometric Jacobian

111

Fig. 3.2. Representation of vectors needed for the computation of the velocity
contribution of a revolute joint to the end-eﬀector linear velocity

3.1.3 Jacobian Computation
In order to compute the Jacobian, it is convenient to proceed separately for
the linear velocity and the angular velocity.
For the contribution to the linear velocity, the time derivative of pe (q) can
be written as
n
n


∂pe
ṗe =
q̇i =
jP i q̇i .
(3.27)
∂qi
i=1
i=1
This expression shows how ṗe can be obtained as the sum of the terms q̇i jP i .
Each term represents the contribution of the velocity of single Joint i to the
end-eﬀector linear velocity when all the other joints are still.
Therefore, by distinguishing the case of a prismatic joint (qi = di ) from
the case of a revolute joint (qi = ϑi ), it is:
• If Joint i is prismatic, from (3.20) it is
q̇i jP i = d˙i z i−1
and then
jP i = z i−1 .
• If Joint i is revolute, observing that the contribution to the linear velocity
is to be computed with reference to the origin of the end-eﬀector frame
(Fig. 3.2), it is
q̇i jP i = ω i−1,i × r i−1,e = ϑ̇i z i−1 × (pe − pi−1 )
and then
jP i = z i−1 × (pe − pi−1 ).

112

3 Diﬀerential Kinematics and Statics

For the contribution to the angular velocity, in view of (3.18), it is
ωe = ωn =

n


ω i−1,i =

i=1

n


jOi q̇i ,

(3.28)

i=1

where (3.19) and (3.23) have been utilized to characterize the terms q̇i jOi ,
and thus in detail:
• If Joint i is prismatic, from (3.19) it is
q̇i jOi = 0
and then
jOi = 0.
• If Joint i is revolute, from (3.23) it is
q̇i jOi = ϑ̇i z i−1
and then
jOi = z i−1 .
In summary, the Jacobian in (3.5) can be partitioned into the (3 × 1)
column vectors jP i and jOi as
⎤
⎡
jP 1
jP n
⎦,
(3.29)
J =⎣
...
jOn
jO1
where


jP i
jOi




⎧
z i−1
⎪
⎪
⎨
0

= 
⎪
⎪
⎩ z i−1 × (pe − pi−1 )
z i−1

for a prismatic joint
(3.30)
for a revolute joint.

The expressions in (3.30) allow Jacobian computation in a simple, systematic
way on the basis of direct kinematics relations. In fact, the vectors z i−1 , pe
and pi−1 are all functions of the joint variables. In particular:
• z i−1 is given by the third column of the rotation matrix R0i−1 , i.e.,
z i−1 = R01 (q1 ) . . . Ri−2
i−1 (qi−1 )z 0

(3.31)

where z 0 = [ 0 0 1 ]T allows the selection of the third column.
• pe is given by the ﬁrst three elements of the fourth column of the transformation matrix T 0e , i.e., by expressing pe in the (4 × 1) homogeneous
form
pe = A01 (q1 ) . . . An−1
(qn )p0
(3.32)
n
where p0 = [ 0

0

0

1 ]T allows the selection of the fourth column.

3.2 Jacobian of Typical Manipulator Structures

113

• pi−1 is given by the ﬁrst three elements of the fourth column of the transformation matrix T 0i−1 , i.e., it can be extracted from
pi−1 = A01 (q1 ) . . . Ai−2
i−1 (qi−1 )p0 .

(3.33)

The above equations can be conveniently used to compute the translational
and rotational velocities of any point along the manipulator structure, as long
as the direct kinematics functions relative to that point are known.
Finally, notice that the Jacobian matrix depends on the frame in which
the end-eﬀector velocity is expressed. The above equations allow computation
of the geometric Jacobian with respect to the base frame. If it is desired to
represent the Jacobian in a diﬀerent Frame u, it is suﬃcient to know the
relative rotation matrix Ru . The relationship between velocities in the two
frames is
 u  u


R
O
ṗe
ṗe
=
,
ω ue
ωe
O Ru
which, substituted in (3.4), gives
 u  u
R
ṗe
=
ω ue
O
and then



Ru
J =
O
u


O
J q̇
Ru

O
J,
Ru

(3.34)

where J u denotes the geometric Jacobian in Frame u, which has been assumed
to be time-invariant.

3.2 Jacobian of Typical Manipulator Structures
In the following, the Jacobian is computed for some of the typical manipulator
structures presented in the previous chapter.
3.2.1 Three-link Planar Arm
In this case, from (3.30) the Jacobian is

z 0 × (p3 − p0 ) z 1 × (p3 − p1 )
J (q) =
z0
z1


z 2 × (p3 − p2 )
.
z2

Computation of the position vectors of the various links gives
⎤
⎤
⎡ ⎤
⎡
⎡
0
a1 c1
a1 c1 + a2 c12
p0 = ⎣ 0 ⎦ p1 = ⎣ a1 s1 ⎦ p2 = ⎣ a1 s1 + a2 s12 ⎦
0
0
0

114

3 Diﬀerential Kinematics and Statics

⎤
a1 c1 + a2 c12 + a3 c123
p3 = ⎣ a1 s1 + a2 s12 + a3 s123 ⎦
0
⎡

while computation of the unit vectors of revolute joint axes gives
⎡ ⎤
0
z0 = z1 = z2 = ⎣ 0 ⎦
1
since they are all parallel to axis z0 . From (3.29) it is
⎡

−a1 s1 − a2 s12 − a3 s123
⎢ a1 c1 + a2 c12 + a3 c123
⎢
0
⎢
J =⎢
0
⎢
⎣
0
1

−a2 s12 − a3 s123
a2 c12 + a3 c123
0
0
0
1

⎤
−a3 s123
a3 c123 ⎥
⎥
0
⎥
⎥.
0
⎥
⎦
0
1

(3.35)

In the Jacobian (3.35), only the three non-null rows are relevant (the rank of
the matrix is at most 3); these refer to the two components of linear velocity
along axes x0 , y0 and the component of angular velocity about axis z0 . This
result can be derived by observing that three DOFs allow speciﬁcation of at
most three end-eﬀector variables; vz , ωx , ωy are always null for this kinematic
structure. If orientation is of no concern, the (2×3) Jacobian for the positional
part can be derived by considering just the ﬁrst two rows, i.e.,


−a1 s1 − a2 s12 − a3 s123 −a2 s12 − a3 s123 −a3 s123
.
(3.36)
JP =
a1 c1 + a2 c12 + a3 c123
a2 c12 + a3 c123
a3 c123
3.2.2 Anthropomorphic Arm
In this case, from (3.30) the Jacobian is

z 0 × (p3 − p0 ) z 1 × (p3 − p1 )
J=
z1
z0


z 2 × (p3 − p2 )
.
z2

Computation of the position vectors of the various links gives
⎤
⎡ ⎤
⎡
0
a2 c1 c2
p0 = p1 = ⎣ 0 ⎦ p2 = ⎣ a2 s1 c2 ⎦
a2 s2
0
⎤
⎡
c1 (a2 c2 + a3 c23 )
p3 = ⎣ s1 (a2 c2 + a3 c23 ) ⎦
a2 s2 + a3 s23

3.2 Jacobian of Typical Manipulator Structures

115

while computation of the unit vectors of revolute joint axes gives
⎤
⎡ ⎤
⎡
0
s1
z 0 = ⎣ 0 ⎦ z 1 = z 2 = ⎣ −c1 ⎦ .
0
1
From (3.29) it is
⎡

⎤
−s1 (a2 c2 + a3 c23 ) −c1 (a2 s2 + a3 s23 ) −a3 c1 s23
⎢ c1 (a2 c2 + a3 c23 ) −s1 (a2 s2 + a3 s23 ) −a3 s1 s23 ⎥
⎢
⎥
0
a2 c2 + a3 c23
a3 c23 ⎥
⎢
J =⎢
⎥.
0
s1
s1
⎢
⎥
⎣
⎦
0
−c1
−c1
1
0
0

(3.37)

Only three of the six rows of the Jacobian (3.37) are linearly independent.
Having 3 DOFs only, it is worth considering the upper (3 × 3) block of the
Jacobian
⎤
⎡
−s1 (a2 c2 + a3 c23 ) −c1 (a2 s2 + a3 s23 ) −a3 c1 s23
(3.38)
J P = ⎣ c1 (a2 c2 + a3 c23 ) −s1 (a2 s2 + a3 s23 ) −a3 s1 s23 ⎦
0
a2 c2 + a3 c23
a3 c23
that describes the relationship between the joint velocities and the end-eﬀector
linear velocity. This structure does not allow an arbitrary angular velocity ω
to be obtained; in fact, the two components ωx and ωy are not independent
(s1 ωy = −c1 ωx ).
3.2.3 Stanford Manipulator
In this case, from (3.30) it is

z 0 × (p6 − p0 ) z 1 × (p6 − p1 )
J=
z0
z1

z2
0


z 3 × (p6 − p3 ) z 4 × (p6 − p4 ) z 5 × (p6 − p5 )
.
z4
z5
z3

Computation of the position vectors of the various links gives
⎤
⎡ ⎤
⎡
0
c1 s2 d3 − s1 d2
p0 = p1 = ⎣ 0 ⎦ p3 = p4 = p5 = ⎣ s1 s2 d3 + c1 d2 ⎦
c2 d3
0
 ⎤
⎡
c1 s2 d3 − s1 d2 + c1 (c2 c4 s5 + s2 c5 ) − s1 s4 s5 d6
p6 = ⎣ s1 s2 d3 + c1 d2 + s1 (c2 c4 s5 + s2 c5 ) + c1 s4 s5 d6 ⎦ ,
c2 d3 + (−s2 c4 s5 + c2 c5 )d6

116

3 Diﬀerential Kinematics and Statics

while computation of the unit vectors of joint axes gives
⎤
⎡ ⎤
⎡
⎡
⎤
0
−s1
c1 s2
z 0 = ⎣ 0 ⎦ z 1 = ⎣ c1 ⎦ z 2 = z 3 = ⎣ s1 s2 ⎦
1
0
c2
⎤
⎤
⎡
⎡
−c1 c2 s4 − s1 c4
c1 (c2 c4 s5 + s2 c5 ) − s1 s4 s5
z 4 = ⎣ −s1 c2 s4 + c1 c4 ⎦ z 5 = ⎣ s1 (c2 c4 s5 + s2 c5 ) + c1 s4 s5 ⎦ .
s2 s4
−s2 c4 s5 + c2 c5
The sought Jacobian can be obtained by developing the computations as
in (3.29), leading to expressing end-eﬀector linear and angular velocity as
a function of joint velocities.

3.3 Kinematic Singularities
The Jacobian in the diﬀerential kinematics equation of a manipulator deﬁnes
a linear mapping
(3.39)
v e = J (q)q̇
between the vector q̇ of joint velocities and the vector v e = [ ṗTe ω Te ]T of endeﬀector velocity. The Jacobian is, in general, a function of the conﬁguration
q; those conﬁgurations at which J is rank-deﬁcient are termed kinematic
singularities. To ﬁnd the singularities of a manipulator is of great interest for
the following reasons:
a) Singularities represent conﬁgurations at which mobility of the structure
is reduced, i.e., it is not possible to impose an arbitrary motion to the
end-eﬀector.
b) When the structure is at a singularity, inﬁnite solutions to the inverse
kinematics problem may exist.
c) In the neighbourhood of a singularity, small velocities in the operational
space may cause large velocities in the joint space.
Singularities can be classiﬁed into:
• Boundary singularities that occur when the manipulator is either outstretched or retracted. It may be understood that these singularities do
not represent a true drawback, since they can be avoided on condition that
the manipulator is not driven to the boundaries of its reachable workspace.
• Internal singularities that occur inside the reachable workspace and are
generally caused by the alignment of two or more axes of motion, or else by
the attainment of particular end-eﬀector conﬁgurations. Unlike the above,
these singularities constitute a serious problem, as they can be encountered
anywhere in the reachable workspace for a planned path in the operational
space.

3.3 Kinematic Singularities

117

Fig. 3.3. Two-link planar arm at a boundary singularity

Example 3.2
To illustrate the behaviour of a manipulator at a singularity, consider a two-link
planar arm. In this case, it is worth considering only the components ṗx and ṗy of
the linear velocity in the plane. Thus, the Jacobian is the (2 × 2) matrix



J=

−a1 s1 − a2 s12
a1 c1 + a2 c12



−a2 s12
.
a2 c12

(3.40)

To analyze matrix rank, consider its determinant given by
det(J ) = a1 a2 s2 .

(3.41)

For a1 , a2 = 0, it is easy to ﬁnd that the determinant in (3.41) vanishes whenever
ϑ2 = 0

ϑ2 = π,

ϑ1 being irrelevant for the determination of singular conﬁgurations. These occur
when the arm tip is located either on the outer (ϑ2 = 0) or on the inner (ϑ2 = π)
boundary of the reachable workspace. Figure 3.3 illustrates the arm posture for
ϑ2 = 0.
By analyzing the diﬀerential motion of the structure in such conﬁguration, it
can be observed that the two column vectors [ −(a1 + a2 )s1 (a1 + a2 )c1 ]T and
[ −a2 s1 a2 c1 ]T of the Jacobian become parallel, and thus the Jacobian rank becomes one; this means that the tip velocity components are not independent (see
point a) above).

3.3.1 Singularity Decoupling
Computation of internal singularities via the Jacobian determinant may be
tedious and of no easy solution for complex structures. For manipulators having a spherical wrist, by analogy with what has already been seen for inverse
kinematics, it is possible to split the problem of singularity computation into
two separate problems:

118

3 Diﬀerential Kinematics and Statics

Fig. 3.4. Spherical wrist at a singularity

• computation of arm singularities resulting from the motion of the ﬁrst 3
or more links,
• computation of wrist singularities resulting from the motion of the wrist
joints.
For the sake of simplicity, consider the case n = 6; the Jacobian can be
partitioned into (3 × 3) blocks as follows:


J 11 J 12
J=
(3.42)
J 21 J 22
where, since the outer 3 joints are all revolute, the expressions of the two right
blocks are respectively
!
J 12 = z 3 × (pe − p3 ) z 4 × (pe − p4 ) z 5 × (pe − p5 )
!
(3.43)
J 22 = z 3 z 4 z 5 .
As singularities are typical of the mechanical structure and do not depend on
the frames chosen to describe kinematics, it is convenient to choose the origin
of the end-eﬀector frame at the intersection of the wrist axes (see Fig. 2.32).
The choice p = pW leads to
!
J 12 = 0 0 0 ,
since all vectors pW − pi are parallel to the unit vectors z i , for i = 3, 4, 5, no
matter how Frames 3, 4, 5 are chosen according to DH convention. In view of
this choice, the overall Jacobian becomes a block lower-triangular matrix. In
this case, computation of the determinant is greatly simpliﬁed, as this is given
by the product of the determinants of the two blocks on the diagonal, i.e.,
det(J ) = det(J 11 )det(J 22 ).
In turn, a true singularity decoupling has been achieved; the condition
det(J 11 ) = 0

(3.44)

3.3 Kinematic Singularities

119

Fig. 3.5. Anthropomorphic arm at an elbow singularity

leads to determining the arm singularities, while the condition
det(J 22 ) = 0
leads to determining the wrist singularities.
Notice, however, that this form of Jacobian does not provide the relationship between the joint velocities and the end-eﬀector velocity, but it leads to
simplifying singularity computation. Below the two types of singularities are
analyzed in detail.
3.3.2 Wrist Singularities
On the basis of the above singularity decoupling, wrist singularities can be
determined by inspecting the block J 22 in (3.43). It can be recognized that the
wrist is at a singular conﬁguration whenever the unit vectors z 3 , z 4 , z 5 are
linearly dependent. The wrist kinematic structure reveals that a singularity
occurs when z 3 and z 5 are aligned, i.e., whenever
ϑ5 = 0

ϑ5 = π.

Taking into consideration only the ﬁrst conﬁguration (Fig. 3.4), the loss of
mobility is caused by the fact that rotations of equal magnitude about opposite
directions on ϑ4 and ϑ6 do not produce any end-eﬀector rotation. Further, the
wrist is not allowed to rotate about the axis orthogonal to z 4 and z 3 , (see
point a) above). This singularity is naturally described in the joint space and
can be encountered anywhere inside the manipulator reachable workspace; as
a consequence, special care is to be taken in programming an end-eﬀector
motion.
3.3.3 Arm Singularities
Arm singularities are characteristic of a speciﬁc manipulator structure; to
illustrate their determination, consider the anthropomorphic arm (Fig. 2.23),

120

3 Diﬀerential Kinematics and Statics

Fig. 3.6. Anthropomorphic arm at a shoulder singularity

whose Jacobian for the linear velocity part is given by (3.38). Its determinant
is
det(J P ) = −a2 a3 s3 (a2 c2 + a3 c23 ).
Like in the case of the planar arm of Example 3.2, the determinant does not
depend on the ﬁrst joint variable.
For a2 , a3 = 0, the determinant vanishes if s3 = 0 and/or (a2 c2 + a3 c23 ) =
0. The ﬁrst situation occurs whenever
ϑ3 = 0

ϑ3 = π

meaning that the elbow is outstretched (Fig. 3.5) or retracted, and is termed
elbow singularity. Notice that this type of singularity is conceptually equivalent to the singularity found for the two-link planar arm.
By recalling the direct kinematics equation in (2.66), it can be observed
that the second situation occurs when the wrist point lies on axis z0 (Fig. 3.6);
it is thus characterized by
p x = py = 0
and is termed shoulder singularity.
Notice that the whole axis z0 describes a continuum of singular conﬁgurations; a rotation of ϑ1 does not cause any translation of the wrist position
(the ﬁrst column of J P is always null at a shoulder singularity), and then
the kinematics equation admits inﬁnite solutions; moreover, motions starting
from the singular conﬁguration that take the wrist along the z1 direction are
not allowed (see point b) above).
If a spherical wrist is connected to an anthropomorphic arm (Fig. 2.26),
the arm direct kinematics is diﬀerent. In this case the Jacobian to consider
represents the block J 11 of the Jacobian in (3.42) with p = pW . Analyzing its

3.4 Analysis of Redundancy

121

determinant leads to ﬁnding the same singular conﬁgurations, which are relative to diﬀerent values of the third joint variables, though — compare (2.66)
and (2.70).
Finally, it is important to remark that, unlike the wrist singularities, the
arm singularities are well identiﬁed in the operational space, and thus they
can be suitably avoided in the end-eﬀector trajectory planning stage.

3.4 Analysis of Redundancy
The concept of kinematic redundancy has been introduced in Sect. 2.10.2;
redundancy is related to the number n of DOFs of the structure, the number m
of operational space variables, and the number r of operational space variables
necessary to specify a given task.
In order to perform a systematic analysis of redundancy, it is worth considering diﬀerential kinematics in lieu of direct kinematics (2.82). To this end,
(3.39) is to be interpreted as the diﬀerential kinematics mapping relating the
n components of the joint velocity vector to the r ≤ m components of the velocity vector v e of concern for the speciﬁc task. To clarify this point, consider
the case of a 3-link planar arm; that is not intrinsically redundant (n = m = 3)
and its Jacobian (3.35) has 3 null rows accordingly. If the task does not specify ωz (r = 2), the arm becomes functionally redundant and the Jacobian to
consider for redundancy analysis is the one in (3.36).
A diﬀerent case is that of the anthropomorphic arm for which only position variables are of concern (n = m = 3). The relevant Jacobian is the one
in (3.38). The arm is neither intrinsically redundant nor can become functionally redundant if it is assigned a planar task; in that case, indeed, the task
would set constraints on the 3 components of end-eﬀector linear velocity.
Therefore, the diﬀerential kinematics equation to consider can be formally
written as in (3.39), i.e.,
(3.45)
v e = J (q)q̇,
where now v e is meant to be the (r × 1) vector of end-eﬀector velocity of
concern for the speciﬁc task and J is the corresponding (r × n) Jacobian
matrix that can be extracted from the geometric Jacobian; q̇ is the (n × 1)
vector of joint velocities. If r < n, the manipulator is kinematically redundant
and there exist (n − r) redundant DOFs.
The Jacobian describes the linear mapping from the joint velocity space to
the end-eﬀector velocity space. In general, it is a function of the conﬁguration.
In the context of diﬀerential kinematics, however, the Jacobian has to be
regarded as a constant matrix, since the instantaneous velocity mapping is
of interest for a given posture. The mapping is schematically illustrated in
Fig. 3.7 with a typical notation from set theory.

122

3 Diﬀerential Kinematics and Statics

Fig. 3.7. Mapping between the joint velocity space and the end-eﬀector velocity
space

The diﬀerential kinematics equation in (3.45) can be characterized in terms
of the range and null spaces of the mapping;2 speciﬁcally, one has that:
• The range space of J is the subspace R(J ) in IRr of the end-eﬀector velocities that can be generated by the joint velocities, in the given manipulator
posture.
• The null space of J is the subspace N (J ) in IRn of joint velocities that do
not produce any end-eﬀector velocity, in the given manipulator posture.
If the Jacobian has full rank , one has


dim R(J ) = r
dim N (J ) = n − r
and the range of J spans the entire space IRr . Instead, if the Jacobian degenerates at a singularity, the dimension of the range space decreases while the
dimension of the null space increases, since the following relation holds:


dim R(J ) + dim N (J ) = n
independently of the rank of the matrix J .
The existence of a subspace N (J ) = ∅ for a redundant manipulator allows
determination of systematic techniques for handling redundant DOFs. To this
end, if q̇ ∗ denotes a solution to (3.45) and P is an (n × n) matrix so that
R(P ) ≡ N (J ),
the joint velocity vector

q̇ = q̇ ∗ + P q̇ 0 ,

(3.46)

with arbitrary q̇ 0 , is also a solution to (3.45). In fact, premultiplying both
sides of (3.46) by J yields
J q̇ = J q̇ ∗ + J P q̇ 0 = J q̇ ∗ = v e
2

See Sect. A.4 for the linear mappings.

3.5 Inverse Diﬀerential Kinematics

123

since J P q̇ 0 = 0 for any q̇ 0 . This result is of fundamental importance for
redundancy resolution; a solution of the kind (3.46) points out the possibility
of choosing the vector of arbitrary joint velocities q̇ 0 so as to exploit advantageously the redundant DOFs. In fact, the eﬀect of q̇ 0 is to generate internal
motions of the structure that do not change the end-eﬀector position and orientation but may allow, for instance, manipulator reconﬁguration into more
dexterous postures for execution of a given task.

3.5 Inverse Diﬀerential Kinematics
In Sect. 2.12 it was shown how the inverse kinematics problem admits closedform solutions only for manipulators having a simple kinematic structure.
Problems arise whenever the end-eﬀector attains a particular position and/or
orientation in the operational space, or the structure is complex and it is not
possible to relate the end-eﬀector pose to diﬀerent sets of joint variables, or
else the manipulator is redundant. These limitations are caused by the highly
nonlinear relationship between joint space variables and operational space
variables.
On the other hand, the diﬀerential kinematics equation represents a linear
mapping between the joint velocity space and the operational velocity space,
although it varies with the current conﬁguration. This fact suggests the possibility to utilize the diﬀerential kinematics equation to tackle the inverse
kinematics problem.
Suppose that a motion trajectory is assigned to the end-eﬀector in terms
of v e and the initial conditions on position and orientation. The aim is to
determine a feasible joint trajectory (q(t), q̇(t)) that reproduces the given
trajectory.
By considering (3.45) with n = r, the joint velocities can be obtained via
simple inversion of the Jacobian matrix
q̇ = J −1 (q)v e .

(3.47)

If the initial manipulator posture q(0) is known, joint positions can be computed by integrating velocities over time, i.e.,
" t
q̇(ς)dς + q(0).
q(t) =
0

The integration can be performed in discrete time by resorting to numerical
techniques. The simplest technique is based on the Euler integration method;
given an integration interval Δt, if the joint positions and velocities at time
tk are known, the joint positions at time tk+1 = tk + Δt can be computed as
q(tk+1 ) = q(tk ) + q̇(tk )Δt.

(3.48)

124

3 Diﬀerential Kinematics and Statics

This technique for inverting kinematics is independent of the solvability
of the kinematic structure. Nonetheless, it is necessary that the Jacobian be
square and of full rank ; this demands further insight into the cases of redundant manipulators and kinematic singularity occurrence.
3.5.1 Redundant Manipulators
When the manipulator is redundant (r < n), the Jacobian matrix has more
columns than rows and inﬁnite solutions exist to (3.45). A viable solution
method is to formulate the problem as a constrained linear optimization problem.
In detail, once the end-eﬀector velocity v e and Jacobian J are given (for
a given conﬁguration q), it is desired to ﬁnd the solutions q̇ that satisfy the
linear equation in (3.45) and minimize the quadratic cost functional of joint
velocities3
1
g(q̇) = q̇ T W q̇
2
where W is a suitable (n × n) symmetric positive deﬁnite weighting matrix.
This problem can be solved with the method of Lagrange multipliers. Consider the modiﬁed cost functional
g(q̇, λ) =

1 T
q̇ W q̇ + λT (v e − J q̇),
2

where λ is an (r × 1) vector of unknown multipliers that allows the incorporation of the constraint (3.45) in the functional to minimize. The requested
solution has to satisfy the necessary conditions:
 T
 T
∂g
∂g
=0
= 0.
∂ q̇
∂λ
From the ﬁrst one, it is W q̇ − J T λ = 0 and thus
q̇ = W −1 J T λ

(3.49)

where the inverse of W exists. Notice that the solution (3.49) is a minimum,
since ∂ 2 g/∂ q̇ 2 = W is positive deﬁnite. From the second condition above, the
constraint
v e = J q̇
is recovered. Combining the two conditions gives
v e = J W −1 J T λ;
under the assumption that J has full rank, J W −1 J T is an (r × r) square
matrix of rank r and thus can be inverted. Solving for λ yields
λ = (J W −1 J T )−1 v e
3

Quadratic forms and the relative operations are recalled in Sect. A.6.

3.5 Inverse Diﬀerential Kinematics

125

which, substituted into (3.49), gives the sought optimal solution
q̇ = W −1 J T (J W −1 J T )−1 v e .

(3.50)

Premultiplying both sides of (3.50) by J , it is easy to verify that this solution
satisﬁes the diﬀerential kinematics equation in (3.45).
A particular case occurs when the weighting matrix W is the identity
matrix I and the solution simpliﬁes into

the matrix

q̇ = J † v e ;

(3.51)

J † = J T (J J T )−1

(3.52)

is the right pseudo-inverse of J .4 The obtained solution locally minimizes the
norm of joint velocities.
It was pointed out above that if q̇ ∗ is a solution to (3.45), q̇ ∗ +P q̇ 0 is also a
solution, where q̇ 0 is a vector of arbitrary joint velocities and P is a projector
in the null space of J . Therefore, in view of the presence of redundant DOFs,
the solution (3.51) can be modiﬁed by the introduction of another term of
the kind P q̇ 0 . In particular, q̇ 0 can be speciﬁed so as to satisfy an additional
constraint to the problem.
In that case, it is necessary to consider a new cost functional in the form
g  (q̇) =

1
(q̇ − q̇ 0 )T (q̇ − q̇ 0 );
2

this choice is aimed at minimizing the norm of vector q̇ − q̇ 0 ; in other words,
solutions are sought which satisfy the constraint (3.45) and are as close as possible to q̇ 0 . In this way, the objective speciﬁed through q̇ 0 becomes unavoidably a secondary objective to satisfy with respect to the primary objective
speciﬁed by the constraint (3.45).
Proceeding in a way similar to the above yields
g  (q̇, λ) =

1
(q̇ − q̇ 0 )T (q̇ − q̇ 0 ) + λT (v e − J q̇);
2

from the ﬁrst necessary condition it is
q̇ = J T λ + q̇ 0

(3.53)

which, substituted into (3.45), gives
λ = (J J T )−1 (v e − J q̇ 0 ).
Finally, substituting λ back in (3.53) gives
q̇ = J † v e + (I n − J † J )q̇ 0 .
4

See Sect. A.7 for the deﬁnition of the pseudo-inverse of a matrix.

(3.54)

126

3 Diﬀerential Kinematics and Statics

As can be easily recognized, the obtained solution is composed of two terms.
The ﬁrst is relative to minimum norm joint velocities. The second, termed
homogeneous solution, attempts to satisfy the additional constraint to specify
via q̇ 0 ;5 the matrix (I − J † J ) is one of those matrices P introduced in (3.46)
which allows the projection of the vector q̇ 0 in the null space of J , so as
not to violate the constraint (3.45). A direct consequence is that, in the case
v e = 0, is is possible to generate internal motions described by (I − J † J )q̇ 0
that reconﬁgure the manipulator structure without changing the end-eﬀector
position and orientation.
Finally, it is worth discussing the way to specify the vector q̇ 0 for a convenient utilization of redundant DOFs. A typical choice is

q̇ 0 = k0

∂w(q)
∂q

T
(3.55)

where k0 > 0 and w(q) is a (secondary) objective function of the joint variables. Since the solution moves along the direction of the gradient of the objective function, it attempts to maximize it locally compatible to the primary
objective (kinematic constraint). Typical objective functions are:
• The manipulability measure, deﬁned as


w(q) = det J (q)J T (q)

(3.56)

which vanishes at a singular conﬁguration; thus, by maximizing this measure, redundancy is exploited to move away from singularities.6
• The distance from mechanical joint limits, deﬁned as
1 
w(q) = −
2n i=1
n



qi − q̄i
qiM − qim

2
(3.57)

where qiM (qim ) denotes the maximum (minimum) joint limit and q̄i the
middle value of the joint range; thus, by maximizing this distance, redundancy is exploited to keep the joint variables as close as possible to the
centre of their ranges.
• The distance from an obstacle, deﬁned as
w(q) = min p(q) − o
p ,o

(3.58)

where o is the position vector of a suitable point on the obstacle (its
centre, for instance, if the obstacle is modelled as a sphere) and p is the
5
6

It should be recalled that the additional constraint has secondary priority with
respect to the primary kinematic constraint.
The manipulability measure is given by the product of the singular values of the
Jacobian (see Problem 3.8).

3.5 Inverse Diﬀerential Kinematics

127

position vector of a generic point along the structure; thus, by maximizing
this distance, redundancy is exploited to avoid collision of the manipulator
with an obstacle (see also Problem 3.9).7
3.5.2 Kinematic Singularities
Both solutions (3.47) and (3.51) can be computed only when the Jacobian
has full rank. Hence, they become meaningless when the manipulator is at a
singular conﬁguration; in such a case, the system v e = J q̇ contains linearly
dependent equations.
It is possible to ﬁnd a solution q̇ by extracting all the linearly independent
equations only if v e ∈ R(J ). The occurrence of this situation means that the
assigned path is physically executable by the manipulator, even though it is
/ R(J ), the system of equations has
at a singular conﬁguration. If instead v e ∈
no solution; this means that the operational space path cannot be executed
by the manipulator at the given posture.
It is important to underline that the inversion of the Jacobian can represent
a serious inconvenience not only at a singularity but also in the neighbourhood
of a singularity. For instance, for the Jacobian inverse it is well known that its
computation requires the computation of the determinant; in the neighbourhood of a singularity, the determinant takes on a relatively small value which
can cause large joint velocities (see point c) in Sect. 3.3). Consider again the
above example of the shoulder singularity for the anthropomorphic arm. If a
path is assigned to the end-eﬀector which passes nearby the base rotation axis
(geometric locus of singular conﬁgurations), the base joint is forced to make
a rotation of about π in a relatively short time to allow the end-eﬀector to
keep tracking the imposed trajectory.
A more rigorous analysis of the solution features in the neighbourhood of
singular conﬁgurations can be developed by resorting to the singular value
decomposition (SVD) of matrix J .8
An alternative solution overcoming the problem of inverting diﬀerential
kinematics in the neighbourhood of a singularity is provided by the so-called
damped least-squares (DLS) inverse
J  = J T (J J T + k 2 I)−1

(3.59)

where k is a damping factor that renders the inversion better conditioned
from a numerical viewpoint. It can be shown that such a solution can be

7

8

If an obstacle occurs along the end-eﬀector path, it is opportune to invert the
order of priority between the kinematic constraint and the additional constraint;
in this way the obstacle may be avoided, but one gives up tracking the desired
path.
See Sect. A.8.

128

3 Diﬀerential Kinematics and Statics

obtained by reformulating the problem in terms of the minimization of the
cost functional
g  (q̇) =

1
1
(v e − J q̇)T (v e − J q̇) + k 2 q̇ T q̇,
2
2

where the introduction of the ﬁrst term allows a ﬁnite inversion error to be
tolerated, with the advantage of norm-bounded velocities. The factor k establishes the relative weight between the two objectives, and there exist techniques for selecting optimal values for the damping factor (see Problem 3.10).

3.6 Analytical Jacobian
The above sections have shown the way to compute the end-eﬀector velocity
in terms of the velocity of the end-eﬀector frame. The Jacobian is computed
according to a geometric technique in which the contributions of each joint
velocity to the components of end-eﬀector linear and angular velocity are
determined.
If the end-eﬀector pose is speciﬁed in terms of a minimal number of parameters in the operational space as in (2.80), it is natural to ask whether
it is possible to compute the Jacobian via diﬀerentiation of the direct kinematics function with respect to the joint variables. To this end, an analytical
technique is presented below to compute the Jacobian, and the existing relationship between the two Jacobians is found.
The translational velocity of the end-eﬀector frame can be expressed as
the time derivative of vector pe , representing the origin of the end-eﬀector
frame with respect to the base frame, i.e.,
ṗe =

∂pe
q̇ = J P (q)q̇.
∂q

(3.60)

For what concerns the rotational velocity of the end-eﬀector frame, the
minimal representation of orientation in terms of three variables φe can be
considered. Its time derivative φ̇e in general diﬀers from the angular velocity
vector deﬁned above. In any case, once the function φe (q) is known, it is
formally correct to consider the Jacobian obtained as
φ̇e =

∂φe
q̇ = J φ (q)q̇.
∂q

(3.61)

Computing the Jacobian J φ (q) as ∂φe /∂q is not straightforward, since the
function φe (q) is not usually available in direct form, but requires computation
of the elements of the relative rotation matrix.
Upon these premises, the diﬀerential kinematics equation can be obtained
as the time derivative of the direct kinematics equation in (2.82), i.e.,
  

ṗe
J P (q)
(3.62)
q̇ = J A (q)q̇
=
ẋe =
J φ (q)
φ̇e

3.6 Analytical Jacobian

129

Fig. 3.8. Rotational velocities of Euler angles ZYZ in current frame

Fig. 3.9. Composition of elementary rotational velocities for computing angular
velocity

where the analytical Jacobian
J A (q) =

∂k(q)
∂q

(3.63)

is diﬀerent from the geometric Jacobian J , since the end-eﬀector angular
velocity ω e with respect to the base frame is not given by φ̇e .
It is possible to ﬁnd the relationship between the angular velocity ω e and
the rotational velocity φ̇e for a given set of orientation angles. For instance,
consider the Euler angles ZYZ deﬁned in Sect. 2.4.1; in Fig. 3.8, the vectors
corresponding to the rotational velocities ϕ̇, ϑ̇, ψ̇ have been represented with
reference to the current frame. Figure 3.9 illustrates how to compute the
contributions of each rotational velocity to the components of angular velocity
about the axes of the reference frame:
• as a result of ϕ̇:
• as a result of ϑ̇:

[ ωx ω y
[ ωx ωy

ωz ]T = ϕ̇ [ 0 0 1 ]T
ωz ]T = ϑ̇ [ −sϕ cϕ 0 ]T

130

3 Diﬀerential Kinematics and Statics

• as a result of ψ̇:

[ ωx

ωz ]T = ψ̇ [ cϕ sϑ

ωy

sϕ sϑ

cϑ ]T ,

and then the equation relating the angular velocity ω e to the time derivative
of the Euler angles φ̇e is9
ω e = T (φe )φ̇e ,
(3.64)
where, in this case,

⎡

0
T = ⎣0
1

−sϕ
cϕ
0

⎤
cϕ sϑ
sϕ sϑ ⎦ .
cϑ

The determinant of matrix T is −sϑ , which implies that the relationship
cannot be inverted for ϑ = 0, π. This means that, even though all rotational
velocities of the end-eﬀector frame can be expressed by means of a suitable
angular velocity vector ω e , there exist angular velocities which cannot be
expressed by means of φ̇e when the orientation of the end-eﬀector frame causes
sϑ = 0.10 In fact, in this situation, the angular velocities that can be described
by φ̇e should have linearly dependent components in the directions orthogonal
to axis z (ωx2 + ωy2 = ϑ̇2 ). An orientation for which the determinant of the
transformation matrix vanishes is termed representation singularity of φe .
From a physical viewpoint, the meaning of ω e is more intuitive than that
of φ̇e . The three components of ω e represent the components of angular velocity with respect to the base frame. Instead, the three elements of φ̇e represent
nonorthogonal components of angular velocity deﬁned with respect to the
axes of a frame that varies as the end-eﬀector orientation varies. On the other
hand, while the integral of φ̇e over time gives φe , the integral of ω e does not
admit a clear physical interpretation, as can be seen in the following example.

Example 3.3
Consider an object whose orientation with respect to a reference frame is known at
time t = 0. Assign the following time proﬁles to ω:
ω = [ 0 π/2 0 ]T 1 < t ≤ 2,
• ω = [ π/2 0 0 ]T 0 ≤ t ≤ 1
T
0≤t≤1
ω = [ π/2 0 0 ]T 1 < t ≤ 2.
• ω = [ 0 π/2 0 ]
The integral of ω gives the same result in the two cases

"

2

ωdt = [ π/2

π/2

0 ]T

0

but the ﬁnal object orientation corresponding to the second timing law is clearly
diﬀerent from the one obtained with the ﬁrst timing law (Fig. 3.10).
9
10

This relation can also be obtained from the rotation matrix associated with the
three angles (see Problem 3.11).
In Sect. 2.4.1, it was shown that for this orientation the inverse solution of the
Euler angles degenerates.

3.6 Analytical Jacobian

131

Fig. 3.10. Nonuniqueness of orientation computed as the integral of angular velocity

Once the transformation T between ω e and φ̇e is given, the analytical
Jacobian can be related to the geometric Jacobian as


I
O
(3.65)
ẋe = T A (φe )ẋe
ve =
O T (φe )
which, in view of (3.4), (3.62), yields
J = T A (φ)J A .

(3.66)

This relationship shows that J and J A , in general, diﬀer. Regarding the use
of either one or the other in all those problems where the inﬂuence of the
Jacobian matters, it is anticipated that the geometric Jacobian will be adopted
whenever it is necessary to refer to quantities of clear physical meaning, while
the analytical Jacobian will be adopted whenever it is necessary to refer to
diﬀerential quantities of variables deﬁned in the operational space.
For certain manipulator geometries, it is possible to establish a substantial
equivalence between J and J A . In fact, when the DOFs cause rotations of
the end-eﬀector all about the same ﬁxed axis in space, the two Jacobians
are essentially the same. This is the case of the above three-link planar arm.
Its geometric Jacobian (3.35) reveals that only rotations about axis z0 are
permitted. The (3 × 3) analytical Jacobian that can be derived by considering
the end-eﬀector position components in the plane of the structure and deﬁning

132

3 Diﬀerential Kinematics and Statics

the end-eﬀector orientation as φ = ϑ1 + ϑ2 + ϑ3 coincides with the matrix
that is obtained by eliminating the three null rows of the geometric Jacobian.

3.7 Inverse Kinematics Algorithms
In Sect. 3.5 it was shown how to invert kinematics by using the diﬀerential
kinematics equation. In the numerical implementation of (3.48), computation
of joint velocities is obtained by using the inverse of the Jacobian evaluated
with the joint variables at the previous instant of time
q(tk+1 ) = q(tk ) + J −1 (q(tk ))v e (tk )Δt.
It follows that the computed joint velocities q̇ do not coincide with those
satisfying (3.47) in the continuous time. Therefore, reconstruction of joint
variables q is entrusted to a numerical integration which involves drift phenomena of the solution; as a consequence, the end-eﬀector pose corresponding
to the computed joint variables diﬀers from the desired one.
This inconvenience can be overcome by resorting to a solution scheme that
accounts for the operational space error between the desired and the actual
end-eﬀector position and orientation. Let
e = xd − x e

(3.67)

be the expression of such error.
Consider the time derivative of (3.67), i.e.,
ė = ẋd − ẋe

(3.68)

which, according to diﬀerential kinematics (3.62), can be written as
ė = ẋd − J A (q)q̇.

(3.69)

Notice in (3.69) that the use of operational space quantities has naturally
lead to using the analytical Jacobian in lieu of the geometric Jacobian. For
this equation to lead to an inverse kinematics algorithm, it is worth relating
the computed joint velocity vector q̇ to the error e so that (3.69) gives a
diﬀerential equation describing error evolution over time. Nonetheless, it is
necessary to choose a relationship between q̇ and e that ensures convergence
of the error to zero.
Having formulated inverse kinematics in algorithmic terms implies that
the joint variables q corresponding to a given end-eﬀector pose xd are accurately computed only when the error xd − k(q) is reduced within a given
threshold; such settling time depends on the dynamic characteristics of the
error diﬀerential equation. The choice of q̇ as a function of e permits ﬁnding
inverse kinematics algorithms with diﬀerent features.

3.7 Inverse Kinematics Algorithms

133

Fig. 3.11. Inverse kinematics algorithm with Jacobian inverse

3.7.1 Jacobian (Pseudo-)inverse
On the assumption that matrix J A is square and nonsingular, the choice
q̇ = J −1
A (q)(ẋd + Ke)

(3.70)

leads to the equivalent linear system
ė + Ke = 0.

(3.71)

If K is a positive deﬁnite (usually diagonal) matrix, the system (3.71) is
asymptotically stable. The error tends to zero along the trajectory with a
convergence rate that depends on the eigenvalues of matrix K;11 the larger
the eigenvalues, the faster the convergence. Since the scheme is practically
implemented as a discrete-time system, it is reasonable to predict that an
upper bound exists on the eigenvalues; depending on the sampling time, there
will be a limit for the maximum eigenvalue of K under which asymptotic
stability of the error system is guaranteed.
The block scheme corresponding to the inverse kinematics algorithm
in (3.70) is illustrated in Fig. 3.11, where k(·) indicates the direct kinematics
function in (2.82). This scheme can be revisited in terms of the usual feedback
control schemes. Speciﬁcally, it can observed that the nonlinear block k(·) is
needed to compute x and thus the tracking error e, while the block J −1
A (q)
has been introduced to compensate for J A (q) and making the system linear.
The block scheme shows the presence of a string of integrators on the forward
loop and then, for a constant reference (ẋd = 0), guarantees a null steadystate error. Further, the feedforward action provided by ẋd for a time-varying
reference ensures that the error is kept to zero (in the case e(0) = 0) along
the whole trajectory, independently of the type of desired reference xd (t).
Finally, notice that (3.70), for ẋd = 0, corresponds to the Newton method
for solving a system of nonlinear equations. Given a constant end-eﬀector
pose xd , the algorithm can be keenly applied to compute one of the admissible
11

See Sect. A.5.

134

3 Diﬀerential Kinematics and Statics

Fig. 3.12. Block scheme of the inverse kinematics algorithm with Jacobian transpose

solutions to the inverse kinematics problem, whenever that does not admit
closed-form solutions, as discussed in Sect. 2.12. Such a method is also useful
in practice at the start-up of the manipulator for a given task, to compute the
corresponding joint conﬁguration.
In the case of a redundant manipulator , solution (3.70) can be generalized
into
(3.72)
q̇ = J †A (ẋd + Ke) + (I n − J †A J A )q̇ 0 ,
which represents the algorithmic version of solution (3.54).
The structure of the inverse kinematics algorithm can be conceptually
adopted for a simple robot control technique, known under the name of kinematic control . As will be seen in Chap. 7, a manipulator is actually an electromechanical system actuated by motor torques, while in Chaps. 8–10 dynamic
control techniques will be presented which will properly account for the nonlinear and coupling eﬀects of the dynamic model.
At ﬁrst approximation, however, it is possible to consider a kinematic
command as system input, typically a velocity. This is possible in view of
the presence of a low-level control loop, which ‘ideally’ imposes any speciﬁed
reference velocity. On the other hand, such a loop already exists in a ‘closed’
control unit, where the user can also intervene with kinematic commands.
In other words, the scheme in Fig. 3.11 can implement a kinematic control,
provided that the integrator is regarded as a simpliﬁed model of the robot,
thanks to the presence of single joint local servos, which ensure a more or
less accurate reproduction of the velocity commands. Nevertheless, it is worth
underlining that such a kinematic control technique yields satisfactory performance only when one does not require too fast motions or rapid accelerations.
The performance of the independent joint control will be analyzed in Sect. 8.3.
3.7.2 Jacobian Transpose
A computationally simpler algorithm can be derived by ﬁnding a relationship
between q̇ and e that ensures error convergence to zero, without requiring
linearization of (3.69). As a consequence, the error dynamics is governed by a

3.7 Inverse Kinematics Algorithms

135

nonlinear diﬀerential equation. The Lyapunov direct method can be utilized
to determine a dependence q̇(e) that ensures asymptotic stability of the error
system. Choose as Lyapunov function candidate the positive deﬁnite quadratic
form12
1
V (e) = eT Ke,
(3.73)
2
where K is a symmetric positive deﬁnite matrix. This function is so that
V (e) > 0 ∀e = 0,

V (0) = 0.

Diﬀerentiating (3.73) with respect to time and accounting for (3.68) gives
V̇ = eT K ẋd − eT K ẋe .

(3.74)

V̇ = eT K ẋd − eT KJ A (q)q̇.

(3.75)

In view of (3.62), it is

At this point, the choice of joint velocities as
q̇ = J TA (q)Ke

(3.76)

V̇ = eT K ẋd − eT KJ A (q)J TA (q)Ke.

(3.77)

leads to
Consider the case of a constant reference (ẋd = 0). The function in (3.77) is
negative deﬁnite, under the assumption of full rank for J A (q). The condition
V̇ < 0 with V > 0 implies that the system trajectories uniformly converge
to e = 0, i.e., the system is asymptotically stable. When N (J TA ) = ∅, the
function in (3.77) is only negative semi-deﬁnite, since V̇ = 0 for e = 0 with
Ke ∈ N (J TA ). In this case, the algorithm can get stuck at q̇ = 0 with e = 0.
However, the example that follows will show that this situation occurs only if
the assigned end-eﬀector position is not actually reachable from the current
conﬁguration.
The resulting block scheme is illustrated in Fig. 3.12, which shows the notable feature of the algorithm to require computation only of direct kinematics
functions k(q), J TA (q).
It can be recognized that (3.76) corresponds to the gradient method for
the solution of a system on nonlinear equations. As in the case of the Jacobian inverse solution, for a given constant end-eﬀector pose xd , the Jacobian
transpose algorithm can be keenly employed to solve the inverse kinematics problem, or more simply to initialize the values of the manipulator joint
variables.
The case when xd is a time-varying function (ẋd = 0) deserves a separate
analysis. In order to obtain V̇ < 0 also in this case, it would be suﬃcient to
choose a q̇ that depends on the (pseudo-)inverse of the Jacobian as in (3.70),
12

See Sect. C.3 for the presentation of the Lyapunov direct method.

136

3 Diﬀerential Kinematics and Statics

Fig. 3.13. Characterization of the anthropomorphic arm at a shoulder singularity
for the admissible solutions of the Jacobian transpose algorithm

recovering the asymptotic stability result derived above.13 For the inversion
scheme based on the transpose, the ﬁrst term on the right-hand side of (3.77)
is not cancelled any more and nothing can be said about its sign. This implies that asymptotic stability along the trajectory cannot be achieved. The
tracking error e(t) is, anyhow, norm-bounded; the larger the norm of K, the
smaller the norm of e.14 In practice, since the inversion scheme is to be implemented in discrete-time, there is an upper bound on the norm of K with
reference to the adopted sampling time.

Example 3.4
Consider the anthropomorphic arm; a shoulder singularity occurs whenever a2 c2 +
a3 c23 = 0 (Fig. 3.6). In this conﬁguration, the transpose of the Jacobian in (3.38) is



J TP =

0
−c1 (a2 s2 + a3 s23 )
−a3 c1 s23

0
−s1 (a2 s2 + a3 s23 )
−a3 s1 s23

0
0
a3 c23



.

By computing the null space of J TP , if νx , νy and νz denote the components of vector
ν along the axes of the base frame, one has the result
νy
1
=−
νx
tan ϑ1
13
14

νz = 0,

Notice that, anyhow, in case of kinematic singularities, it is necessary to resort
to an inverse kinematics scheme that does not require inversion of the Jacobian.
Notice that the negative deﬁnite term is a quadratic function of the error, while
the other term is a linear function of the error. Therefore, for an error of very
small norm, the linear term prevails over the quadratic term, and the norm of K
should be increased to reduce the norm of e as much as possible.

3.7 Inverse Kinematics Algorithms

137

implying that the direction of N (J TP ) coincides with the direction orthogonal to the
plane of the structure (Fig. 3.13). The Jacobian transpose algorithm gets stuck if,
with K diagonal and having all equal elements, the desired position is along the line
normal to the plane of the structure at the intersection with the wrist point. On the
other hand, the end-eﬀector cannot physically move from the singular conﬁguration
along such a line. Instead, if the prescribed path has a non-null component in the
plane of the structure at the singularity, algorithm convergence is ensured, since in
that case Ke ∈
/ N (J TP ).

In summary, the algorithm based on the computation of the Jacobian
transpose provides a computationally eﬃcient inverse kinematics method that
can be utilized also for paths crossing kinematic singularities.
3.7.3 Orientation Error
The inverse kinematics algorithms presented in the above sections utilize the
analytical Jacobian since they operate on error variables (position and orientation) that are deﬁned in the operational space.
For what concerns the position error, it is obvious that its expression is
given by
(3.78)
eP = pd − pe (q)
where pd and pe denote respectively the desired and computed end-eﬀector
positions. Further, its time derivative is
ėP = ṗd − ṗe .

(3.79)

On the other hand, for what concerns the orientation error , its expression
depends on the particular representation of end-eﬀector orientation, namely,
Euler angles, angle and axis, and unit quaternion.
Euler angles
The orientation error is chosen according to an expression formally analogous
to (3.78), i.e.,
eO = φd − φe (q)
(3.80)
where φd and φe denote respectively the desired and computed set of Euler
angles. Further, its time derivative is
ėO = φ̇d − φ̇e .

(3.81)

Therefore, assuming that neither kinematic nor representation singularities
occur, the Jacobian inverse solution for a nonredundant manipulator is derived
from (3.70), i.e.,

138

3 Diﬀerential Kinematics and Statics

q̇ = J −1
A (q)



ṗd + K P eP
φ̇d + K O eO


(3.82)

where K P and K O are positive deﬁnite matrices.
As already pointed out in Sect. 2.10 for computation of the direct kinematics function in the form (2.82), the determination of the orientation variables
from the joint variables is not easy except for simple cases (see Example 2.5).
To this end, it is worth recalling that computation of the angles φe , in a
minimal representation of orientation, requires computation of the rotation
matrix Re = [ ne se ae ]; in fact, only the dependence of Re on q is known
in closed form, but not that of φe on q. Further, the use of inverse functions (Atan2) in (2.19), (2.22) involves a non-negligible complexity in the
computation of the analytical Jacobian, and the occurrence of representation
singularities constitutes another drawback for the orientation error based on
Euler angles.
Diﬀerent kinds of remarks are to be made about the way to assign a time
proﬁle for the reference variables φd chosen to represent end-eﬀector orientation. The most intuitive way to specify end-eﬀector orientation is to refer to
the orientation of the end-eﬀector frame (nd , sd , ad ) with respect to the base
frame. Given the limitations pointed out in Sect. 2.10 about guaranteeing orthonormality of the unit vectors along time, it is necessary ﬁrst to compute
the Euler angles corresponding to the initial and ﬁnal orientation of the endeﬀector frame via (2.19), (2.22); only then a time evolution can be generated.
Such solutions will be presented in Chap. 4.
A radical simpliﬁcation of the problem at issue can be obtained for manipulators having a spherical wrist. Section 2.12.2 pointed out the possibility to
solve the inverse kinematics problem for the position part separately from that
for the orientation part. This result also has an impact at algorithmic level. In
fact, the implementation of an inverse kinematics algorithm for determining
the joint variables inﬂuencing the wrist position allows the computation of
the time evolution of the wrist frame RW (t). Hence, once the desired time
evolution of the end-eﬀector frame Rd (t) is given, it is suﬃcient to compute
the Euler angles ZYZ from the matrix RTW Rd by applying (2.19). As shown
in Sect. 2.12.5, these angles are directly the joint variables of the spherical
wrist. See also Problem 3.14.
The above considerations show that the inverse kinematics algorithms
based on the analytical Jacobian are eﬀective for kinematic structures having
a spherical wrist which are of signiﬁcant interest. For manipulator structures
which cannot be reduced to that class, it may be appropriate to reformulate
the inverse kinematics problem on the basis of a diﬀerent deﬁnition of the
orientation error.

3.7 Inverse Kinematics Algorithms

139

Angle and axis
If Rd = [ nd sd ad ] denotes the desired rotation matrix of the end-eﬀector
frame and Re = [ ne se ae ] the rotation matrix that can be computed
from the joint variables, the orientation error between the two frames can be
expressed as
(3.83)
eO = r sin ϑ
where ϑ and r identify the angle and axis of the equivalent rotation that can
be deduced from the matrix
R(ϑ, r) = Rd RTe (q),

(3.84)

describing the rotation needed to align R with Rd . Notice that (3.83) gives a
unique relationship for −π/2 < ϑ < π/2. The angle ϑ represents the magnitude of an orientation error, and thus the above limitation is not restrictive
since the tracking error is typically small for an inverse kinematics algorithm.
By comparing the oﬀ-diagonal terms of the expression of R(ϑ, r) in (2.25)
with the corresponding terms resulting on the right-hand side of (3.84), it can
be found that a functional expression of the orientation error in (3.83) is (see
Problem 3.16)
eO =

1
(ne (q) × nd + se (q) × sd + ae (q) × ad );
2

(3.85)

the limitation on ϑ is transformed in the condition nTe nd ≥ 0, sTe sd ≥ 0,
aTe ad ≥ 0.
Diﬀerentiating (3.85) with respect to time and accounting for the expression of the columns of the derivative of a rotation matrix in (3.8) gives (see
Problem 3.19)
ėO = LT ω d − Lω e
(3.86)
where


1
S(nd )S(ne ) + S(sd )S(se ) + S(ad )S(ae ) .
(3.87)
2
At this point, by exploiting the relations (3.2), (3.3) of the geometric Jacobian
expressing ṗe and ω e as a function of q̇, (3.79), (3.86) become
 
 


 
I O
ṗd − J P (q)q̇
ėP
ṗd
=
−
J q̇.
(3.88)
ė =
=
T
T
ėO
O L
L ωd
L ω d − LJ O (q)q̇
L=−

The expression in (3.88) suggests the possibility of devising inverse kinematics
algorithms analogous to the ones derived above, but using the geometric Jacobian in place of the analytical Jacobian. For instance, the Jacobian inverse
solution for a nonredundant nonsingular manipulator is


ṗd + K P eP

−1
.
(3.89)
q̇ = J (q)
L−1 LT ω d + K O eO

140

3 Diﬀerential Kinematics and Statics

It is worth remarking that the inverse kinematics solution based on (3.89)
is expected to perform better than the solution based on (3.82) since it uses
the geometric Jacobian in lieu of the analytical Jacobian, thus avoiding the
occurrence of representation singularities.
Unit quaternion
In order to devise an inverse kinematics algorithm based on the unit quaternion, a suitable orientation error should be deﬁned. Let Qd = {ηd , d } and
Qe = {ηe , e } represent the quaternions associated with Rd and Re , respectively. The orientation error can be described by the rotation matrix
Rd RTe and, in view of (2.37), can be expressed in terms of the quaternion
ΔQ = {Δη, Δ} where
(3.90)
ΔQ = Qd ∗ Q−1
e .
It can be recognized that ΔQ = {1, 0} if and only if Re and Rd are aligned.
Hence, it is suﬃcient to deﬁne the orientation error as
eO = Δ = ηe (q)d − ηd e (q) − S(d )e (q),

(3.91)

where the skew-symmetric operator S(·) has been used. Notice, however, that
the explicit computation of ηe and e from the joint variables is not possible
but it requires the intermediate computation of the rotation matrix Re that
is available from the manipulator direct kinematics; then, the quaternion can
be extracted using (2.34).
At this point, a Jacobian inverse solution can be computed as


ṗd + K P eP
−1
q̇ = J (q)
(3.92)
ω d + K O eO
where noticeably the geometric Jacobian has been used. Substituting (3.92)
into (3.4) gives (3.79) and
ω d − ω e + K O eO = 0.

(3.93)

It should be observed that now the orientation error equation is nonlinear
in eO since it contains the end-eﬀector angular velocity error instead of the
time derivative of the orientation error. To this end, it is worth considering
the relationship between the time derivative of the quaternion Qe and the
angular velocity ω e . This can be found to be (see Problem 3.19)
1
η̇e = − Te ω e
2
1
˙ e = (ηe I 3 − S(e )) ω e
2

(3.94)
(3.95)

which is the so-called quaternion propagation. A similar relationship holds
between the time derivative of Qd and ω d .

3.7 Inverse Kinematics Algorithms

141

To study stability of system (3.93), consider the positive deﬁnite Lyapunov
function candidate
V = (ηd − ηe )2 + (d − e )T (d − e ).

(3.96)

In view of (3.94), (3.95), diﬀerentiating (3.96) with respect to time and accounting for (3.93) yields (see Problem 3.20)
V̇ = −eTO K O eO

(3.97)

which is negative deﬁnite, implying that eO converges to zero.
In summary, the inverse kinematics solution based on (3.92) uses the geometric Jacobian as the solution based on (3.89) but is computationally lighter.
3.7.4 Second-order Algorithms
The above inverse kinematics algorithms can be deﬁned as ﬁrst-order algorithms, in that they allow the inversion of a motion trajectory, speciﬁed at
the end-eﬀector in terms of of position and orientation, into the equivalent
joint positions and velocities.
Nevertheless, as will be seen in Chap. 8, for control purposes it may be
necessary to invert a motion trajectory speciﬁed in terms of position, velocity
and acceleration. On the other hand, the manipulator is inherently a secondorder mechanical system, as will be revealed by the dynamic model to be
derived in Chap. 7.
The time diﬀerentiation of the diﬀerential kinematics equation (3.62) leads
to
(3.98)
ẍe = J A (q)q̈ + J̇ A (q, q̇)q̇
which gives the relationship between the joint space accelerations and the
operational space accelerations.
Under the assumption of a square and non-singular matrix J A , the secondorder diﬀerential kinematics (3.98) can be inverted in terms of the joint accelerations


(q)
ẍ
−
J̇
(q,
q̇)
q̇
.
(3.99)
q̈ = J −1
e
A
A
The numerical integration of (3.99) to reconstruct the joint velocities and
positions would unavoidably lead to a drift of the solution; therefore, similarly
to the inverse kinematics algorithm with the Jacobian inverse, it is worth
considering the error deﬁned in (3.68) along with its derivative
ë = ẍd − ẍe

(3.100)

ë = ẍd − J A (q)q̈ − J̇ A (q, q̇)q̇.

(3.101)

which, in view of (3.98), yields

142

3 Diﬀerential Kinematics and Statics

Fig. 3.14. Block scheme of the second-order inverse kinematics algorithm with
Jacobian inverse

At this point, it is advisable to choose the joint acceleration vector as


q̈ = J −1
(3.102)
A (q) ẍd + K D ė + K P e − J̇ A (q, q̇)q̇
where K D and K P are positive deﬁnite (typically diagonal) matrices. Substituting (3.102) into (3.101) leads to the equivalent linear error system
ë + K D ė + K P e = 0

(3.103)

which is asymptotically stable: the error tends to zero along the trajectory with
a convergence speed depending on the choice of the matrices K P e K D . The
second-order inverse kinematics algorithm is illustrated in the block scheme
of Fig. 3.14.
In the case of a redundant manipulator , the generalization of (3.102) leads
to an algorithmic solution based on the Jacobian pseudo-inverse of the kind


q̈ = J †A ẍd + K D ė + K P e − J̇ A (q, q̇)q̇ + (I n − J †A J A )q̈ 0
(3.104)
where the vector q̈ 0 represents arbitrary joint accelerations which can be chosen so as to (locally) optimize an objective function like those considered in
Sect. 3.5.1.
As for the ﬁrst-order inverse kinematics algorithms, it is possible to consider other expressions for the orientation error which, unlike the Euler angles,
refer to an angle and axis description, else to the unit quaternion.

3.7 Inverse Kinematics Algorithms

143

3.7.5 Comparison Among Inverse Kinematics Algorithms
In order to make a comparison of performance among the inverse kinematics
algorithms presented above, consider the 3-link planar arm in Fig. 2.20 whose
link lengths are a1 = a2 = a3 = 0.5 m. The direct kinematics for this arm is
given by (2.83), while its Jacobian can be found from (3.35) by considering
the 3 non-null rows of interest for the operational space.
Let the arm be at the initial posture q = [ π −π/2 −π/2 ]T rad, corresponding to the end-eﬀector pose: p = [ 0 0.5 ]T m, φ = 0 rad. A circular path
of radius 0.25 m and centre at (0.25, 0.5) m is assigned to the end-eﬀector. Let
the motion trajectory be


0.25(1 − cos πt)
pd (t) =
0 ≤ t ≤ 4;
0.25(2 + sin πt)
i.e., the end-eﬀector has to make two complete circles in a time of 2 s per
circle. As regards end-eﬀector orientation, initially it is required to follow the
trajectory
π
0 ≤ t ≤ 4;
φd (t) = sin t
24
i.e., the end-eﬀector has to attain a diﬀerent orientation (φd = 0.5 rad) at the
end of the two circles.
The inverse kinematics algorithms were implemented on a computer by
adopting the Euler numerical integration scheme (3.48) with an integration
time Δt = 1 ms.
At ﬁrst, the inverse kinematics along the given trajectory has been performed by using (3.47). The results obtained in Fig. 3.15 show that the norm
of the position error along the whole trajectory is bounded; at steady state,
after t = 4, the error sets to a constant value in view of the typical drift of
open-loop schemes. A similar drift can be observed for the orientation error.
Next, the inverse kinematics algorithm based on (3.70) using the Jacobian
inverse has been used, with the matrix gain K = diag{500, 500, 100}. The
resulting joint positions and velocities as well as the tracking errors are shown
in Fig. 3.16. The norm of the position error is radically decreased and converges to zero at steady state, thanks to the closed-loop feature of the scheme;
the orientation error, too, is decreased and tends to zero at steady state.
On the other hand, if the end-eﬀector orientation is not constrained, the
operational space becomes two-dimensional and is characterized by the ﬁrst
two rows of the direct kinematics in (2.83) as well as by the Jacobian in (3.36);
a redundant DOF is then available. Hence, the inverse kinematics algorithm
based on (3.72) using the Jacobian pseudo-inverse has been used with K =
diag{500, 500}. If redundancy is not exploited (q̇ 0 = 0), the results in Fig. 3.17
reveal that position tracking remains satisfactory and, of course, the endeﬀector orientation freely varies along the given trajectory.
With reference to the previous situation, the use of the Jacobian transpose
algorithm based on (3.76) with K = diag{500, 500} gives rise to a tracking

144

3 Diﬀerential Kinematics and Statics
pos error norm

−3

2

x 10

x 10

−0.2
[rad]

1.5
[m]

orien error

−5

0

1
0.5

−0.4
−0.6
−0.8

0
0

1

2

[s]

3

4

−1
0

5

1

2

[s]

3

4

5

Fig. 3.15. Time history of the norm of end-eﬀector position error and orientation
error with the open-loop inverse Jacobian algorithm
joint pos

5

3

2
0

1

[s]

3

4

5

−10
0

x 10

0.8

−1

0.6

−2

0.4
0.2
0
0

1

2

[s]

3

4

5

4

5

orien error

−8

0

[rad]

[m]

2

pos error norm

−5

1

3

−5

2
−5
0

1

5
[rad/s]

[rad]

1

0

joint vel

10

x 10

−3
−4

1

2

[s]

3

4

5

−5
0

1

2

[s]

3

Fig. 3.16. Time history of the joint positions and velocities, and of the norm of endeﬀector position error and orientation error with the closed-loop inverse Jacobian
algorithm

error (Fig. 3.18) which is anyhow bounded and rapidly tends to zero at steady
state.
In order to show the capability of handling the degree of redundancy, the
algorithm based on (3.72) with q̇ 0 = 0 has been used; two types of constraints

3.7 Inverse Kinematics Algorithms
pos error norm

−6

5

x 10

145

orien

0.5

0

3

[rad]

[m]

4

2

−0.5

1
0
0

1

2

[s]

3

4

−1
0

5

1

2

[s]

3

4

5

Fig. 3.17. Time history of the norm of end-eﬀector position error and orientation
with the Jacobian pseudo-inverse algorithm
pos error norm

0.01

orien

0.5

0

0.006

[rad]

[m]

0.008

0.004

−0.5

0.002
0
0

1

2

[s]

3

4

5

−1
0

1

2

[s]

3

4

5

Fig. 3.18. Time history of the norm of end-eﬀector position error and orientation
with the Jacobian transpose algorithm

have been considered concerning an objective function to locally maximize
according to the choice (3.55). The ﬁrst function is
w(ϑ2 , ϑ3 ) =

1 2
(s + s23 )
2 2

that provides a manipulability measure. Notice that such a function is computationally simpler than the function in (3.56), but it still describes a distance
from kinematic singularities in an eﬀective way. The gain in (3.55)) has been
set to k0 = 50. In Fig. 3.19, the joint trajectories are reported for the two
cases with and without (k0 = 0) constraint. The addition of the constraint
leads to having coincident trajectories for Joints 2 and 3. The manipulability
measure in the constrained case (continuous line) attains larger values along
the trajectory compared to the unconstrained case (dashed line). It is worth
underlining that the tracking position error is practically the same in the two
cases (Fig. 3.17), since the additional joint velocity contribution is projected
in the null space of the Jacobian so as not to alter the performance of the
end-eﬀector position task.
Finally, it is worth noticing that in the constrained case the resulting joint
trajectories are cyclic, i.e., they take on the same values after a period of

146

3 Diﬀerential Kinematics and Statics
joint pos

5

joint pos

5
1

0

[rad/s]

[rad]

1

3

0
2

2
−5
0

3

1

[s]

3

4

−5
0

5

pos error norm

−6

5

2

x 10

1

2

[s]

3

4

5

3

4

5

manip

1

0.95

3

[rad]

[m]

4

2

0.9

1
0
0

1

2

[s]

3

4

5

0.85
0

1

2

[s]

Fig. 3.19. Time history of the joint positions, the norm of end-eﬀector position
error, and the manipulability measure with the Jacobian pseudo-inverse algorithm
and manipulability constraint; upper left: with the unconstrained solution, upper
right: with the constrained solution

the circular path. This does not happen for the unconstrained case, since the
internal motion of the structure causes the arm to be in a diﬀerent posture
after one circle.
The second objective function considered is the distance from mechanical
joint limits in (3.57). Speciﬁcally, it is assumed what follows: the ﬁrst joint
does not have limits (q1m = −2π, q1M = 2π), the second joint has limits q2m =
−π/2, q2M = π/2, and the third joint has limits q3m = −3π/2, q3M = −π/2.
It is not diﬃcult to verify that, in the unconstrained case, the trajectories of
Joints 2 and 3 in Fig. 3.19 violate the respective limits. The gain in (3.55)
has been set to k0 = 250. The results in Fig. 3.20 show the eﬀectiveness of
the technique with utilization of redundancy, since both Joints 2 and 3 tend
to invert their motion — with respect to the unconstrained trajectories in
Fig. 3.19 — and keep far from the minimum limit for Joint 2 and the maximum
limit for Joint 3, respectively. Such an eﬀort does not appreciably aﬀect the
position tracking error, whose norm is bounded anyhow within acceptable
values.

3.8 Statics
joint 2 pos

6

6

4

4

2

2
[rad]

[rad]

joint 1 pos

0

0

−2

−2

−4

−4

−6
0

147

−6
1

2

[s]

3

4

5

0

joint 3 pos

5

1

[s]

3

4

5

4

5

pos error norm

−4

2

2

x 10

[m]

[rad]

1.5
0

1
0.5

−5
0

1

2

[s]

3

4

5

0
0

1

2

[s]

3

Fig. 3.20. Time history of the joint positions and the norm of end-eﬀector position
error with the Jacobian pseudo-inverse algorithm and joint limit constraint (joint
limits are denoted by dashed lines)

3.8 Statics
The goal of statics is to determine the relationship between the generalized
forces applied to the end-eﬀector and the generalized forces applied to the
joints — forces for prismatic joints, torques for revolute joints — with the
manipulator at an equilibrium conﬁguration.
Let τ denote the (n × 1) vector of joint torques and γ the (r × 1) vector
of end-eﬀector forces15 where r is the dimension of the operational space of
interest.
The application of the principle of virtual work allows the determination
of the required relationship. The mechanical manipulators considered are systems with time-invariant, holonomic constraints, and thus their conﬁgurations
depend only on the joint variables q and not explicitly on time. This implies
that virtual displacements coincide with elementary displacements.
Consider the elementary works performed by the two force systems. As for
the joint torques, the elementary work associated with them is
dWτ = τ T dq.
15

(3.105)

Hereafter, generalized forces at the joints are often called torques, while generalized forces at the end-eﬀector are often called forces.

148

3 Diﬀerential Kinematics and Statics

As for the end-eﬀector forces γ, if the force contributions f e are separated by
the moment contributions μe , the elementary work associated with them is
dWγ = f Te dpe + μTe ω e dt,

(3.106)

where dpe is the linear displacement and ω e dt is the angular displacement16
By accounting for the diﬀerential kinematics relationship in (3.4), (3.5),
the relation (3.106) can be rewritten as
dWγ = f Te J P (q)dq + μTe J O (q)dq
=

(3.107)

γ Te J (q)dq

where γ e = [ f Te μTe ]T . Since virtual and elementary displacements coincide,
the virtual works associated with the two force systems are
δWτ = τ T δq
δWγ = γ Te J (q)δq,

(3.108)
(3.109)

where δ is the usual symbol to indicate virtual quantities.
According to the principle of virtual work, the manipulator is at static
equilibrium if and only if
δWτ = δWγ

∀δq,

(3.110)

i.e., the diﬀerence between the virtual work of the joint torques and the virtual
work of the end-eﬀector forces must be null for all joint displacements.
From (3.109), notice that the virtual work of the end-eﬀector forces is
null for any displacement in the null space of J . This implies that the joint
torques associated with such displacements must be null at static equilibrium.
Substituting (3.108), (3.109) into (3.110) leads to the notable result
τ = J T (q)γ e

(3.111)

stating that the relationship between the end-eﬀector forces and the joint
torques is established by the transpose of the manipulator geometric Jacobian.
3.8.1 Kineto-Statics Duality
The statics relationship in (3.111), combined with the diﬀerential kinematics
equation in (3.45), points out a property of kineto-statics duality. In fact, by
adopting a representation similar to that of Fig. 3.7 for diﬀerential kinematics,
one has that (Fig. 3.21):
• The range space of J T is the subspace R(J T ) in IRn of the joint torques
that can balance the end-eﬀector forces, in the given manipulator posture.
16

The angular displacement has been indicated by ω e dt in view of the problems of
integrability of ω e discussed in Sect. 3.6.

3.8 Statics

149

Fig. 3.21. Mapping between the end-eﬀector force space and the joint torque space

• The null space of J T is the subspace N (J T ) in IRr of the end-eﬀector forces
that do not require any balancing joint torques, in the given manipulator
posture.
It is worth remarking that the end-eﬀector forces γ e ∈ N (J T ) are entirely
absorbed by the structure in that the mechanical constraint reaction forces
can balance them exactly. Hence, a manipulator at a singular conﬁguration
remains in the given posture whatever end-eﬀector force γ e is applied so that
γ e ∈ N (J T ).
The relations between the two subspaces are established by
N (J ) ≡ R⊥ (J T )

R(J ) ≡ N ⊥ (J T )

and then, once the manipulator Jacobian is known, it is possible to characterize completely diﬀerential kinematics and statics in terms of the range and
null spaces of the Jacobian and its transpose.
On the basis of the above duality, the inverse kinematics scheme with the
Jacobian transpose in Fig. 3.12 admits an interesting physical interpretation.
Consider a manipulator with ideal dynamics τ = q̇ (null masses and unit
viscous friction coeﬃcients); the algorithm update law q̇ = J T Ke plays the
role of a generalized spring of stiﬀness constant K generating a force Ke that
pulls the end-eﬀector towards the desired posture in the operational space.
If this manipulator is allowed to move, e.g., in the case Ke ∈
/ N (J T ), the
end-eﬀector attains the desired posture and the corresponding joint variables
are determined.
3.8.2 Velocity and Force Transformation
The kineto-statics duality concept presented above can be useful to characterize the transformation of velocities and forces between two coordinate frames.
Consider a reference coordinate frame O0 –x0 y0 z0 and a rigid body moving
with respect to such a frame. Then let O1 –x1 y1 z1 and O2 –x2 y2 z2 be two

150

3 Diﬀerential Kinematics and Statics

Fig. 3.22. Representation of linear and angular velocities in diﬀerent coordinate
frames on the same rigid body

coordinate frames attached to the body (Fig. 3.22). The relationships between
translational and rotational velocities of the two frames with respect to the
reference frame are given by
ω2 = ω1
ṗ2 = ṗ1 + ω 1 × r 12 .
By exploiting the skew-symmetric operator S(·) in (3.9), the above relations
can be compactly written as

 


ṗ2
I −S(r 12 )
ṗ1
=
.
(3.112)
ω2
ω1
O
I
All vectors in (3.112) are meant to be referred to the reference frame O0 –
x0 y0 z0 . On the other hand, if vectors are referred to their own frames, it
is
r 12 = R1 r 112
and also
ṗ1 = R1 ṗ11
ω 1 = R1 ω 11

ṗ2 = R2 ṗ22 = R1 R12 ṗ22
ω 2 = R2 ω 22 = R1 R12 ω 22 .

Accounting for (3.112) and (3.11) gives
R1 R12 ṗ22 = R1 ṗ11 − R1 S(r 112 )RT1 R1 ω 11
R1 R12 ω 22 = R1 ω 11 .
Eliminating the dependence on R1 , which is premultiplied to each term on
both sides of the previous relations, yields17
 2  2
 1 
ṗ2
R1 −R21 S(r 112 )
ṗ1
=
(3.113)
2
2
O
R1
ω2
ω 11
17

Recall that RT R = I, as in (2.4).

3.8 Statics

151

giving the sought general relationship of velocity transformation between two
frames.
It may be observed that the transformation matrix in (3.113) plays the
role of a true Jacobian, since it characterizes a velocity transformation, and
thus (3.113) may be shortly written as
v 22 = J 21 v 11 .

(3.114)

At this point, by virtue of the kineto-statics duality, the force transformation
between two frames can be directly derived in the form
γ 11 = J 21 T γ 22
which can be detailed into18
 1 
f1
R12
=
S(r 112 )R12
μ11

O
R12

(3.115)


f 22
μ22


.

(3.116)

Finally, notice that the above analysis is instantaneous in that, if a coordinate
frame varies with respect to the other, it is necessary to recompute the Jacobian of the transformation through the computation of the related rotation
matrix of one frame with respect to the other.
3.8.3 Closed Chain
As discussed in Sect. 2.8.3, whenever the manipulator contains a closed chain,
there is a functional relationship between the joint variables. In particular,
the closed chain structure is transformed into a tree-structured open chain by
virtually cutting the loop at a joint. It is worth choosing such a cut joint as
one of the unactuated joints. Then, the constraints (2.59) or (2.60) should be
solved for a reduced number of joint variables, corresponding to the DOFs of
the chain. Therefore, it is reasonable to assume that at least such independent
joints are actuated, while the others may or may not be actuated. Let q o =
T
[ q Ta q Tu ] denote the vector of joint variables of the tree-structured open
chain, where q a and q u are the vectors of actuated and unactuated joint
variables, respectively. Assume that from the above constraints it is possible
to determine a functional expression
q u = q u (q a ).

(3.117)

Time diﬀerentiation of (3.117) gives the relationship between joint velocities
in the form
(3.118)
q̇ o = Υ q̇ a
where

18

⎡

⎤
I
Υ = ⎣ ∂q u ⎦
∂q a

The skew-symmetry property S + S T = O is utilized.

(3.119)

152

3 Diﬀerential Kinematics and Statics

is the transformation matrix between the two vectors of joint velocities, which
in turn plays the role of a Jacobian.
At this point, according to an intuitive kineto-statics duality concept, it is
possible to describe the transformation between the corresponding vectors of
joint torques in the form
(3.120)
τa = ΥTτo
where τ o = [ τ Ta

T

τ Tu ] , with obvious meaning of the quantities.

Example 3.5
Consider the parallelogram arm of Sect. 2.9.2. On the assumption to actuate the
two Joints 1 and 1 at the base, it is q a = [ ϑ1 ϑ1 ]T and q u = [ ϑ2 ϑ3 ]T .
Then, using (2.64), the transformation matrix in (3.119) is

⎡

1
⎢ 0
Υ =⎣
−1
1

⎤

0
1 ⎥
.
1 ⎦
−1

Hence, in view of (3.120), the torque vector of the actuated joints is


τa =
while obviously τ u = [ 0
are unactuated.

τ 1 − τ 2 + τ 3
τ1 + τ2 − τ3


(3.121)

0 ]T in agreement with the fact that both Joints 2 and 3

3.9 Manipulability Ellipsoids
The diﬀerential kinematics equation in (3.45) and the statics equation in
(3.111), together with the duality property, allow the deﬁnition of indices for
the evaluation of manipulator performance. Such indices can be helpful both
for mechanical manipulator design and for determining suitable manipulator
postures to execute a given task in the current conﬁguration.
First, it is desired to represent the attitude of a manipulator to arbitrarily
change end-eﬀector position and orientation. This capability is described in
an eﬀective manner by the velocity manipulability ellipsoid.
Consider the set of joint velocities of constant (unit) norm
q̇ T q̇ = 1;

(3.122)

this equation describes the points on the surface of a sphere in the joint velocity space. It is desired to describe the operational space velocities that can

3.9 Manipulability Ellipsoids

153

be generated by the given set of joint velocities, with the manipulator in a
given posture. To this end, one can utilize the diﬀerential kinematics equation
in (3.45) solved for the joint velocities; in the general case of a redundant manipulator (r < n) at a nonsingular conﬁguration, the minimum-norm solution
q̇ = J † (q)v e can be considered which, substituted into (3.122), yields

v Te J †T (q)J † (q) v e = 1.
Accounting for the expression of the pseudo-inverse of J in (3.52) gives
−1
v Te J (q)J T(q)
v e = 1,

(3.123)

which is the equation of the points on the surface of an ellipsoid in the endeﬀector velocity space.
The choice of the minimum-norm solution rules out the presence of internal
motions for the redundant structure. If the general solution (3.54) is used for
q̇, the points satisfying (3.122) are mapped into points inside the ellipsoid
whose surface is described by (3.123).
For a nonredundant manipulator, the diﬀerential kinematics solution (3.47)
is used to derive (3.123); in this case the points on the surface of the sphere in
the joint velocity space are mapped into points on the surface of the ellipsoid
in the end-eﬀector velocity space.
Along the direction of the major axis of the ellipsoid, the end-eﬀector can
move at large velocity, while along the direction of the minor axis small endeﬀector velocities are obtained. Further, the closer the ellipsoid is to a sphere
— unit eccentricity — the better the end-eﬀector can move isotropically along
all directions of the operational space. Hence, it can be understood why this
ellipsoid is an index characterizing manipulation ability of the structure in
terms of velocities.
As can be recognized from (3.123), the shape and orientation of the ellipsoid are determined by the core of its quadratic form and then by the matrix
J J T which is in general a function of the manipulator conﬁguration. The
directions of the principal axes of the ellipsoid are determined by the eigenwhile the dimensions of the
vectors ui , for i = 1, . . . , r, of the matrix J J T , 
axes are given by the singular values of J , σi =

λi (J J T ), for i = 1, . . . , r,

where λi (J J T ) denotes the generic eigenvalue of J J T .
A global representative measure of manipulation ability can be obtained
by considering the volume of the ellipsoid. This volume is proportional to the
quantity


w(q) = det J (q)J T (q)
which is the manipulability measure already introduced in (3.56). In the case
of a nonredundant manipulator (r = n), w reduces to
#
#
(3.124)
w(q) = #det J (q) # .

154

3 Diﬀerential Kinematics and Statics

1

[m]

0.5
0
−0.5
−1
0

0.5

1
[m]

1.5

2

Fig. 3.23. Velocity manipulability ellipses for a two-link planar arm in diﬀerent
postures

It is easy to recognize that it is always w > 0, except for a manipulator at a
singular conﬁguration when w = 0. For this reason, this measure is usually
adopted as a distance of the manipulator from singular conﬁgurations.

Example 3.6
Consider the two-link planar arm. From the expression in (3.41), the manipulability
measure is in this case
w = |det(J )| = a1 a2 |s2 |.
Therefore, as a function of the arm postures, the manipulability is maximum for
ϑ2 = ±π/2. On the other hand, for a given constant reach a1 + a2 , the structure
oﬀering the maximum manipulability, independently of ϑ1 and ϑ2 , is the one with
a1 = a2 .
These results have a biomimetic interpretation in the human arm, if that is
regarded as a two-link arm (arm + forearm). The condition a1 = a2 is satisﬁed with
good approximation. Further, the elbow angle ϑ2 is usually in the neighbourhood of
π/2 in the execution of several tasks, such as that of writing. Hence, the human being
tends to dispose the arm in the most dexterous conﬁguration from a manipulability
viewpoint.
Figure 3.23 illustrates the velocity manipulability ellipses for a certain number of
postures with the tip along the horizontal axis and a1 = a2 = 1. It can be seen that
when the arm is outstretched the ellipsoid is very thin along the vertical direction.
Hence, one recovers the result anticipated in the study of singularities that the arm
in this posture can generate tip velocities preferably along the vertical direction. In
Fig. 3.24, moreover, the behaviour of the minimum and maximum singular values of
the matrix J is illustrated as a function of tip position along axis x; it can be veriﬁed
that the minimum singular value is null when the manipulator is at a singularity
(retracted or outstretched).
Therefore, with reference to the postures, manipulability has a maximum for
ϑ2 = ±π/2. On the other hand, for a given total extension a1 + a2 , the structure
which, independently of ϑ1 and ϑ2 , oﬀers the largest manipulability is that with
a1 = a2 .

3.9 Manipulability Ellipsoids
2.5

155

max

[m]

2
1.5
1
min

0.5
0
0

0.5

1
[m]

1.5

2

Fig. 3.24. Minimum and maximum singular values of J for a two-link planar arm
as a function of the arm posture

1

[m]

0.5
0
−0.5
−1
0

0.5

1
[m]

1.5

2

Fig. 3.25. Force manipulability ellipses for a two-link planar arm in diﬀerent postures

The manipulability measure w has the advantage of being easy to compute,
through the determinant of matrix J J T . However, its numerical value does
not constitute an absolute measure of the actual closeness of the manipulator
to a singularity. It is enough to consider the above example and take two
arms of identical structure, one with links of 1 m and the other with links of
1 cm. Two diﬀerent values of manipulability are obtained which diﬀer by four
orders of magnitude. Hence, in that case it is convenient to consider only |s2 |
— eventually |ϑ2 | — as the manipulability measure. In more general cases
when it is not easy to ﬁnd a simple, meaningful index, one can consider the
ratio between the minimum and maximum singular values of the Jacobian
σr /σ1 which is equivalent to the inverse of the condition number of matrix J .
This ratio gives not only a measure of the distance from a singularity (σr = 0),
but also a direct measure of eccentricity of the ellipsoid. The disadvantage in
utilizing this index is its computational complexity; it is practically impossible
to compute it in symbolic form, i.e., as a function of the joint conﬁguration,
except for matrices of reduced dimension.
On the basis of the existing duality between diﬀerential kinematics and
statics, it is possible to describe the manipulability of a structure not only

156

3 Diﬀerential Kinematics and Statics

with reference to velocities, but also with reference to forces. To be speciﬁc,
one can consider the sphere in the space of joint torques
τTτ = 1

(3.125)

which, accounting for (3.111), is mapped into the ellipsoid in the space of
end-eﬀector forces

γ Te J (q)J T(q) γ e = 1
(3.126)
which is deﬁned as the force manipulability ellipsoid . This ellipsoid characterizes the end-eﬀector forces that can be generated with the given set of joint
torques, with the manipulator in a given posture.
As can be easily recognized from (3.126), the core of the quadratic form is
constituted by the inverse of the matrix core of the velocity ellipsoid in (3.123).
This feature leads to the notable result that the principal axes of the force
manipulability ellipsoid coincide with the principal axes of the velocity manipulability ellipsoid, while the dimensions of the respective axes are in inverse
proportion. Therefore, according to the concept of force/velocity duality, a
direction along which good velocity manipulability is obtained is a direction
along which poor force manipulability is obtained, and vice versa.
In Fig. 3.25, the manipulability ellipses for the same postures as those
of the example in Fig. 3.23 are illustrated. A comparison of the shape and
orientation of the ellipses conﬁrms the force/velocity duality eﬀect on the
manipulability along diﬀerent directions.
It is worth pointing out that these manipulability ellipsoids can be represented geometrically in all cases of an operational space of dimension at most
3. Therefore, if it is desired to analyze manipulability in a space of greater
dimension, it is worth separating the components of linear velocity (force)
from those of angular velocity (moment), also avoiding problems due to nonhomogeneous dimensions of the relevant quantities (e.g., m/s vs rad/s). For
instance, for a manipulator with a spherical wrist, the manipulability analysis
is naturally prone to a decoupling between arm and wrist.
An eﬀective interpretation of the above results can be achieved by regarding the manipulator as a mechanical transformer of velocities and forces from
the joint space to the operational space. Conservation of energy dictates that
an ampliﬁcation in the velocity transformation is necessarily accompanied by
a reduction in the force transformation, and vice versa. The transformation
ratio along a given direction is determined by the intersection of the vector
along that direction with the surface of the ellipsoid. Once a unit vector u
along a direction has been assigned, it is possible to compute the transformation ratio for the force manipulability ellipsoid as
−1/2


α(q) =

T

T

u J (q)J (q)u

(3.127)

3.9 Manipulability Ellipsoids

157

velocity

writing plane
force

Fig. 3.26. Velocity and force manipulability ellipses for a 3-link planar arm in a
typical conﬁguration for a task of controlling force and velocity

and for the velocity manipulability ellipsoid as

β(q) =

−1
u
uT J (q)J T (q)

−1/2
.

(3.128)

The manipulability ellipsoids can be conveniently utilized not only for analyzing manipulability of the structure along diﬀerent directions of the operational space, but also for determining compatibility of the structure to execute
a task assigned along a direction. To this end, it is useful to distinguish between actuation tasks and control tasks of velocity and force. In terms of the
relative ellipsoid, the task of actuating a velocity (force) requires preferably
a large transformation ratio along the task direction, since for a given set of
joint velocities (forces) at the joints it is possible to generate a large velocity
(force) at the end-eﬀector. On the other hand, for a control task it is important to have a small transformation ratio so as to gain good sensitivity to
errors that may occur along the given direction.
Revisiting once again the duality between velocity manipulability ellipsoid
and force manipulability ellipsoid, it can be found that an optimal direction to
actuate a velocity is also an optimal direction to control a force. Analogously,
a good direction to actuate a force is also a good direction to control a velocity.
To have a tangible example of the above concept, consider the typical task
of writing on a horizontal surface for the human arm; this time, the arm is regarded as a 3-link planar arm: arm + forearm + hand. Restricting the analysis
to a two-dimensional task space (the direction vertical to the surface and the
direction of the line of writing), one has to achieve ﬁne control of the vertical
force (the pressure of the pen on the paper) and of the horizontal velocity (to
write in good calligraphy). As a consequence, the force manipulability ellipse
tends to be oriented horizontally for correct task execution. Correspondingly,
the velocity manipulability ellipse tends to be oriented vertically in perfect
agreement with the task requirement. In this case, from Fig. 3.26 the typical
conﬁguration of the human arm when writing can be recognized.

158

3 Diﬀerential Kinematics and Statics

force
velocity
throwing direction

Fig. 3.27. Velocity and force manipulability ellipses for a 3-link planar arm in a
typical conﬁguration for a task of actuating force and velocity

An opposite example to the previous one is that of the human arm when
throwing a weight in the horizontal direction. In fact, now it is necessary to
actuate a large vertical force (to sustain the weight) and a large horizontal
velocity (to throw the load for a considerable distance). Unlike the above, the
force (velocity) manipulability ellipse tends to be oriented vertically (horizontally) to successfully execute the task. The relative conﬁguration in Fig. 3.27
is representative of the typical attitude of the human arm when, for instance,
releasing the ball in a bowling game.
In the above two examples, it is worth pointing out that the presence of a
two-dimensional operational space is certainly advantageous to try reconﬁguring the structure in the best conﬁguration compatible with the given task.
In fact, the transformation ratios deﬁned in (3.127) and (3.128) are scalar
functions of the manipulator conﬁgurations that can be optimized locally according to the technique for exploiting redundant DOFs previously illustrated.

Bibliography
The concept of geometric Jacobian was originally introduced in [240] and the
problem of its computationally eﬃcient determination is considered in [173].
The concept of analytical Jacobian is presented in [114] with reference to
operational space control.
Inverse diﬀerential kinematics dates back to [240] under the name of resolved rate control. The use of the Jacobian pseudo-inverse is due to [118].
The adoption of the damped least-squares inverse has been independently
proposed by [161] and [238]; a tutorial on the topic is [42]. The inverse
kinematics algorithm based on the Jacobian transpose has been originally
proposed in [198, 16]. Further details about the orientation error are found
in [142, 250, 132, 41].
The utilization of the joint velocities in the null space of the Jacobian for
redundancy resolution is proposed in [129] and further reﬁned in [147] regarding the choice of the objective functions. The approach based on task priority

Problems

159

is presented in [163]; other approaches based on the concept of augmented
task space are presented in [14, 69, 199, 203, 194, 37]. For global redundancy
resolutions see [162]. A complete treatment of redundant manipulators can be
found in [160] while a tutorial is [206].
The extension of inverse kinematics to the second order has been proposed
in [207], while the symbolic diﬀerentiation of the solutions in terms of joint
velocities to obtain stable acceleration solutions can be found in [208]. Further
details about redundancy resolution are in [59].
The concepts of kineto-statics duality are discussed in [191]. The manipulability ellipsoids are proposed in [245, 248] and employed in [44] for posture
dexterity analysis with regard to manipulation tasks.

Problems
3.1. Prove (3.11).
3.2. Compute the Jacobian of the cylindrical arm in Fig. 2.35.
3.3. Compute the Jacobian of the SCARA manipulator in Fig. 2.36.
3.4. Find the singularities of the 3-link planar arm in Fig. 2.20.
3.5. Find the singularities of the spherical arm in Fig. 2.22.
3.6. Find the singularities of the cylindrical arm in Fig. 2.35.
3.7. Find the singularities of the SCARA manipulator in Fig. 2.36.
3.8. Show that the manipulability measure deﬁned in (3.56) is given by the
product of the singular values of the Jacobian matrix.
3.9. For the 3-link planar arm in Fig. 2.20, ﬁnd an expression of the distance
of the arm from a circular obstacle of given radius and coordinates.
3.10. Find the solution to the diﬀerential kinematics equation with the
damped least-square inverse in (3.59).
3.11. Prove (3.64) in an alternative way, i.e., by computing S(ω e ) as in (3.6)
starting from R(φ) in (2.18).
3.12. With reference to (3.64), ﬁnd the transformation matrix T (φe ) in the
case of RPY angles.
3.13. With reference to (3.64), ﬁnd the triplet of Euler angles for which
T (0) = I.
3.14. Show how the inverse kinematics scheme of Fig. 3.11 can be simpliﬁed
in the case of a manipulator having a spherical wrist.

160

3 Diﬀerential Kinematics and Statics

3.15. Find an expression of the upper bound on the norm of e for the solution (3.76) in the case ẋd = 0.
3.16. Prove (3.81).
3.17. Prove (3.86), (3.87).
3.18. Prove that the equation relating the angular velocity to the time derivative of the quaternion is given by
ω = 2S()˙ + 2η ˙ − 2η̇.
[Hint: Start by showing that (2.33) can be rewritten as R(η, ) = (2η 2 − 1)I +
2T + 2ηS()].
3.19. Prove (3.94), (3.95).
3.20. Prove that the time derivative of the Lyapunov function in (3.96) is
given by (3.97).
3.21. Consider the 3-link planar arm in Fig. 2.20, whose link lengths are
respectively 0.5 m, 0.3 m, 0.3 m. Perform a computer implementation of the
inverse kinematics algorithm using the Jacobian pseudo-inverse along the operational space path given by a straight line connecting the points of coordinates (0.8, 0.2) m and (0.8, −0.2) m. Add a constraint aimed at avoiding link
collision with a circular object located at ø = [ 0.3 0 ]T m of radius 0.1 m. The
initial arm conﬁguration is chosen so that pe (0) = pd (0). The ﬁnal time is
2 s. Use sinusoidal motion timing laws. Adopt the Euler numerical integration
scheme (3.48) with an integration time Δt = 1 ms.
3.22. Consider the SCARA manipulator in Fig. 2.36, whose links both have a
length of 0.5 m and are located at a height of 1 m from the supporting plane.
Perform a computer implementation of the inverse kinematics algorithms with
both Jacobian inverse and Jacobian transpose along the operational space
path whose position is given by a straight line connecting the points of coordinates (0.7, 0, 0) m and (0, 0.8, 0.5) m, and whose orientation is given by
a rotation from 0 rad to π/2 rad. The initial arm conﬁguration is chosen so
that xe (0) = xd (0). The ﬁnal time is 2 s. Use sinusoidal motion timing laws.
Adopt the Euler numerical integration scheme (3.48) with an integration time
Δt = 1 ms.
3.23. Prove that the directions of the principal axes of the force and velocity
manipulability ellipsoids coincide while their dimensions are in inverse proportion.

4
Trajectory Planning

For the execution of a speciﬁc robot task, it is worth considering the main
features of motion planning algorithms. The goal of trajectory planning is to
generate the reference inputs to the motion control system which ensures that
the manipulator executes the planned trajectories. The user typically speciﬁes
a number of parameters to describe the desired trajectory. Planning consists of
generating a time sequence of the values attained by an interpolating function
(typically a polynomial) of the desired trajectory. This chapter presents some
techniques for trajectory generation, both in the case when the initial and
ﬁnal point of the path are assigned (point-to-point motion), and in the case
when a ﬁnite sequence of points are assigned along the path (motion through
a sequence of points). First, the problem of trajectory planning in the joint
space is considered, and then the basic concepts of trajectory planning in
the operational space are illustrated. The treatment of the motion planning
problem for mobile robots is deferred to Chap. 12.

4.1 Path and Trajectory
The minimal requirement for a manipulator is the capability to move from
an initial posture to a ﬁnal assigned posture. The transition should be characterized by motion laws requiring the actuators to exert joint generalized
forces which do not violate the saturation limits and do not excite the typically modelled resonant modes of the structure. It is then necessary to devise
planning algorithms that generate suitably smooth trajectories.
In order to avoid confusion between terms often used as synonyms, the
diﬀerence between a path and a trajectory is to be explained. A path denotes
the locus of points in the joint space, or in the operational space, which the
manipulator has to follow in the execution of the assigned motion; a path is
then a pure geometric description of motion. On the other hand, a trajectory
is a path on which a timing law is speciﬁed, for instance in terms of velocities
and/or accelerations at each point.

162

4 Trajectory Planning

In principle, it can be conceived that the inputs to a trajectory planning
algorithm are the path description, the path constraints, and the constraints
imposed by manipulator dynamics, whereas the outputs are the end-eﬀector
trajectories in terms of a time sequence of the values attained by position,
velocity and acceleration.
A geometric path cannot be fully speciﬁed by the user for obvious complexity reasons. Typically, a reduced number of parameters is speciﬁed such
as extremal points, possible intermediate points, and geometric primitives interpolating the points. Also, the motion timing law is not typically speciﬁed
at each point of the geometric path, but rather it regards the total trajectory
time, the constraints on the maximum velocities and accelerations, and eventually the assignment of velocity and acceleration at points of particular interest. On the basis of the above information, the trajectory planning algorithm
generates a time sequence of variables that describe end-eﬀector position and
orientation over time in respect of the imposed constraints. Since the control
action on the manipulator is carried out in the joint space, a suitable inverse
kinematics algorithm is to be used to reconstruct the time sequence of joint
variables corresponding to the above sequence in the operational space.
Trajectory planning in the operational space naturally allows the presence
of path constraints to be accounted; these are due to regions of workspace
which are forbidden to the manipulator, e.g., due to the presence of obstacles.
In fact, such constraints are typically better described in the operational space,
since their corresponding points in the joint space are diﬃcult to compute.
With regard to motion in the neighbourhood of singular conﬁgurations and
presence of redundant DOFs, trajectory planning in the operational space may
involve problems diﬃcult to solve. In such cases, it may be advisable to specify
the path in the joint space, still in terms of a reduced number of parameters.
Hence, a time sequence of joint variables has to be generated which satisfy
the constraints imposed on the trajectory.
For the sake of clarity, in the following, the case of joint space trajectory
planning is treated ﬁrst. The results will then be extended to the case of
trajectories in the operational space.

4.2 Joint Space Trajectories
A manipulator motion is typically assigned in the operational space in terms
of trajectory parameters such as the initial and ﬁnal end-eﬀector pose, possible intermediate poses, and travelling time along particular geometric paths.
If it is desired to plan a trajectory in the joint space, the values of the joint
variables have to be determined ﬁrst from the end-eﬀector position and orientation speciﬁed by the user. It is then necessary to resort to an inverse
kinematics algorithm, if planning is done oﬀ-line, or to directly measure the
above variables, if planning is done by the teaching-by-showing technique (see
Chap. 6).

4.2 Joint Space Trajectories

163

The planning algorithm generates a function q(t) interpolating the given
vectors of joint variables at each point, in respect of the imposed constraints.
In general, a joint space trajectory planning algorithm is required to have
the following features:
• the generated trajectories should be not very demanding from a computational viewpoint,
• the joint positions and velocities should be continuous functions of time
(continuity of accelerations may be imposed, too),
• undesirable eﬀects should be minimized, e.g., nonsmooth trajectories interpolating a sequence of points on a path.
At ﬁrst, the case is examined when only the initial and ﬁnal points on
the path and the traveling time are speciﬁed (point-to-point); the results are
then generalized to the case when also intermediate points along the path are
speciﬁed (motion through a sequence of points). Without loss of generality,
the single joint variable q(t) is considered.
4.2.1 Point-to-Point Motion
In point-to-point motion, the manipulator has to move from an initial to a
ﬁnal joint conﬁguration in a given time tf . In this case, the actual end-eﬀector
path is of no concern. The algorithm should generate a trajectory which, in
respect to the above general requirements, is also capable of optimizing some
performance index when the joint is moved from one position to another.
A suggestion for choosing the motion primitive may stem from the analysis
of an incremental motion problem. Let I be the moment of inertia of a rigid
body about its rotation axis. It is required to take the angle q from an initial
value qi to a ﬁnal value qf in a time tf . It is obvious that inﬁnite solutions
exist to this problem. Assumed that rotation is executed through a torque τ
supplied by a motor, a solution can be found which minimizes the energy dissipated in the motor. This optimization problem can be formalized as follows.
Having set q̇ = ω, determine the solution to the diﬀerential equation
I ω̇ = τ
subject to the condition
"

tf

ω(t)dt = qf − qi

o

so as to minimize the performance index
" tf
τ 2 (t)dt.
0

It can be shown that the resulting solution is of the type
ω(t) = at2 + bt + c.

164

4 Trajectory Planning

Even though the joint dynamics cannot be described in the above simple
manner,1 the choice of a third-order polynomial function to generate a joint
trajectory represents a valid solution for the problem at issue.
Therefore, to determine a joint motion, the cubic polynomial
q(t) = a3 t3 + a2 t2 + a1 t + a0

(4.1)

can be chosen, resulting into a parabolic velocity proﬁle
q̇(t) = 3a3 t2 + 2a2 t + a1
and a linear acceleration proﬁle
q̈(t) = 6a3 t + 2a2 .
Since four coeﬃcients are available, it is possible to impose, besides the initial
and ﬁnal joint position values qi and qf , also the initial and ﬁnal joint velocity
values q̇i and q̇f which are usually set to zero. Determination of a speciﬁc
trajectory is given by the solution to the following system of equations:
a0 = qi
a1 = q̇i
a3 t3f + a2 t2f + a1 tf + a0 = qf
3a3 t2f + 2a2 tf + a1 = q̇f ,
that allows the computation of the coeﬃcients of the polynomial in (4.1).2
Figure 4.1 illustrates the timing law obtained with the following data: qi = 0,
qf = π, tf = 1, and q̇i = q̇f = 0. As anticipated, velocity has a parabolic proﬁle, while acceleration has a linear proﬁle with initial and ﬁnal discontinuity.
If it is desired to assign also the initial and ﬁnal values of acceleration, six
constraints have to be satisﬁed and then a polynomial of at least ﬁfth order
is needed. The motion timing law for the generic joint is then given by
q(t) = a5 t5 + a4 t4 + a3 t3 + a2 t2 + a1 t + a0 ,

(4.2)

whose coeﬃcients can be computed, as for the previous case, by imposing the
conditions for t = 0 and t = tf on the joint variable q(t) and on its ﬁrst
two derivatives. With the choice (4.2), one obviously gives up minimizing the
above performance index.
An alternative approach with timing laws of blended polynomial type is
frequently adopted in industrial practice, which allows a direct veriﬁcation
1
2

In fact, recall that the moment of inertia about the joint axis is a function of
manipulator conﬁguration.
Notice that it is possible to normalize the computation of the coeﬃcients, so as
to be independent both on the ﬁnal time tf and on the path length |qf − qi |.

4.2 Joint Space Trajectories

165

pos

[rad]

3
2
1
0
0

0.2

0.4

0.6
[s]

0.8

1

0.8

1

0.8

1

vel

5

[rad/s]

4
3
2
1
0
0

0.2

0.4

0.6
[s]
acc

20

[rad/s^2]

10
0
−10
−20
0

0.2

0.4

0.6
[s]

Fig. 4.1. Time history of position, velocity and acceleration with a cubic polynomial
timing law

of whether the resulting velocities and accelerations can be supported by the
physical mechanical manipulator.
In this case, a trapezoidal velocity proﬁle is assigned, which imposes a
constant acceleration in the start phase, a cruise velocity, and a constant
deceleration in the arrival phase. The resulting trajectory is formed by a linear
segment connected by two parabolic segments to the initial and ﬁnal positions.
In the following, the problem is formulated by assuming that the ﬁnal time
of trajectory duration has been assigned. However, in industrial practice, the
user is oﬀered the option to specify the velocity percentage with respect to the
maximum allowable velocity; this choice is aimed at avoiding occurrences when

166

4 Trajectory Planning

Fig. 4.2. Characterization of a timing law with trapezoidal velocity proﬁle in terms
of position, velocity and acceleration

the speciﬁcation of a much too short motion duration would involve much too
large values of velocities and/or accelerations, beyond those achievable by the
manipulator.
As can be seen from the velocity proﬁles in Fig. 4.2, it is assumed that both
initial and ﬁnal velocities are null and the segments with constant accelerations
have the same time duration; this implies an equal magnitude q̈c in the two
segments. Notice also that the above choice leads to a symmetric trajectory
with respect to the average point qm = (qf + qi )/2 at tm = tf /2.
The trajectory has to satisfy some constraints to ensure the transition
from qi to qf in a time tf . The velocity at the end of the parabolic segment
must be equal to the (constant) velocity of the linear segment, i.e.,
q̈c tc =

qm − qc
tm − tc

(4.3)

where qc is the value attained by the joint variable at the end of the parabolic
segment at time tc with constant acceleration q̈c (recall that q̇(0) = 0). It is
then
1
(4.4)
qc = qi + q̈c t2c .
2
Combining (4.3), (4.4) gives
q̈c t2c − q̈c tf tc + qf − qi = 0.

(4.5)

4.2 Joint Space Trajectories

167

Usually, q̈c is speciﬁed with the constraint that sgn q̈c = sgn (qf − qi ); hence,
for given tf , qi and qf , the solution for tc is computed from (4.5) as (tc ≤ tf /2)
$
1 t2f q̈c − 4(qf − qi )
tf
−
tc =
.
(4.6)
2
2
q̈c
Acceleration is then subject to the constraint
|q̈c | ≥

4|qf − qi |
.
t2f

(4.7)

When the acceleration q̈c is chosen so as to satisfy (4.7) with the equality
sign, the resulting trajectory does not feature the constant velocity segment
any more and has only the acceleration and deceleration segments (triangular
proﬁle).
Given qi , qf and tf , and thus also an average transition velocity, the constraint in (4.7) allows the imposition of a value of acceleration consistent with
the trajectory. Then, tc is computed from (4.6), and the following sequence of
polynomials is generated:
⎧
1
2
0 ≤ t ≤ tc
⎪
⎨ qi + 2 q̈c t
q(t) = qi + q̈c tc (t − tc /2)
tc < t ≤ tf − tc
(4.8)
⎪
⎩
qf − 12 q̈c (tf − t)2
tf − tc < t ≤ tf .
Figure 4.3 illustrates a representation of the motion timing law obtained by
imposing the data: qi = 0, qf = π, tf = 1, and |q̈c | = 6π.
Specifying acceleration in the parabolic segment is not the only way to
determine trajectories with trapezoidal velocity proﬁle. Besides qi , qf and tf ,
one can specify also the cruise velocity q̇c which is subject to the constraint
|qf − qi |
2|qf − qi |
< |q̇c | ≤
.
tf
tf

(4.9)

By recognizing that q̇c = q̈c tc , (4.5) allows the computation of tc as
tc =

qi − qf + q̇c tf
,
q̇c

(4.10)

and thus the resulting acceleration is
q̈c =

q̇c2
.
qi − qf + q̇c tf

(4.11)

The computed values of tc and q̈c as in (4.10), (4.11) allow the generation of
the sequence of polynomials expressed by (4.8).
The adoption of a trapezoidal velocity proﬁle results in a worse performance index compared
to the cubic polynomial. The decrease is, however,
%t
limited; the term 0 f τ 2 dt increases by 12.5% with respect to the optimal
case.

168

4 Trajectory Planning
pos

[rad]

3
2
1
0
0

0.2

0.4

0.6
[s]

0.8

1

0.8

1

0.8

1

vel

5

[rad/s]

4
3
2
1
0
0

0.2

0.4

0.6
[s]
acc

20

[rad/s^2]

10
0
−10
−20
0

0.2

0.4

0.6
[s]

Fig. 4.3. Time history of position, velocity and acceleration with a trapezoidal
velocity proﬁle timing law

4.2.2 Motion Through a Sequence of Points
In several applications, the path is described in terms of a number of points
greater than two. For instance, even for the simple point-to-point motion
of a pick-and-place task, it may be worth assigning two intermediate points
between the initial point and the ﬁnal point; suitable positions can be set for
lifting oﬀ and setting down the object, so that reduced velocities are obtained
with respect to direct transfer of the object. For more complex applications,
it may be convenient to assign a sequence of points so as to guarantee better
monitoring on the executed trajectories; the points are to be speciﬁed more
densely in those segments of the path where obstacles have to be avoided

4.2 Joint Space Trajectories

169

Fig. 4.4. Characterization of a trajectory on a given path obtained through interpolating polynomials

or a high path curvature is expected. It should not be forgotten that the
corresponding joint variables have to be computed from the operational space
poses.
Therefore, the problem is to generate a trajectory when N points, termed
path points, are speciﬁed and have to be reached by the manipulator at certain
instants of time. For each joint variable there are N constraints, and then one
might want to use an (N − 1)-order polynomial. This choice, however, has the
following disadvantages:
• It is not possible to assign the initial and ﬁnal velocities.
• As the order of a polynomial increases, its oscillatory behaviour increases,
and this may lead to trajectories which are not natural for the manipulator.
• Numerical accuracy for computation of polynomial coeﬃcients decreases
as order increases.
• The resulting system of constraint equations is heavy to solve.
• Polynomial coeﬃcients depend on all the assigned points; thus, if it is
desired to change a point, all of them have to be recomputed.
These drawbacks can be overcome if a suitable number of low-order interpolating polynomials, continuous at the path points, are considered in place
of a single high-order polynomial.
According to the previous section, the interpolating polynomial of lowest
order is the cubic polynomial , since it allows the imposition of continuity of
velocities at the path points. With reference to the single joint variable, a
function q(t) is sought, formed by a sequence of N − 1 cubic polynomials
Πk (t), for k = 1, . . . , N − 1, continuous with continuous ﬁrst derivatives. The
function q(t) attains the values qk for t = tk (k = 1, . . . , N ), and q1 = qi ,
t1 = 0, qN = qf , tN = tf ; the qk ’s represent the path points describing

170

4 Trajectory Planning

the desired trajectory at t = tk (Fig. 4.4). The following situations can be
considered:
• Arbitrary values of q̇(t) are imposed at the path points.
• The values of q̇(t) at the path points are assigned according to a certain
criterion.
• The acceleration q̈(t) has to be continuous at the path points.
To simplify the problem, it is also possible to ﬁnd interpolating polynomials
of order less than three which determine trajectories passing nearby the path
points at the given instants of time.
Interpolating polynomials with imposed velocities at path points
This solution requires the user to be able to specify the desired velocity at
each path point; the solution does not possess any novelty with respect to the
above concepts.
The system of equations allowing computation of the coeﬃcients of the
N − 1 cubic polynomials interpolating the N path points is obtained by imposing the following conditions on the generic polynomial Πk (t) interpolating
qk and qk+1 , for k = 1, . . . , N − 1:
Πk (tk ) = qk
Πk (tk+1 ) = qk+1
Π̇k (tk ) = q̇k
Π̇k (tk+1 ) = q̇k+1 .
The result is N − 1 systems of four equations in the four unknown coeﬃcients
of the generic polynomial; these can be solved one independently of the other.
The initial and ﬁnal velocities of the trajectory are typically set to zero (q̇1 =
q̇N = 0) and continuity of velocity at the path points is ensured by setting
Π̇k (tk+1 ) = Π̇k+1 (tk+1 )
for k = 1, . . . , N − 2.
Figure 4.5 illustrates the time history of position, velocity and acceleration
obtained with the data: q1 = 0, q2 = 2π, q3 = π/2, q4 = π, t1 = 0, t2 = 2, t3 =
3, t4 = 5, q̇1 = 0, q̇2 = π, q̇3 = −π, q̇4 = 0. Notice the resulting discontinuity
on the acceleration, since only continuity of velocity is guaranteed.
Interpolating polynomials with computed velocities at path points
In this case, the joint velocity at a path point has to be computed according
to a certain criterion. By interpolating the path points with linear segments,
the relative velocities can be computed according to the following rules:

4.2 Joint Space Trajectories

171

pos

[rad]

6
4
2
0
0

2

[s]

4

6

4

6

4

6

vel

[rad/s]

5

0

−5

0

2

[s]
acc

[rad/s^2]

20
0
−20
−40
0

2

[s]

Fig. 4.5. Time history of position, velocity and acceleration with a timing law of
interpolating polynomials with velocity constraints at path points

q̇1 = 0

0
q̇k = 1

2 (vk + vk+1 )

sgn (vk ) = sgn (vk+1 )
sgn (vk ) = sgn (vk+1 )

(4.12)

q̇N = 0,
where vk = (qk − qk−1 )/(tk − tk−1 ) gives the slope of the segment in the
time interval [tk−1 , tk ]. With the above settings, the determination of the
interpolating polynomials is reduced to the previous case.
Figure 4.6 illustrates the time history of position, velocity and acceleration
obtained with the following data: q1 = 0, q2 = 2π, q3 = π/2, q4 = π, t1 = 0,
t2 = 2, t3 = 3, t4 = 5, q̇1 = 0, q̇4 = 0. It is easy to recognize that the imposed

172

4 Trajectory Planning
pos

[rad]

6
4
2
0
0

2

[s]

4

6

4

6

4

6

vel

[rad/s]

5

0

−5

0

2

[s]
acc

30
[rad/s^2]

20
10
0
−10
−20
−30
0

2

[s]

Fig. 4.6. Time history of position, velocity and acceleration with a timing law of
interpolating polynomials with computed velocities at path points

sequence of path points leads to having zero velocity at the intermediate
points.
Interpolating polynomials with continuous accelerations at path
points (splines)
Both the above two solutions do not ensure continuity of accelerations at
the path points. Given a sequence of N path points, the acceleration is also
continuous at each tk if four constraints are imposed, namely, two position
constraints for each of the adjacent cubics and two constraints guaranteeing

4.2 Joint Space Trajectories

173

continuity of velocity and acceleration. The following equations have then to
be satisﬁed:
Πk−1 (tk ) = qk
Πk−1 (tk ) = Πk (tk )
Π̇k−1 (tk ) = Π̇k (tk )
Π̈k−1 (tk ) = Π̈k (tk ).
The resulting system for the N path points, including the initial and ﬁnal
points, cannot be solved. In fact, it is formed by 4(N − 2) equations for the
intermediate points and 6 equations for the extremal points; the position
constraints for the polynomials Π0 (t1 ) = qi and ΠN (tf ) = qf have to be
excluded since they are not deﬁned. Also, Π̇0 (t1 ), Π̈0 (t1 ), Π̇N (tf ), Π̈N (tf ) do
not have to be counted as polynomials since they are just the imposed values
of initial and ﬁnal velocities and accelerations. In summary, one has 4N − 2
equations in 4(N − 1) unknowns.
The system can be solved only if one eliminates the two equations which
allow the arbitrary assignment of the initial and ﬁnal acceleration values.
Fourth-order polynomials should be used to include this possibility for the
ﬁrst and last segment.
On the other hand, if only third-order polynomials are to be used, the following deception can be operated. Two virtual points are introduced for which
continuity constraints on position, velocity and acceleration can be imposed,
without specifying the actual positions, though. It is worth remarking that the
eﬀective location of these points is irrelevant, since their position constraints
regard continuity only. Hence, the introduction of two virtual points implies
the determination of N + 1 cubic polynomials.
Consider N + 2 time instants tk , where t2 and tN +1 conventionally refer to
the virtual points. The system of equations for determining the N + 1 cubic
polynomials can be found by taking the 4(N − 2) equations:
Πk−1 (tk ) = qk

(4.13)

Πk−1 (tk ) = Πk (tk )
Π̇k−1 (tk ) = Π̇k (tk )
Π̈k−1 (tk ) = Π̈k (tk )

(4.14)
(4.15)
(4.16)

for k = 3, . . . , N , written for the N − 2 intermediate path points, the 6 equations:
Π1 (t1 ) = qi

(4.17)

Π̇1 (t1 ) = q̇i
Π̈1 (t1 ) = q̈i ,

(4.18)
(4.19)

ΠN +1 (tN +2 ) = qf

(4.20)

174

4 Trajectory Planning

Π̇N +1 (tN +2 ) = q̇f

(4.21)

Π̈N +1 (tN +2 ) = q̈f

(4.22)

written for the initial and ﬁnal points, and the 6 equations:
Πk−1 (tk ) = Πk (tk )

(4.23)

Π̇k−1 (tk ) = Π̇k (tk )
Π̈k−1 (tk ) = Π̈k (tk )

(4.24)
(4.25)

for k = 2, N + 1, written for the two virtual points. The resulting system
has 4(N + 1) equations in 4(N + 1) unknowns, that are the coeﬃcients of the
N + 1 cubic polynomials.
The solution to the system is computationally demanding, even for low
values of N . Nonetheless, the problem can be cast in a suitable form so as
to solve the resulting system of equations with a computationally eﬃcient
algorithm. Since the generic polynomial Πk (t) is a cubic, its second derivative
must be a linear function of time which then can be written as
Π̈k (t) =

Π̈k (tk )
Π̈k (tk+1 )
(tk+1 − t) +
(t − tk )
Δtk
Δtk

k = 1, . . . , N + 1, (4.26)

where Δtk = tk+1 − tk indicates the time interval to reach qk+1 from qk . By
integrating (4.26) twice over time, the generic polynomial can be written as
Πk (t) =

Π̈k (tk )
Π̈k (tk+1 )
(tk+1 − t)3 +
(t − tk )3
(4.27)
6Δtk
6Δtk
'
&
Πk (tk+1 ) Δtk Π̈k (tk+1 )
(t − tk )
−
+
Δtk
6
'
&
Πk (tk ) Δtk Π̈k (tk )
(tk+1 − t)
+
−
k = 1, . . . , N + 1,
Δtk
6

which depends on the 4 unknowns: Πk (tk ), Πk (tk+1 ), Π̈k (tk ), Π̈k (tk+1 ).
Notice that the N variables qk for k = 2, N + 1 are given via (4.13), while
continuity is imposed for q2 and qN +1 via (4.23). By using (4.14), (4.17),
(4.20), the unknowns in the N + 1 equations in (4.27) reduce to 2(N + 2).
By observing that the equations in (4.18), (4.21) depend on q2 and qN +1 , and
that q̇i and q̇f are given, q2 and qN +1 can be computed as a function of Π̈1 (t1 )
and Π̈N +1 (tN +2 ), respectively. Thus, a number of 2(N + 1) unknowns are left.
By accounting for (4.16), (4.25), and noticing that in ((4.19), (4.22) q̈i and
q̈f are given, the unknowns reduce to N .
At this point, (4.15), (4.24) can be utilized to write the system of N
equations in N unknowns:
Π̇1 (t2 ) = Π̇2 (t2 )

4.2 Joint Space Trajectories

175

..
.
Π̇N (tN +1 ) = Π̇N +1 (tN +1 ).
Time-diﬀerentiation of (4.27) gives both Π̇k (tk+1 ) and Π̇k+1 (tk+1 ) for k =
1, . . . , N , and thus it is possible to write a system of linear equations of the
kind
T
(4.28)
A [ Π̈2 (t2 ) . . . Π̈N +1 (tN +1 ) ] = b
which presents a vector b of known terms and a nonsingular coeﬃcient matrix
A; the solution to this system always exists and is unique. It can be shown
that the matrix A has a tridiagonal band structure of the type
⎡
⎤
a11 a12 . . .
0
0
0
0
⎢ a21 a22 . . .
⎥
⎢ .
⎥
..
..
..
..
⎥,
.
A=⎢
.
.
.
.
⎢ .
⎥
⎣ 0
0 . . . aN −1,N −1 aN −1,N ⎦
0
0 ...
aN,N −1
aN N
which simpliﬁes the solution to the system (see Problem 4.4). This matrix
is the same for all joints, since it depends only on the time intervals Δtk
speciﬁed.
An eﬃcient solution algorithm exists for the above system which is given
by a forward computation followed by a backward computation. From the
ﬁrst equation, Π̈2 (t2 ) can be computed as a function of Π̈3 (t3 ) and then
substituted in the second equation, which then becomes an equation in the
unknowns Π̈3 (t3 ) and Π̈4 (t4 ). This is carried out forward by transforming all
the equations in equations with two unknowns, except the last one which will
have Π̈N +1 (tN +1 ) only as unknown. At this point, all the unknowns can be
determined step by step through a backward computation.
The above sequence of cubic polynomials is termed spline to indicate
smooth functions that interpolate a sequence of given points ensuring continuity of the function and its derivatives.
Figure 4.7 illustrates the time history of position, velocity and acceleration
obtained with the data: q1 = 0, q3 = 2π, q4 = π/2, q6 = π, t1 = 0, t3 = 2,
t4 = 3, t6 = 5, q̇1 = 0, q̇6 = 0. Two diﬀerent pairs of virtual points were
considered at the time instants: t2 = 0.5, t5 = 4.5 (solid line in the ﬁgure),
and t2 = 1.5, t5 = 3.5 (dashed line in the ﬁgure), respectively. Notice the
parabolic velocity proﬁle and the linear acceleration proﬁle. Further, for the
second pair, larger values of acceleration are obtained, since the relative time
instants are closer to those of the two intermediate points.
Interpolating linear polynomials with parabolic blends
A simpliﬁcation in trajectory planning can be achieved as follows. Consider
the case when it is desired to interpolate N path points q1 , . . . , qN at time

176

4 Trajectory Planning
pos

[rad]

6
4
2
0
0

1

2

[s]

3

4

5

3

4

5

3

4

5

vel

[rad/s]

5

0

−5
0

1

2

[s]
acc

30
20
[rad/s^2]

10
0
−10
−20
−30
0

1

2

[s]

Fig. 4.7. Time history of position, velocity and acceleration with a timing law of
cubic splines for two diﬀerent pairs of virtual points

instants t1 , . . . , tN with linear segments. To avoid discontinuity problems on
the ﬁrst derivative at the time instants tk , the function q(t) must have a
parabolic proﬁle (blend ) around tk ; as a consequence, the entire trajectory is
composed of a sequence of linear and quadratic polynomials, which in turn
implies that a discontinuity on q̈(t) is tolerated.
Then let Δtk = tk+1 − tk be the time distance between qk and qk+1 , and
Δtk,k+1 be the time interval during which the trajectory interpolating qk and
qk+1 is a linear function of time. Also let q̇k,k+1 be the constant velocity and
q̈k be the acceleration in the parabolic blend whose duration is Δtk . The
resulting trajectory is illustrated in Fig. 4.8. The values of qk , Δtk , and Δtk

4.2 Joint Space Trajectories

177

Fig. 4.8. Characterization of a trajectory with interpolating linear polynomials with
parabolic blends

are assumed to be given. Velocity and acceleration for the intermediate points
are computed as
qk − qk−1
Δtk−1
q̇k,k+1 − q̇k−1,k
;
q̈k =
Δtk

q̇k−1,k =

(4.29)
(4.30)

these equations are straightforward.
The ﬁrst and last segments deserve special care. In fact, if it is desired to
maintain the coincidence of the trajectory with the ﬁrst and last segments,
at least for a portion of time, the resulting trajectory has a longer duration
given by tN − t1 + (Δt1 + ΔtN )/2, where q̇0,1 = q̇N,N +1 = 0 has been imposed
for computing initial and ﬁnal accelerations.
Notice that q(t) reaches none of the path points qk but passes nearby
(Fig. 4.8). In this situation, the path points are more appropriately termed
via points; the larger the blending acceleration, the closer the passage to a via
point.
On the basis of the given qk , Δtk and Δtk , the values of q̇k−1,k and q̈k
are computed via (4.29), (4.30) and a sequence of linear polynomials with
parabolic blends is generated. Their expressions as a function of time are not
derived here to avoid further loading of the analytic presentation.
Figure 4.9 illustrates the time history of position, velocity and acceleration
obtained with the data: q1 = 0, q2 = 2π, q3 = π/2, q4 = π, t1 = 0, t2 = 2,
t3 = 3, t4 = 5, q̇1 = 0, q̇4 = 0. Two diﬀerent values for the blend times have
been considered: Δtk = 0.2 (solid line in the ﬁgure) and Δtk = 0.6 (dashed
line in the ﬁgure), for k = 1, . . . , 4, respectively. Notice that in the ﬁrst case
the passage of q(t) is closer to the via points, though at the expense of higher
acceleration values.

178

4 Trajectory Planning
pos

[rad]

6
4
2
0
0

1

2

[s]

3

4

5

3

4

5

3

4

5

vel
4

[rad/s]

2
0
−2
−4
−6

0

1

2

[s]
acc

40

[rad/s^2]

20
0
−20
−40
0

1

2

[s]

Fig. 4.9. Time history of position, velocity and acceleration with a timing law of
interpolating linear polynomials with parabolic blends

The technique presented above turns out to be an application of the trapezoidal velocity proﬁle law to the interpolation problem. If one gives up a trajectory passing near a via point at a prescribed instant of time, the use of
trapezoidal velocity proﬁles allows the development of a trajectory planning
algorithm which is attractive for its simplicity.
In particular, consider the case of one intermediate point only, and suppose
that trapezoidal velocity proﬁles are considered as motion primitives with
the possibility to specify the initial and ﬁnal point and the duration of the
motion only; it is assumed that q̇i = q̇f = 0. If two segments with trapezoidal
velocity proﬁles were generated, the manipulator joint would certainly reach

4.3 Operational Space Trajectories

179

the intermediate point, but it would be forced to stop there, before continuing
the motion towards the ﬁnal point. A keen alternative is to start generating
the second segment ahead of time with respect to the end of the ﬁrst segment,
using the sum of velocities (or positions) as a reference. In this way, the joint
is guaranteed to reach the ﬁnal position; crossing of the intermediate point at
the speciﬁed instant of time is not guaranteed, though.
Figure 4.10 illustrates the time history of position, velocity and acceleration obtained with the data: qi = 0, qf = 3π/2, ti = 0, tf = 2. The intermediate point is located at q = π with t = 1, the maximum acceleration values
in the two segments are respectively |q̈c | = 6π and |q̈c | = 3π, and the time
anticipation is 0.18. As predicted, with time anticipation, the assigned intermediate position becomes a via point with the advantage of an overall shorter
time duration. Notice, also, that velocity does not vanish at the intermediate
point.

4.3 Operational Space Trajectories
A joint space trajectory planning algorithm generates a time sequence of values for the joint variables q(t) so that the manipulator is taken from the
initial to the ﬁnal conﬁguration, eventually by moving through a sequence of
intermediate conﬁgurations. The resulting end-eﬀector motion is not easily
predictable, in view of the nonlinear eﬀects introduced by direct kinematics.
Whenever it is desired that the end-eﬀector motion follows a geometrically
speciﬁed path in the operational space, it is necessary to plan trajectory execution directly in the same space. Planning can be done either by interpolating
a sequence of prescribed path points or by generating the analytical motion
primitive and the relative trajectory in a punctual way.
In both cases, the time sequence of the values attained by the operational
space variables is utilized in real time to obtain the corresponding sequence
of values of the joint space variables, via an inverse kinematics algorithm. In
this regard, the computational complexity induced by trajectory generation
in the operational space and related kinematic inversion sets an upper limit
on the maximum sampling rate to generate the above sequences. Since these
sequences constitute the reference inputs to the motion control system, a
linear microinterpolation is typically carried out. In this way, the frequency
at which reference inputs are updated is increased so as to enhance dynamic
performance of the system.
Whenever the path is not to be followed exactly, its characterization can
be performed through the assignment of N points specifying the values of the
variables xe chosen to describe the end-eﬀector pose in the operational space
at given time instants tk , for k = 1, . . . , N . Similar to what was presented
in the above sections, the trajectory is generated by determining a smooth
interpolating vector function between the various path points. Such a function

180

4 Trajectory Planning
pos
5

[rad]

4
3
2
1
0
0

0.5

1
[s]

1.5

2

1.5

2

1.5

2

vel
4

[rad/s]

3
2
1
0
0

0.5

1
[s]
acc

[rad/s^2]

20
10
0
−10
−20
0

0.5

1
[s]

Fig. 4.10. Time history of position, velocity and acceleration with a timing law of
interpolating linear polynomials with parabolic blends obtained by anticipating the
generation of the second segment of trajectory

can be computed by applying to each component of xe any of the interpolation
techniques illustrated in Sect. 4.2.2 for the single joint variable.
Therefore, for given path (or via) points xe (tk ), the corresponding components xei (tk ), for i = 1, . . . r (where r is the dimension of the operational
space of interest) can be interpolated with a sequence of cubic polynomials, a
sequence of linear polynomials with parabolic blends, and so on.
On the other hand, if the end-eﬀector motion has to follow a prescribed
trajectory of motion, this must be expressed analytically. It is then necessary

4.3 Operational Space Trajectories

181

to refer to motion primitives deﬁning the geometric features of the path and
time primitives deﬁning the timing law on the path itself.
4.3.1 Path Primitives
For the deﬁnition of path primitives it is convenient to refer to the parametric
description of paths in space. Then let p be a (3 × 1) vector and f (σ) a continuous vector function deﬁned in the interval [σi , σf ]. Consider the equation
p = f (σ);

(4.31)

with reference to its geometric description, the sequence of values of p with
σ varying in [σi , σf ] is termed path in space. The equation in (4.31) deﬁnes
the parametric representation of the path Γ and the scalar σ is called parameter. As σ increases, the point p moves on the path in a given direction.
This direction is said to be the direction induced on Γ by the parametric
representation (4.31). A path is closed when p(σf ) = p(σi ); otherwise it is
open.
Let pi be a point on the open path Γ on which a direction has been ﬁxed.
The arc length s of the generic point p is the length of the arc of Γ with
extremes p and pi if p follows pi , the opposite of this length if p precedes pi .
The point pi is said to be the origin of the arc length (s = 0).
From the above presentation it follows that to each value of s a welldetermined path point corresponds, and then the arc length can be used as a
parameter in a diﬀerent parametric representation of the path Γ :
p = f (s);

(4.32)

the range of variation of the parameter s will be the sequence of arc lengths
associated with the points of Γ .
Consider a path Γ represented by (4.32). Let p be a point corresponding
to the arc length s. Except for special cases, p allows the deﬁnition of three
unit vectors characterizing the path. The orientation of such vectors depends
exclusively on the path geometry, while their direction depends also on the
direction induced by (4.32) on the path.
The ﬁrst of such unit vectors is the tangent unit vector denoted by t. This
vector is oriented along the direction induced on the path by s.
The second unit vector is the normal unit vector denoted by n. This vector
is oriented along the line intersecting p at a right angle with t and lies in the
so-called osculating plane O (Fig. 4.11); such plane is the limit position of the
plane containing the unit vector t and a point p ∈ Γ when p tends to p along
the path. The direction of n is so that the path Γ , in the neighbourhood of p
with respect to the plane containing t and normal to n, lies on the same side
of n.
The third unit vector is the binormal unit vector denoted by b. This vector
is so that the frame (t, n, b) is right-handed (Fig. 4.11). Notice that it is not
always possible to deﬁne uniquely such a frame.

182

4 Trajectory Planning

Fig. 4.11. Parametric representation of a path in space

It can be shown that the above three unit vectors are related by simple
relations to the path representation Γ as a function of the arc length. In
particular, it is
t=

dp
ds

1 d2 p
n= ( 2 ( 2
( d p ( ds
(
(
( ds2 (
b = t × n.

(4.33)

Typical path parametric representations are reported below which are useful
for trajectory generation in the operational space.
Rectilinear path
Consider the linear segment connecting point pi to point pf . The parametric
representation of this path is
s
(p − pi ).
(4.34)
p(s) = pi +
pf − pi  f
Notice that p(0) = pi and p(pf − pi ) = pf . Hence, the direction induced
on Γ by the parametric representation (4.34) is that going from pi to pf .
Diﬀerentiating (4.34) with respect to s gives
1
dp
=
(p − pi )
ds
pf − pi  f
d2 p
= 0.
ds2
In this case it is not possible to deﬁne the frame (t, n, b) uniquely.

(4.35)
(4.36)

4.3 Operational Space Trajectories

183

Fig. 4.12. Parametric representation of a circle in space

Circular path
Consider a circle Γ in space. Before deriving its parametric representation, it
is necessary to introduce its signiﬁcant parameters. Suppose that the circle is
speciﬁed by assigning (Fig. 4.12):
• the unit vector of the circle axis r,
• the position vector d of a point along the circle axis,
• the position vector pi of a point on the circle.
With these parameters, the position vector c of the centre of the circle can
be found. Let δ = pi − d; for pi not to be on the axis, i.e., for the circle not
to degenerate into a point, it must be
|δ T r| < δ;
in this case it is
c = d + (δ T r)r.

(4.37)

It is now desired to ﬁnd a parametric representation of the circle as a function
of the arc length. Notice that this representation is very simple for a suitable
choice of the reference frame. To see this, consider the frame O –x y  z  , where
O coincides with the centre of the circle, axis x is oriented along the direction
of the vector pi − c, axis z  is oriented along r and axis y  is chosen so as to
complete a right-handed frame. When expressed in this reference frame, the
parametric representation of the circle is
⎡
⎤
ρ cos (s/ρ)
(4.38)
p (s) = ⎣ ρ sin (s/ρ) ⎦ ,
0

184

4 Trajectory Planning

where ρ = pi − c is the radius of the circle and the point pi has been
assumed as the origin of the arc length. For a diﬀerent reference frame, the
path representation becomes
p(s) = c + Rp (s),

(4.39)

where c is expressed in the frame O–xyz and R is the rotation matrix of
frame O – x y  z  with respect to frame O–xyz which, in view of (2.3), can be
written as
R = [ x y  z  ];
x , y  , z  indicate the unit vectors of the frame expressed in the frame O–xyz.
Diﬀerentiating (4.39) with respect to s gives
⎡
⎤
−sin (s/ρ)
dp
= R ⎣ cos (s/ρ) ⎦
(4.40)
ds
0
⎡
⎤
−cos (s/ρ)/ρ
d2 p
= R ⎣ −sin (s/ρ)/ρ ⎦ .
(4.41)
ds2
0
4.3.2 Position
Let xe be the vector of operational space variables expressing the pose of
the manipulator’s end-eﬀector as in (2.80). Generating a trajectory in the
operational space means to determine a function xe (t) taking the end-eﬀector
frame from the initial to the ﬁnal pose in a time tf along a given path with a
speciﬁc motion timing law. First, consider end-eﬀector position. Orientation
will follow.
Let pe = f (s) be the (3 × 1) vector of the parametric representation of the
path Γ as a function of the arc length s; the origin of the end-eﬀector frame
moves from pi to pf in a time tf . For simplicity, suppose that the origin of
the arc length is at pi and the direction induced on Γ is that going from pi
to pf . The arc length then goes from the value s = 0 at t = 0 to the value
s = sf (path length) at t = tf . The timing law along the path is described by
the function s(t).
In order to ﬁnd an analytic expression for s(t), any of the above techniques
for joint trajectory generation can be employed. In particular, either a cubic
polynomial or a sequence of linear segments with parabolic blends can be
chosen for s(t).
It is worth making some remarks on the time evolution of pe on Γ , for a
given timing law s(t). The velocity of point pe is given by the time derivative
of pe
dp
ṗe = ṡ e = ṡt,
ds

4.3 Operational Space Trajectories

185

where t is the tangent vector to the path at point p in (4.33). Then, ṡ represents the magnitude of the velocity vector relative to point p, taken with
the positive or negative sign depending on the direction of ṗ along t. The
magnitude of ṗ starts from zero at t = 0, then it varies with a parabolic or
trapezoidal proﬁle as per either of the above choices for s(t), and ﬁnally it
returns to zero at t = tf .
As a ﬁrst example, consider the segment connecting point pi with point pf .
The parametric representation of this path is given by (4.34). Velocity and acceleration of pe can be easily computed by recalling the rule of diﬀerentiation
of compound functions, i.e.,
ṡ
(p − pi ) = ṡt
pf − pi  f
s̈
p̈e =
(p − pi ) = s̈t.
pf − pi  f

ṗe =

(4.42)
(4.43)

As a further example, consider a circle Γ in space. From the parametric
representation derived above, in view of (4.40), (4.41), velocity and acceleration of point pe on the circle are
⎡
⎤
−ṡ sin (s/ρ)
ṗe = R ⎣ ṡ cos (s/ρ) ⎦
(4.44)
0
⎡ 2
⎤
−ṡ cos (s/ρ)/ρ − s̈ sin (s/ρ)
p̈e = R ⎣ −ṡ2 sin (s/ρ)/ρ + s̈ cos (s/ρ) ⎦ .
(4.45)
0
Notice that the velocity vector is aligned with t, and the acceleration vector
is given by two contributions: the ﬁrst is aligned with n and represents the
centripetal acceleration, while the second is aligned with t and represents the
tangential acceleration.
Finally, consider the path consisting of a sequence of N + 1 points,
p0 , p1 , . . . , pN , connected by N segments. A feasible parametric representation of the overall path is the following:
pe = p0 +

N

j=1

sj
(p − pj−1 ),
pj − pj−1  j

(4.46)

with j = 1, . . . , N . In (4.46) sj is the arc length associated with the j-th
segment of the path, connecting point pj−1 to point pj , deﬁned as
⎧
0 ≤ t ≤ tj−1
⎪
⎨0

tj−1 < t < tj
(4.47)
sj (t) = sj (t)
⎪
⎩ p − p 
t
≤
t
≤
t
,
j
f
j
j−1

186

4 Trajectory Planning

where t0 = 0 and tN = tf are respectively the initial and ﬁnal time instants of
the trajectory, tj is the time instant corresponding to point pj and sj (t) can
be an analytical function of cubic polynomial type, linear type with parabolic
blends, and so forth, which varies continuously from the value sj = 0 at
t = tj−1 to the value sj = pj − pj−1  at t = tj .
The velocity and acceleration of pe can be easily found by diﬀerentiating (4.46):
ṗe =

N

j=1

p̈e =

N

j=1


ṡj
(pj − pj−1 ) =
ṡj tj
pj − pj−1 
j=1

(4.48)


s̈j
(pj − pj−1 ) =
s̈j tj ,
pj − pj−1 
j=1

(4.49)

N

N

where tj is the tangent unit vector of the j-th segment.
Because of the discontinuity of the ﬁrst derivative at the path points between two non-aligned segments, the manipulator will have to stop and then
go along the direction of the following segment. Assumed a relaxation of the
constraint to pass through the path points, it is possible to avoid a manipulator stop by connecting the segments near the above points, which will then
be named operational space via points so as to guarantee, at least, continuity
of the ﬁrst derivative.
As already illustrated for planning of interpolating linear polynomials with
parabolic blends passing by the via points in the joint space, the use of trapezoidal velocity proﬁles for the arc lengths allows the development of a rather
simple planning algorithm
In detail, it will be suﬃcient to properly anticipate the generation of the
single segments, before the preceding segment has been completed. This leads
to modifying (4.47) as follows:
⎧
0 ≤ t ≤ tj−1 − Δtj
⎪
⎨0

tj−1 − Δtj < t < tj − Δtj
(4.50)
sj (t) = sj (t + Δtj )
⎪
⎩ p − p 
tj − Δtj ≤ t ≤ tf − ΔtN ,
j
j−1
where Δtj is the time advance at which the j-th segment is generated, which
can be recursively evaluated as
Δtj = Δtj−1 + δtj ,
with j = 1, . . . , N e Δt0 = 0. Notice that this time advance is given by the
sum of two contributions: the former, Δtj−1 , accounts for the sum of the time
advances at which the preceding segments have been generated, while the
latter, δtj , is the time advance at which the generation of the current segment
starts.

4.3 Operational Space Trajectories

187

4.3.3 Orientation
Consider now end-eﬀector orientation. Typically, this is speciﬁed in terms of
the rotation matrix of the (time-varying) end-eﬀector frame with respect to
the base frame. As is well known, the three columns of the rotation matrix
represent the three unit vectors of the end-eﬀector frame with respect to the
base frame. To generate a trajectory, however, a linear interpolation on the
unit vectors ne , se , ae describing the initial and ﬁnal orientation does not
guarantee orthonormality of the above vectors at each instant of time.
Euler angles
In view of the above diﬃculty, for trajectory generation purposes, orientation
is often described in terms of the Euler angles triplet φe = (ϕ, ϑ, ψ) for which
a timing law can be speciﬁed. Usually, φe moves along the segment connecting
its initial value φi to its ﬁnal value φf . Also in this case, it is convenient to
choose a cubic polynomial or a linear segment with parabolic blends timing
law. In this way, in fact, the angular velocity ω e of the time-varying frame,
which is related to φ̇e by the linear relationship (3.64), will have continuous
magnitude.
Therefore, for given φi and φf and timing law, the position, velocity and
acceleration proﬁles are
s
(φ − φi )
φf − φi  f
ṡ
(φ − φi )
φ̇e =
φf − φi  f
s̈
(φ − φi );
φ̈e =
φf − φi  f

φe = φi +

(4.51)

where the timing law for s(t) has to be speciﬁed. The three unit vectors of the
end-eﬀector frame can be computed — with reference to Euler angles ZYZ
— as in (2.18), the end-eﬀector frame angular velocity as in (3.64), and the
angular acceleration by diﬀerentiating (3.64) itself.
Angle and axis
An alternative way to generate a trajectory for orientation of clearer interpretation in the Cartesian space can be derived by resorting to the the angle
and axis description presented in Sect. 2.5. Given two coordinate frames in
the Cartesian space with the same origin and diﬀerent orientation, it is always
possible to determine a unit vector so that the second frame can be obtained
from the ﬁrst frame by a rotation of a proper angle about the axis of such
unit vector.

188

4 Trajectory Planning

Let Ri and Rf denote respectively the rotation matrices of the initial
frame Oi –xi yi zi and the ﬁnal frame Of –xf yf zf , both with respect to the
base frame. The rotation matrix between the two frames can be computed by
recalling that Rf = Ri Rif ; the expression in (2.5) leads to
⎤
⎡
r11 r12 r13
Rif = RTi Rf = ⎣ r21 r22 r23 ⎦ .
r31 r32 r33
If the matrix Ri (t) is deﬁned to describe the transition from Ri to Rf , it
must be Ri (0) = I and Ri (tf ) = Rif . Hence, the matrix Rif can be expressed
as the rotation matrix about a ﬁxed axis in space; the unit vector r i of the
axis and the angle of rotation ϑf can be computed by using (2.27):


r11 + r22 + r33 − 1
ϑf = cos −1
(4.52)
2
⎡
⎤
r32 − r23
1
⎣ r13 − r31 ⎦
(4.53)
r=
2 sin ϑf
r21 − r12
for sin ϑf = 0.
The matrix Ri (t) can be interpreted as a matrix Ri (ϑ(t), r i ) and computed
via (2.25); it is then suﬃcient to assign a timing law to ϑ, of the type of those
presented for the single joint with ϑ(0) = 0 and ϑ(tf ) = ϑf , and compute the
components of r i from (4.52). Since r i is constant, the resulting velocity and
acceleration are respectively
ω i = ϑ̇ r i
ω̇ i = ϑ̈ r i .

(4.54)
(4.55)

Finally, in order to characterize the end-eﬀector orientation trajectory with
respect to the base frame, the following transformations are needed:
Re (t) = Ri Ri (t)
ω e (t) = Ri ω i (t)
ω̇ e (t) = Ri ω̇ i (t).
Once a path and a trajectory have been speciﬁed in the operational space
in terms of pe (t) and φe (t) or Re (t), inverse kinematics techniques can be
used to ﬁnd the corresponding trajectories in the joint space q(t).

Bibliography
Trajectory planning for robot manipulators has been addressed since the ﬁrst
works in the ﬁeld of robotics [178]. The formulation of the interpolation problem of the path points by means of diﬀerent classes of functions has been
suggested in [26].

Problems

189

The generation of motion trajectories through sequences of points in the
joint space using splines is due to [131]. Alternative formulations for this
problem are found in [56]. For a complete treatment of splines, including
geometric properties and computational aspects, see [54]. In [155] a survey
on the functions employed for trajectory planning of a single motion axis
is given, which accounts for performance indices and eﬀects of unmodelled
ﬂexible dynamics.
Cartesian space trajectory planning and the associated motion control
problem have been originally treated in [179]. The systematic management
of the motion by the via points using interpolating linear polynomials with
parabolic blends has been proposed in [229]. A detailed presentation of the
general aspects of the geometric primitives that can be utilized in robotics to
deﬁne Cartesian space paths can be found in the computer graphics text [73].

Problems
4.1. Compute the joint trajectory from q(0) = 1 to q(2) = 4 with null initial
and ﬁnal velocities and accelerations.
4.2. Compute the timing law q(t) for a joint trajectory with velocity proﬁle
of the type q̇(t) = k(1 − cos (at)) from q(0) = 0 to q(2) = 3.
4.3. Given the values for the joint variable: q(0) = 0, q(2) = 2, and q(4) = 3,
compute the two ﬁfth-order interpolating polynomials with continuous velocities and accelerations.
4.4. Show that the matrix A in (4.28) has a tridiagonal band structure.
4.5. Given the values for the joint variable: q(0) = 0, q(2) = 2, and q(4) = 3,
compute the cubic interpolating spline with null initial and ﬁnal velocities and
accelerations.
4.6. Given the values for the joint variable: q(0) = 0, q(2) = 2, and q(4) = 3,
ﬁnd the interpolating polynomial with linear segments and parabolic blends
with null initial and ﬁnal velocities.
4.7. Find the timing law p(t) for a Cartesian space rectilinear path with trapezoidal velocity proﬁle from p(0) = [ 0 0.5 0 ]T to p(2) = [ 1 −0.5 0 ]T .
4.8. Find the timing law p(t) for a Cartesian space circular path with trapezoidal velocity proﬁle from p(0) = [ 0 0.5 1 ]T to p(2) = [ 0 −0.5 1 ]T ;
the circle is located in the plane x = 0 with centre at c = [ 0 0 1 ]T and
radius ρ = 0.5, and is executed clockwise for an observer aligned with x.

5
Actuators and Sensors

In this chapter, two basic robot components are treated: actuators and sensors. In the ﬁrst part, the features of an actuating system are presented in
terms of the power supply, power ampliﬁer, servomotor and transmission. In
view of their control versatility, two types of servomotors are used, namely,
electric servomotors for actuating the joints of small and medium size manipulators, and hydraulic servomotors for actuating the joints of large size
manipulators. The models describing the input/output relationship for such
servomotors are derived, together with the control schemes of the drives. The
electric servomotors are also employed to actuate the wheels of the mobile
robots, which will be dealt with in Chap. 11. Successively, proprioceptive sensors are presented which allow measurement of the quantities characterizing
the internal state of the manipulator, namely, encoders and resolvers for joint
position measurement, tachometers for joint velocity measurement; further,
exteroceptive sensors are presented including force sensors for end-eﬀector
force measurement, distance sensors for detection of objects in the workspace,
and vision sensors for the measurement of the characteristic parameters of
such objects, whenever the manipulator interacts with the environment.

5.1 Joint Actuating System
The motion imposed to a manipulator’s joint is realized by an actuating system
which in general consists of:
•
•
•
•

a
a
a
a

power supply,
power ampliﬁer ,
servomotor ,
transmission.

The connection between the various components is illustrated in Fig. 5.1
where the exchanged powers are shown. To this end, recall that power can

192

5 Actuators and Sensors

Fig. 5.1. Components of a joint actuating system

always be expressed as the product of a ﬂow and a force quantity, whose
physical context allows the speciﬁcation of the nature of the power (mechanical, electric, hydraulic, or pneumatic).
In terms of a global input/output relationship, Pc denotes the (usually
electric) power associated with the control law signal, whereas Pu represents
the mechanical power required to the joint to actuate the motion. The intermediate connections characterize the supply power Pa of the motor (of
electric, hydraulic, or pneumatic type), the power provided by the primary
source Pp of the same physical nature as that of Pa , and the mechanical power
Pm developed by the motor. Moreover, Pda , Pds and Pdt denote the powers
lost for dissipation in the conversions performed respectively by the ampliﬁer,
motor and transmission.
To choose the components of an actuating system, it is worth starting
from the requirements imposed on the mechanical power Pu by the force and
velocity that describe the joint motion.
5.1.1 Transmissions
The execution of joint motions of a manipulator demands low speeds with
high torques. In general, such requirements do not allow an eﬀective use of the
mechanical features of servomotors, which typically provide high speeds with
low torques in optimal operating conditions. It is then necessary to interpose
a transmission (gear ) to optimize the transfer of mechanical power from the
motor (Pm ) to the joint (Pu ). During this transfer, the power Pdt is dissipated
as a result of friction.
The choice of the transmission depends on the power requirements, the
kind of desired motion, and the allocation of the motor with respect to the
joint. In fact, the transmission allows the outputs of the motor to be transformed both quantitatively (velocity and torque) and qualitatively (a rotational motion about the motor axis into a translational motion of the joint).
Also, it allows the static and dynamic performance of a manipulator to be optimized, by reducing the eﬀective loads when the motor is located upstream

5.1 Joint Actuating System

193

of the joint; for instance, if some motors are mounted to the base of the robot,
the total weight of the manipulator is decreased and the power-to-weight ratio
is increased.
The following transmissions are typically used for industrial robots:
• Spur gears that modify the characteristics of the rotational motion of the
motor by changing the axis of rotation and/or by translating the application point; spur gears are usually constructed with wide cross-section
teeth and squat shafts.
• Lead screws that convert rotational motion of the motor into translational
motion, as needed for actuation of prismatic joints; in order to reduce friction, ball screws are usually employed that are preloaded so as to increase
stiﬀness and decrease backlash.
• Timing belts and chains which are equivalent from a kinematic viewpoint
and are employed to locate the motor remotely from the axis of the actuated joint. The stress on timing belts may cause strain, and then these
are used in applications requiring high speeds and low forces. On the other
hand, chains are used in applications requiring low speeds, since their large
mass may induce vibration at high speeds.
On the assumption of rigid transmissions with no backlash, the relationship between input forces (velocities) and output forces (velocities) is purely
proportional.
The mechanical features of the motor used for an actuating system may
sometimes allow a direct connection of the motor to the joint without the use
of any transmission element (direct drive). The drawbacks due to transmission elasticity and backlash are thus eliminated, although more sophisticated
control algorithms are required, since the absence of reduction gears does not
allow the nonlinear coupling terms in the dynamic model to be neglected.
The use of direct-drive actuating systems is not yet popular for industrial
manipulators, in view of the cost and size of the motors as well as of control
complexity.
5.1.2 Servomotors
Actuation of joint motions is entrusted to motors which allow the realization
of a desired motion for the mechanical system. Concerning the kind of input
power Pa , motors can be classiﬁed into three groups:
• Pneumatic motors which utilize the pneumatic energy provided by a compressor and transform it into mechanical energy by means of pistons or
turbines.
• Hydraulic motors which transform the hydraulic energy stored in a reservoir into mechanical energy by means of suitable pumps.
• Electric motors whose primary supply is the electric energy available from
the electric distribution system.

194

5 Actuators and Sensors

A portion of the input power Pa is converted to output as mechanical
power Pm , and the rest (Pds ) is dissipated because of mechanical, electric,
hydraulic, or pneumatic loss.
The motors employed in robotics are the evolution of the motors employed in industrial automation having powers ranging from about 10 W to
about 10 kW. For the typical performance required, such motors should have
the following requirements with respect to those employed in conventional
applications:
•
•
•
•
•
•

low inertia and high power-to-weight ratio,
possibility of overload and delivery of impulse torques,
capability to develop high accelerations,
wide velocity range (from 1 to 1000 revolutes/min),
high positioning accuracy (at least 1/1000 of a circle),
low torque ripple so as to guarantee continuous rotation even at low speed.

These requirements are enhanced by the good trajectory tracking and
positioning accuracy demanded for an actuating system for robots, and thus
the motor must play the role of a servomotor . In this respect, pneumatic
motors are diﬃcult to control accurately, in view of the unavoidable ﬂuid
compressibility errors. Therefore, they are not widely employed, if not for
the actuation of the typical opening and closing motions of the jaws in a
gripper tool, then for the actuation of simple arms used in applications where
continuous motion control is not of concern.
The most employed motors in robotics applications are electric servomotors. Among them, the most popular are permanent-magnet direct-current
(DC) servomotors and brushless DC servomotors, in view of their good control ﬂexibility.
The permanent-magnet DC servomotor consists of:
• A stator coil that generates magnetic ﬂux; this generator is always a permanent magnet made by ferromagnetic ceramics or rare earths (high ﬁelds
in contained space).
• An armature that includes the current-carrying winding that surrounds a
rotary ferromagnetic core (rotor).
• A commutator that provides an electric connection by means of brushes
between the rotating armature winding and the external feed winding,
according to a commutation logic determined by the rotor motion.
The brushless DC servomotor consists of:
• A rotating coil (rotor) that generates magnetic ﬂux; this generator is a
permanent magnet made by ferromagnetic ceramics or rare earths.
• A stationary armature (stator) made by a polyphase winding.
• A static commutator that, on the basis of the signals provided by a position sensor located on the motor shaft, generates the feed sequence of the
armature winding phases as a function of the rotor motion.

5.1 Joint Actuating System

195

With reference to the above details of constructions, a comparison between the operating principle of a permanent-magnet DC and a brushless DC
servomotor leads to the following considerations.
In the brushless DC motor, by means of the rotor position sensor, the
winding orthogonal to the magnetic ﬁeld of the coil is found; then, feeding the
winding makes the rotor rotate. As a consequence of rotation, the electronic
control module commutes the feeding on the winding of the various phases in
such a way that the resulting ﬁeld at the armature is always kept orthogonal to
that of the coil. As regards electromagnetic interaction, such a motor operates
in a way similar to that of a permanent-magnet DC motor where the brushes
are at an angle of π/2 with respect to the direction of the excitation ﬂux. In
fact, feeding the armature coil makes the rotor rotate, and commutation of
brushes from one plate of the commutator to the other allows the rotor to be
maintained in rotation. The role played by the brushes and commutator in
a permanent-magnet DC motor is analogous to that played by the position
sensor and electronic control module in a brushless DC motor.
The main reason for using a brushless DC motor is to eliminate the problems due to mechanical commutation of the brushes in a permanent-magnet
DC motor. In fact, the presence of the commutator limits the performance
of a permanent-magnet DC motor, since this provokes electric loss due to
voltage drops at the contact between the brushes and plates, and mechanical loss due to friction and arcing during commutation from one plate to the
next one caused by the inductance of the winding. The elimination of the
causes provoking such inconveniences, i.e., the brushes and plates, allows an
improvement of motor performance in terms of higher speeds and less material
wear.
The inversion between the functions of stator and rotor leads to further
advantages. The presence of a winding on the stator instead of the rotor facilitates heat disposal. The absence of a rotor winding, together with the possibility of using rare-earth permanent magnets, allows construction of more
compact rotors which are, in turn, characterized by a low moment of inertia. Therefore, the size of a brushless DC motor is smaller than that of a
permanent-magnet DC motor of the same power; an improvement of dynamic
performance can also be obtained by using a brushless DC motor. For the
choice of the most suitable servomotor for a speciﬁc application, the cost
factor plays a relevant role.
Not uncommon are also stepper motors. These actuators are controlled
by suitable excitation sequences and their operating principle does not require measurement of motor shaft angular position. The dynamic behaviour
of stepper motors is greatly inﬂuenced by payload, though. Also, they induce
vibration of the mechanical structure of the manipulator. Such inconveniences
conﬁne the use of stepper motors to the ﬁeld of micromanipulators, for which
low-cost implementation prevails over the need for high dynamic performance.
A certain number of applications features the employment of hydraulic
servomotors, which are based on the simple operating principle of volume

196

5 Actuators and Sensors

variation under the action of compressed ﬂuid. From a construction viewpoint,
they are characterized by one or more chambers made by pistons (cylinders
reciprocating in tubular housings). Linear servomotors have a limited range
and are constituted by a single piston. Rotary servomotors have unlimited
range and are constituted by several pistons (usually an odd number) with an
axial or radial disposition with respect to the motor axis of rotation. These
servomotors oﬀer a static and dynamic performance comparable with that
oﬀered by electric servomotors.
The diﬀerences between electric and hydraulic servomotors can be fundamentally observed from a plant viewpoint. In this respect, electric servomotors
present the following advantages:
•
•
•
•
•

widespread availability of power supply,
low cost and wide range of products,
high power conversion eﬃciency,
easy maintenance,
no pollution of working environment.
Instead, they present the following limitations:

• burnout problems at static situations caused by the eﬀect of gravity on
the manipulator; emergency brakes are then required,
• need for special protection when operating in ﬂammable environments.
Hydraulic servomotors present the following drawbacks:
•
•
•
•
•

need for a hydraulic power station,
high cost, narrow range of products, and diﬃculty of miniaturization,
low power conversion eﬃciency,
need for operational maintenance,
pollution of working environment due to oil leakage.
In their favour it is worth pointing out that they:

•
•
•
•

do not suﬀer from burnout in static situations,
are self-lubricated and the circulating ﬂuid facilitates heat disposal,
are inherently safe in harmful environments,
have excellent power-to-weight ratios.
From an operational viewpoint, it can be observed that:

• Both types of servomotors have a good dynamic behaviour, although the
electric servomotor has greater control ﬂexibility. The dynamic behaviour
of a hydraulic servomotor depends on the temperature of the compressed
ﬂuid.
• The electric servomotor is typically characterized by high speeds and low
torques, and as such it requires the use of gear transmissions (causing
elasticity and backlash). On the other hand, the hydraulic servomotor is
capable of generating high torques at low speeds.

5.1 Joint Actuating System

197

In view of the above remarks, hydraulic servomotors are speciﬁcally employed for manipulators that have to carry heavy payloads; in this case, not
only is the hydraulic servomotor the most suitable actuator, but also the cost
of the plant accounts for a reduced percentage on the total cost of the manipulation system.
5.1.3 Power Ampliﬁers
The power ampliﬁer has the task of modulating, under the action of a control
signal, the power ﬂow which is provided by the primary supply and has to be
delivered to the actuators for the execution of the desired motion. In other
words, the ampliﬁer takes a fraction of the power available at the source which
is proportional to the control signal; then it transmits this power to the motor
in terms of suitable force and ﬂow quantities.
The inputs to the ampliﬁer are the power taken from the primary source
Pp and the power associated with the control signal Pc . The total power is
partly delivered to the actuator (Pa ) and partly lost in dissipation (Pda ).
Given the typical use of electric and hydraulic servomotors, the operational
principles of the respective ampliﬁers are discussed.
To control an electric servomotor , it is necessary to provide it with a
voltage or current of suitable form depending on the kind of servomotor employed. Voltage (or current) is direct for permanent-magnet DC servomotors,
while it is alternating for brushless DC servomotors. The value of voltage for
permanent-magnet DC servomotors or the values of voltage and frequency for
brushless DC servomotors are determined by the control signal of the ampliﬁer, so as to make the motor execute the desired motion.
For the power ranges typically required by joint motions (of the order
of a few kilowatts), transistor ampliﬁers are employed which are suitably
switched by using pulse-width modulation (PWM) techniques. They allow the
achievement of a power conversion eﬃciency Pa /(Pp + Pc ) greater than 0.9
and a power gain Pa /Pc of the order of 106 . The ampliﬁers employed to control permanent-magnet DC servomotors are DC-to-DC converters (choppers),
whereas those employed to control brushless DC servomotors are DC-to-AC
converters (inverters).
Control of a hydraulic servomotor is performed by varying the ﬂow rate of
the compressed ﬂuid delivered to the motor. The task of modulating the ﬂow
rate is typically entrusted to an interface (electro-hydraulic servovalve). This
allows a relationship to be established between the electric control signal and
the position of a distributor which is able to vary the ﬂow rate of the ﬂuid
transferred from the primary source to the motor. The electric control signal
is usually current-ampliﬁed and feeds a solenoid which moves (directly or indirectly) the distributor, whose position is measured by a suitable transducer.
In this way, a position servo on the valve stem is obtained which reduces
occurrence of any stability problem that may arise on motor control. The
magnitude of the control signal determines the ﬂow rate of the compressed

198

5 Actuators and Sensors

ﬂuid through the distributor, according to a characteristic which is possibly
made linear by means of a keen mechanical design.
5.1.4 Power Supply
The task of the power supply is to supply the primary power to the ampliﬁer
which is needed for operation of the actuating system.
In the case of electric servomotors, the power supply consists of a transformer and a typically uncontrolled bridge rectiﬁer. These allow the alternating voltage available from the distribution to be converted into a direct
voltage of suitable magnitude which is required to feed the power ampliﬁer.
In the case of hydraulic servomotors, the power supply is obviously more
complex. In fact, a gear or piston pump is employed to compress the ﬂuid
which is driven by a primary motor operating at constant speed, typically a
three-phase nonsynchronous motor. To reduce the unavoidable pressure oscillations provoked by a ﬂow rate demand depending on operational conditions
of the motor, a reservoir is interfaced to store hydraulic energy. Such a reservoir, in turn, plays the same role as the ﬁlter capacitor used at the output of a
bridge rectiﬁer. The hydraulic power station is completed by the use of various
components (ﬁlters, pressure valves, and check valves) that ensure proper operation of the system. Finally, it can be inferred how the presence of complex
hydraulic circuits operating at high pressures (of the order of 100 atm) causes
an appreciable pollution of the working environment.

5.2 Drives
This section presents the operation of the electric drives and the hydraulic
drives for the actuation of a manipulator’s joints. Starting from the mathematical models describing the dynamic behaviour, the block schemes are
derived which allow an emphasis on the control features and the eﬀects of the
use of a mechanical transmission.
5.2.1 Electric Drives
From a modelling viewpoint, a permanent-magnet DC motor and a brushless
DC motor provided with the commutation module and position sensor can be
described by the same diﬀerential equations. In the domain of the complex
variable s, the electric balance of the armature is described by the equations
Va = (Ra + sLa )Ia + Vg
Vg = kv Ωm

(5.1)
(5.2)

where Va and Ia respectively denote armature voltage and current, Ra and
La are respectively the armature resistance and inductance, and Vg denotes

5.2 Drives

199

Fig. 5.2. Block scheme of an electric drive

the back electromotive force which is proportional to the angular velocity Ωm
through the voltage constant kv that depends on the construction details of
the motor as well as on the magnetic ﬂux of the coil.
The mechanical balance is described by the equations
Cm = (sIm + Fm )Ωm + Cl
Cm = kt Ia

(5.3)
(5.4)

where Cm and Cl respectively denote the driving torque and load reaction
torque, Im and Fm are respectively the moment of inertia and viscous friction
coeﬃcient at the motor shaft, and the torque constant kt is numerically equal
to kv in the SI unit system for a compensated motor.
Concerning the power ampliﬁer, the input/output relationship between
the control voltage Vc and the armature voltage Va is given by the transfer
function
Va
Gv
=
(5.5)
Vc
1 + sTv
where Gv denotes the voltage gain and Tv is a time constant that can be
neglected with respect to the other time constants of the system. In fact, by
using a modulation frequency in the range of 10 to 100 kHz, the time constant
of the ampliﬁer is in the range of 10−5 to 10−4 ) s.
The block scheme of the servomotor with power ampliﬁer (electric drive)
is illustrated in Fig. 5.2. In such a scheme, besides the blocks corresponding to
the above relations, there is an armature current feedback loop where current
is thought of as measured by a transducer ki between the power ampliﬁer and
the armature winding of the motor. Further, the scheme features a current
regulator Ci (s) as well as an element with a nonlinear saturation characteristic. The aim of such feedback is twofold. On one hand, the voltage Vc plays
the role of a current reference and thus, by means of a suitable choice of the
regulator Ci (s), the lag between the current Ia and the voltage Vc can be
reduced with respect to the lag between Ia and Vc . On the other hand, the
introduction of a saturation nonlinearity allows the limitation of the magni-

200

5 Actuators and Sensors

Fig. 5.3. Block scheme of an electric drive as a velocity-controlled generator

tude of Vc , and then it works like a current limit which ensures protection of
the power ampliﬁer whenever abnormal operating conditions occur.
The choice of the regulator Ci (s) of the current loop allows a velocitycontrolled or torque-controlled behaviour to be obtained from the electric
drive, depending on the values attained by the loop gain. In fact, in the case
of ki = 0, recalling that the mechanical viscous friction coeﬃcient is negligible
with respect to the electrical friction coeﬃcient
Fm 

kv kt
,
Ra

(5.6)

assuming a unit gain constant for Ci (s)1 and Cl = 0 yields
ωm ≈

Gv 
v
kv c

(5.7)

and thus the drive system behaves like a velocity-controlled generator .
Instead, when ki = 0, choosing a large loop gain for the current loop
Ra ) leads at steady state to
(Kki


kt
kv
ωm ;
vc −
(5.8)
cm ≈
ki
Gv
the drive behaves like a torque-controlled generator since, in view of the large
value of Gv , the driving torque is practically independent of the angular velocity.
As regards the dynamic behaviour, it is worth considering a reduced-order
model which can be obtained by neglecting the electric time constant La /Ra
with respect to the mechanical time constant Im /Fm , assuming Tv ≈ 0 and
a purely proportional controller. These assumptions, together with ki = 0,
lead to the block scheme in Fig. 5.3 for the velocity-controlled generator. On
Ra and kv Ω/Kki ≈ 0, the resulting
the other hand, if it is assumed Kki
block scheme of the torque-controlled generator is that in Fig. 5.4. From the

1

It is assumed Ci (0) = 1; in the case of presence of an integral action in Ci (s), it
should be lims→0 sC(s) = 1.

5.2 Drives

201

Fig. 5.4. Block scheme of an electric drive as a torque-controlled generator

above schemes, the following input/output relations between control voltage,
reaction torque, and angular velocity can be derived:

Ωm

1
Ra
kv
kv kt

=
G V −
C
Ra Im v c
Ra Im l
1+s
1+s
kv kt
kv kt

(5.9)

for the velocity-controlled generator, and

Ωm

kt
1
ki Fm
F
m
=
V−
C
Im c
Im l
1+s
1+s
Fm
Fm

(5.10)

for the torque-controlled generator. These transfer functions show how, without current feedback, the system has a better rejection of disturbance torques
in terms of both equivalent gain (Ra /kv kt  1/Fm ) and time response
(Ra Im /kv kt  Im /Fm ).
The relationship between the control input and the actuator position output can be expressed in a uniﬁed manner by the transfer function
M (s) =
where
km =

1
kv

km
s(1 + sTm )

Tm =

Ra Im
kv kt

(5.11)

(5.12)

for the velocity-controlled generator, while for the torque-controlled generator
it is
kt
Im
km =
Tm =
.
(5.13)
ki Fm
Fm
Notice how the power ampliﬁer, in the velocity control case, contributes to
the input/output relation with the constant Gv , while in the case of current
control the ampliﬁer, being inside a local feedback loop, does not appear as a
stand alone but rather in the expression of km with a factor 1/ki .
These considerations lead to the following conclusions. In all such applications where the drive system has to provide high rejection of disturbance
torques (as in the case of independent joint control, see Sect. 8.3) it is not
advisable to have a current feedback in the loop, at least when all quantities

202

5 Actuators and Sensors

Fig. 5.5. Block scheme of an electric drive with nonlinear current feedback

are within their nominal values. In this case, the problem of setting a protection can be solved by introducing a current limit that is not performed by
a saturation on the control signal but it exploits a current feedback with a
dead-zone nonlinearity on the feedback path, as shown in Fig. 5.5. Therefore,
an actual current limit is obtained whose precision is as high as the slope
of the dead zone; it is understood that stability of the current loop is to be
addressed when operating in this way.
As will be shown in Sect. 8.5, centralized control schemes, instead, demand
the drive system to behave as a torque-controlled generator. It is then clear
that a current feedback with a suitable regulator Ci (s) should be used so as
to confer a good static and dynamic behaviour to the current loop. In this
case, servoing of the driving torque is achieved indirectly, since it is based on a
current measurement which is related to the driving torque by means of gain
1/kt .
5.2.2 Hydraulic Drives
No matter how a hydraulic servomotor is constructed, the derivation of its
input/output mathematical model refers to the basic equations describing
the relationship between ﬂow rate and pressure, the relationship between the
ﬂuid and the parts in motion, and the mechanical balance of the parts in
motion. Let Q represent the volume ﬂow rate supplied by the distributor; the
ﬂow rate balance is given by the equation
Q = Qm + Ql + Qc

(5.14)

where Qm is the ﬂow rate transferred to the motor, Ql is the ﬂow rate due to
leakage, and Qc is the ﬂow rate related to ﬂuid compressibility. The terms Ql
and Qc are taken into account in view of the high operating pressures (of the
order of 100 atm).
Let P denote the diﬀerential pressure of the servomotor due to the load;
then it can be assumed that
(5.15)
Ql = kl P .
Regarding the loss for compressibility, if V denotes the instantaneous volume
of the ﬂuid, one has
Qc = γV sP
(5.16)

5.2 Drives

203

Fig. 5.6. Block scheme of a hydraulic drive

where γ is the uniform compressibility coeﬃcient of the ﬂuid. Notice that the
proportional factor kc = γV between the time derivative of the pressure and
the ﬂow rate due to compressibility depends on the volume of the ﬂuid; therefore, in the case of rotary servomotors, kc is a constant, whereas in the case
of a linear servomotor, the volume of ﬂuid varies and thus the characteristic
of the response depends on the operating point.
The volume ﬂow rate transferred to the motor is proportional to the volume variation in the chambers per time unit; with reference from now on to a
rotary servomotor, such variation is proportional to the angular velocity, and
then
(5.17)
Qm = kq Ωm .
The mechanical balance of the parts in motion is described by
Cm = (sIm + Fm )Ωm + Cl

(5.18)

with obvious meaning of the symbols. Finally, the driving torque is proportional to the diﬀerential pressure of the servomotor due to the load, i.e.,
Cm = kt P .

(5.19)

Concerning the servovalve, the transfer function between the stem position
X and the control voltage Vc is expressed by
X
Gs
=
Vc
1 + sTs

(5.20)

thanks to the linearizing eﬀect achieved by position feedback; Gs is the equivalent gain of the servovalve, whereas its time constant Ts is of the order of ms
and thus it can be neglected with respect to the other time constants of the
system.
Finally, regarding the distributor, the relationship between the diﬀerential pressure, the ﬂow rate, and the stem displacement is highly nonlinear;
linearization about an operating point leads to the equation
P = kx X − kr Q.

(5.21)

204

5 Actuators and Sensors

Fig. 5.7. Schematic representation of a mechanical gear

By virtue of (5.14)–(5.21), the servovalve/distributor/motor complex (hydraulic drive) is represented by the block scheme of Fig. 5.6. A comparison
between the schemes in Figs. 5.2 and 5.6 clearly shows the formal analogy
in the dynamic behaviour of an electric and a hydraulic servomotor. Nevertheless, such analogy should not induce one to believe that it is possible to
make a hydraulic drive play the role of a velocity- or torque-controlled generator, as for an electric drive. In this case, the pressure feedback loop (formally
analogous to the current feedback loop) is indeed a structural characteristic
of the system and, as such, it cannot be modiﬁed but with the introduction
of suitable transducers and the realization of the relative control circuitry.
5.2.3 Transmission Eﬀects
In order to describe quantitatively the eﬀects introduced by the use of a transmission (mechanical gear ) between the servomotor and the actuated joint, it
is worth referring to the mechanical coupling realized by a pair of spur gears of
radius rm and r, which is schematically represented in Fig. 5.7; the kinematic
pair is assumed to be ideal (without backlash) and connects the rotation axis
of the servomotor with the axis of the corresponding joint.
With reference to an electric servomotor, it is assumed that the rotor of
the servomotor is characterized by an inertia moment Im about its rotation
axis and a viscous friction coeﬃcient Fm ; likewise, I and F denote the inertia
moment and the viscous friction coeﬃcient of the load. The inertia moments
and the friction coeﬃcients of the gears are assumed to have been included
in the corresponding parameters of the motor (for the gear of radius rm ) and
of the load (for the gear of radius r). Let cm denote the driving torque of the
motor and cl the reaction torque applied to the load axis. Also let ωm and ϑm
denote the angular velocity and position of the motor axis, while ω and ϑ
denote the corresponding quantities at the load side. Finally, f indicates the
force exchanged at the contact between the teeth of the two gears.2
2

In the case considered, it has been assumed that both the motor and the load
are characterized by revolute motions; if the load should exhibit a translation
motion, the following arguments can be easily extended, with analogous results,
by replacing the angular displacements with linear displacements and the inertia
moments with masses at the load side.

5.2 Drives

205

The gear reduction ratio is deﬁned as
kr =

ωm
r
ϑm
=
=
rm
ϑ
ω

(5.22)

since, in the absence of slipping in the kinematic coupling, it is rm ϑm = rϑ.
The gear reduction ratio, in the case when it is representative of the coupling between a servomotor and the joint of a robot manipulator, attains
values much larger than unity (rm  r) — typically from a few tens to a few
hundreds.
The force f exchanged between the two gears generates a reaction torque f ·
rm for the motion at the motor axis and a driving torque f · r for the rotation
motion of the load.
The mechanical balances at the motor side and the load side are respectively:
cm = Im ω̇m + Fm ωm + f rm

(5.23)

f r = I ω̇ + F ω + cl .

(5.24)

To describe the motion with reference to the motor angular velocity, in view
of (5.22), combining the two equations gives at the motor side
cm = Ieq ω̇m + Feq ωm +


where
Ieq =

I
Im + 2
kr




Feq =

cl
kr

(5.25)

F
Fm + 2
kr


.

(5.26)

The expressions (5.25), (5.26) show how, in the case of a gear with large
reduction ratio, the inertia moment and the viscous friction coeﬃcient of the
load are reﬂected at the motor axis with a reduction of a factor 1/kr2 ; the
reaction torque, instead, is reduced by a factor 1/kr . If this torque depends
on ϑ in a nonlinear fashion, then the presence of a large reduction ratio tends
to linearize the dynamic equation.

Example 5.1
In Fig. 5.8 a rigid pendulum is represented, which is actuated by the torque f · r to
the load axis after the gear. In this case, the dynamic equations of the system are
cm = Im ω̇m + Fm ωm + f rm

(5.27)

f r = I ω̇ + F ω + mg sin ϑ

(5.28)

where I is the inertia moment of the pendulum at the load axis, F is the viscous friction coeﬃcient, m is the pendulum mass, its length and g the gravity acceleration.
Reporting (5.28) to the motor axis gives



cm = Ieq ω̇m + Feq ωm +

mg
kr





sin

ϑm
kr



(5.29)

206

5 Actuators and Sensors

Fig. 5.8. Pendulum actuated via mechanical gear

from which it is clear how the contribution of the nonlinear term is reduced by the
factor kr .

The example of the pendulum has been considered to represent an n-link
manipulator with revolute joints, for which each link, considered as isolated
from the others, can be considered as a simple rigid pendulum. The connection
with other links introduces, in reality, other nonlinear eﬀects which complicate
the input/output model; in this regard, it is suﬃcient to notice that, in the
case of a double pendulum, the inertia moment at the motor side of the ﬁrst
link depends also on the angular position of the second link.
In Chap. 7 the eﬀect introduced by the presence of transmissions in a
generic n-link manipulator structure will be studied in detail. Nevertheless, it
can already be understood how the nonlinear couplings between the motors of
the various links will be reduced by the presence of transmissions with large
reduction ratios.
5.2.4 Position Control
After having examined the modalities to control the angular velocity of an
electric or hydraulic drive, the motion control problem for a link of a generic
manipulator is to be solved. A structure is sought which must be capable of
determining, in an automatic way, the time evolution of the quantity chosen
to control the drive, so that the actuated joint executes the required motion
allowing the end-eﬀector to execute a given task.
Once a trajectory has been speciﬁed for the end-eﬀector pose, the solution of the inverse kinematics problem allows the computation of the desired
trajectories for the various joints, which thus can be considered as available.
Several control techniques can be adopted to control the manipulator motion; the choice of a particular solution depends on the required dynamic
performance, the kind of motion to execute, the kinematic structure, and the

5.2 Drives

207

Fig. 5.9. General block scheme of electric drive control

choice to utilize either servomotors with transmissions or torque motors with
joint direct drive.
The simplest solution is to consider, at ﬁrst approximation, the motion
of a joint independent of the motion of the other joints, i.e., the interaction
can be regarded as a disturbance. Assume the reference trajectory ϑr (t) is
available. According to classical automatic control theory, to ensure that the
angular motor position ϑm , properly measured by means of a transducer with
constant kT P , follows ϑr , it is worth resorting to a feedback control system
providing ‘robustness’ with respect to both model uncertainty on the motor
and the load, and the presence of a disturbance. A more detailed treatment
is deferred to Chap. 8, where the most congenial solutions to solve the above
problems will be presented.
In the following, the problem of joint position control is tackled by assuming an electric DC servomotor; the choice is motivated by the diﬀusion of this
technology, due to the high ﬂexibility of these actuators providing optimal
responses in the large majority of motion control applications.
The choice of a feedback control system to realize a position servo at
the motor axis requires the adoption of a controller ; this device generates
a signal which, applied to the power ampliﬁer, automatically generates the
driving torque producing an axis motion very close to the desired motion ϑr .
Its structure should be so that the error between the reference input and the
measured output is minimized, even in the case of inaccurate knowledge of
the dynamics of the motor, the load, and a disturbance. The rejection action
of the disturbance is the more eﬃcient, the smaller the magnitude of the
disturbance.
On the other hand, according to (5.9), the disturbance is minimized, provided the drive is velocity-controlled. In this case, in view of (5.6), the reaction
torque inﬂuences the motor axis velocity with a coeﬃcient equal to Ra /kv kt
which is much smaller than 1/Fm , which represents instead the weight on
the reaction torque in the case when the drive is torque-controlled. Therefore,
with reference to Fig. 5.3, the general scheme of drive control with position
feedback is illustrated in Fig. 5.9, where the disturbance d represents the load

208

5 Actuators and Sensors

Fig. 5.10. Block scheme of drive control with position feedback

torque and the value of the power ampliﬁer gain has been included in the
control action.
Besides reducing the eﬀects of the disturbance on the output, the structure
of the controller must ensure an optimal trade-oﬀ between the stability of the
feedback control system and the capability of the output to dynamically track
the reference with a reduced error.
The reduction of the disturbance eﬀects on the output can be achieved
by conferring a large value of the gain before the point of intervention of
the disturbance, without aﬀecting stability. If, at steady state (ϑr = cost,
cl = cost), it is desired to cancel the disturbance eﬀect on the output, the
controller must act an integral action on the error given by the diﬀerence
between ϑr and kT P ϑm .
The above requirements suggest the use of a simple controller with an integral and a proportional action on the error; the proportional action is added
to realize a stabilizing action, which, however, cannot confer to the closed-loop
system a damped transient response with a suﬃciently short sampling time.
This behaviour is due to the presence of a double pole at the origin of the
transfer function of the forward path.
The resulting control scheme is illustrated in Fig. 5.10, where km and Tm
are respectively the voltage-to-velocity gain constant and the characteristic
time constant of the motor in (5.12). The parameters of the controller KP
and TP should be keenly chosen so as to ensure stability of the feedback
control system and obtain a good dynamic behaviour.
To improve the transient response, the industrial drives employed for position servoing may also include a local feedback loop based on the angular
velocity measurement (tachometer feedback). The general scheme with position and velocity feedback is illustrated in Fig. 5.11; besides the position
transducer, a velocity transducer is used with constant kT V , as well as a simple proportional controller with gain KP . With the adoption of the tachometer
feedback, the proportional-integral controller with parameters KV and TV is
retained in the internal velocity loop so as to cancel the eﬀects of the disturbance on the position ϑm at steady state. The presence of two feedback loops,
in lieu of one, around the intervention point of the disturbance is expected
to lead to a further reduction of the disturbance eﬀects on the output also
during the transients.

5.3 Proprioceptive Sensors

209

Fig. 5.11. Block scheme of drive control with position and velocity feedback

The adoption of tachometer feedback may also improve the transient response of the whole control system with respect to the previous case. With
a keen choice of the controller parameters, indeed, it is possible to achieve a
transfer function between ϑm and ϑr with a larger bandwidth and reduced
resonance phenomena. The result is a faster transient response with reduced
oscillations, thus improving the capability of ϑm (t) to track more demanding
reference trajectories ϑr (t).
The above analysis will be further detailed in Sect. 8.3.
The position servo may also utilize a current-controller motor; the schemes
in Figs. 5.9–5.11 can be adopted, provided that the constants in (5.13) are
used in the transfer function (5.11) and the disturbance D is weighed with
the quantity ki /kt in lieu of Ra /kt . In that case, the voltage gain Gv of the
power ampliﬁer will not contribute to the control action.
As a ﬁnal consideration, the general control structure presented above
may be extended to the case when the motor is coupled to a load via a gear
reduction. In such a case, it is suﬃcient to account for (5.25) and (5.26), i.e.,
replace Im and Fm with the quantities Ieq and Feq , and scale the disturbance
by the factor 1/kr .

5.3 Proprioceptive Sensors
The adoption of sensors is of crucial importance to achieve high-performance
robotic systems. It is worth classifying sensors into proprioceptive sensors that
measure the internal state of the manipulator, and exteroceptive sensors that
provide the robot with knowledge of the surrounding environment.
In order to guarantee that a coordinated motion of the mechanical structure is obtained in correspondence of the task planning, suitable parameter
identiﬁcation and control algorithms are used which require the on-line measurement, by means of proprioceptive sensors, of the quantities characterizing
the internal state of the manipulator, i.e.:

210

5 Actuators and Sensors

• joint positions,
• joint velocities,
• joint torques.
On the other hand, typical exteroceptive sensors include:
•
•
•
•
•

force sensors,
tactile sensors,
proximity sensors,
range sensors,
vision sensors.

The goal of such sensors is to extract the features characterizing the interaction of the robot with the objects in the environment, so as to enhance
the degree of autonomy of the system. To this class also belong those sensors
which are speciﬁc for the robotic application, such as sound, humidity, smoke,
pressure, and temperature sensors. Fusion of the available sensory data can
be used for (high-level) task planning, which in turn characterizes a robot as
the intelligent connection of perception to action.
In the following, the main features of the proprioceptive sensors are illustrated, while those of the exteroceptive sensors will be presented in the next
section.
5.3.1 Position Transducers
The aim of position transducers is to provide an electric signal proportional
to the linear or angular displacement of a mechanical apparatus with respect
to a given reference position. They are mostly utilized for control of machine
tools, and thus their range is wide. Potentiometers, linear variable-diﬀerential
transformers (LVDT), and inductosyns may be used to measure linear displacements. Potentiometers, encoders, resolvers and synchros may be used to
measure angular displacements.
Angular displacement transducers are typically employed in robotics applications since, also for prismatic joints, the servomotor is of a rotary type.
In view of their precision, robustness and reliability, the most common transducers are the encoders and resolvers, whose operating principles are detailed
in what follows.
On the other hand, linear displacement transducers (LVDT’s and inductosyns) are mainly employed in measuring robots.
Encoder
There are two types of encoder: absolute and incremental. The absolute encoder consists of an optical-glass disk on which concentric circles (tracks) are
disposed; each track has an alternating sequence of transparent sectors and
matte sectors obtained by deposit of a metallic ﬁlm. A light beam is emitted in

5.3 Proprioceptive Sensors

211

Fig. 5.12. Schematic representation of an absolute encoder

correspondence of each track which is intercepted by a photodiode or a phototransistor located on the opposite side of the disk. By a suitable arrangement
of the transparent and matte sectors, it is possible to convert a ﬁnite number
of angular positions into corresponding digital data. The number of tracks
determines the length of the word, and thus the resolution of the encoder.
To avoid problems of incorrect measurement in correspondence of a simultaneous multiple transition between matte and transparent sectors, it is
worth utilizing a Gray-code encoder whose schematic representation is given
in Fig. 5.12 with reference to the implementation of 4 tracks that allow the
discrimination of 16 angular positions. It can be noticed that measurement
ambiguity is eliminated, since only one change of contrast occurs at each transition (Table 5.1). For the typical resolution required for joint control, absolute
encoders with a minimum number of 12 tracks (bits) are employed (resolution
of 1/4096 per circle). Such encoders can provide unambiguous measurements
only in a circle. If a gear reduction is present, a circle at the joint side corresponds to several circles at the motor side, and thus a simple electronics is
needed to count and store the number of actual circles.
Table 5.1. Coding table with Gray-code
#
0
1
2
3
4
5
6
7

Code
0000
0001
0011
0010
0110
0111
0101
0100

#
8
9
10
11
12
13
14
15

Code
1100
1101
1111
1110
1010
1011
1001
1000

212

5 Actuators and Sensors

Fig. 5.13. Schematic representation of an incremental encoder

Incremental encoders have a wider use than absolute encoders, since they
are simpler from a construction viewpoint and thus cheaper. Like the absolute
one, the incremental encoder consists of an optical disk on which two tracks
are disposed, whose transparent and matte sectors (in equal number on the
two tracks) are mutually in quadrature. The presence of two tracks also allows,
besides the number of transitions associated with any angular rotation, the
detection of the sign of rotation. Often a third track is present with one single
matte sector which allows the deﬁnition of an absolute mechanical zero as
a reference for angular position. A schematic representation is illustrated in
Fig. 5.13.
The use of an incremental encoder for a joint actuating system clearly
demands the evaluation of absolute positions. This is performed by means of
suitable counting and storing electronic circuits. To this end, it is worth noticing that the position information is available on volatile memories, and thus
it can be corrupted due to the eﬀect of disturbances acting on the electronic
circuit, or else ﬂuctuations in the supply voltage. Such limitation obviously
does not occur for absolute encoders, since the angular position information
is coded directly on the optical disk.
The optical encoder has its own signal processing electronics inside the
case, which provides direct digital position measurements to be interfaced with
the control computer. If an external circuitry is employed, velocity measurements can be reconstructed from position measurements. In fact, if a pulse
is generated at each transition, a velocity measurement can be obtained in
three possible ways, namely, by using a voltage-to-frequency converter (with
analog output), by (digitally) measuring the frequency of the pulse train, or
by (digitally) measuring the sampling time of the pulse train. Between these
last two techniques, the former is suitable for high-speed measurements while
the latter is suitable for low-speed measurements.

5.3 Proprioceptive Sensors

213

Fig. 5.14. Electric scheme of a resolver with functional diagram of a tracking-type
RDC

Resolver
The resolver is an electromechanical position transducer which is compact
and robust. Its operating principle is based on the mutual induction between
two electric circuits which allow continuous transmission of angular position
without mechanical limits. The information on the angular position is associated with the magnitude of two sinusoidal voltages, which are treated by a
suitable resolver-to-digital converter (RDC) to obtain the digital data corresponding to the position measurement. The electric scheme of a resolver with
the functional diagram of a tracking-type RDC is illustrated in Fig. 5.14.
From a construction viewpoint, the resolver is a small electric machine
with a rotor and a stator; the inductance coil is on the rotor while the stator
has two windings at 90 electrical degrees one from the other. By feeding the
rotor with a sinusoidal voltage V sin ωt (with typical frequencies in the range
of 0.4 to 10 kHz), a voltage is induced on the stator windings whose magnitude depends on the rotation angle θ. The two voltages are fed to two digital
multipliers, whose input is α and whose outputs are algebraically summed
to achieve V sin ωt sin (θ − α); this signal is then ampliﬁed and sent to the
input of a synchronous detector, whose ﬁltered output is proportional to the
quantity sin (θ − α). The resulting signal, after a suitable compensating action, is integrated and then sent to the input of a voltage-controlled oscillator
(VCO) (a voltage-to-frequency converter) whose output pulses are input to a
forward-backward counter. Digital data of the quantity α are available on the
output register of the counter, which represent a measurement of the angle θ.
It can be recognized that the converter works according to a feedback
principle. The presence of two integrators (one is represented by the forwardbackward counter) in the loop ensures that the (digital) position and (analog)
velocity measurements are error-free as long as the rotor rotates at constant
speed; actually, a round-oﬀ error occurs on the word α and thus aﬀects the
position measurement. The compensating action is needed to confer suitable
stability properties and bandwidth to the system. Whenever digital data are
wished also for velocity measurements, it is necessary to use an analog-to-

214

5 Actuators and Sensors

digital converter. Since the resolver is a very precise transducer, a resolution
of 1 bit out of 16 can be obtained at the output of the RDC.
5.3.2 Velocity Transducers
Even though velocity measurements can be reconstructed from position transducers, it is often preferred to resort to direct measurements of velocity, by
means of suitable transducers. Velocity transducers are employed in a wide
number of applications and are termed tachometers. The most common devices of this kind are based on the operating principles of electric machines.
The two basic types of tachometers are the direct-current (DC) tachometer
and the alternating-current (AC) tachometer .
DC tachometer
The direct-current tachometer is the most used transducer in the applications.
It is a small DC generator whose magnetic ﬁeld is provided by a permanent
magnet. Special care is paid to its construction, so as to achieve a linear
input/output relationship and to reduce the eﬀects of magnetic hysteresis
and temperature. Since the ﬁeld ﬂux is constant, when the rotor is set in
rotation, its output voltage is proportional to angular speed according to the
constant characteristic of the machine.
Because of the presence of a commutator, the output voltage has a residual ripple which cannot be eliminated by proper ﬁltering, since its frequency
depends on angular speed. A linearity range of 0.1 to 1% can be obtained,
whereas the residual ripple coeﬃcient is of 2 to 5% of the mean value of the
output signal.
AC tachometer
In order to avoid the drawbacks caused by the presence of a residual ripple in
the output of a DC tachometer, one may resort to an AC tachometer. While
the DC tachometer is a true DC generator, the AC tachometer diﬀers from a
generator. In fact, if a synchronous generator would be used, the frequency of
the output signal would be proportional to the angular speed.
To obtain an alternating voltage whose magnitude is proportional to speed,
one may resort to an electric machine that is structurally diﬀerent from the
synchronous generator. The AC tachometer has two windings on the stator
mutually in quadrature and a cup rotor. If one of the windings is fed by a
constant-magnitude sinusoidal voltage, a sinusoidal voltage is induced on the
other winding which has the same frequency, a magnitude proportional to
angular speed, and a phase equal or opposite to that of the input voltage
according to the sign of rotation; the exciting frequency is usually set to
400 Hz. The use of a synchronous detector then yields an analog measurement

5.4 Exteroceptive Sensors

215

Fig. 5.15. a) Schematic representation of a strain gauge. b) Its insertion in a
Wheatstone bridge

of angular velocity. In this case, the output ripple can be eliminated by a
proper ﬁlter, since its fundamental frequency is twice as much as the supply
frequency.
The performance of AC tachometers is comparable to that of DC tachometers. Two further advantages of AC tachometers are the lack of wiping contacts
and the presence of a low moment of inertia, in view of the use of a lightweight
cup rotor. However, a residual voltage occurs, even when the rotor is still, because of the unavoidable parasitic couplings between the stator coil and the
measurement circuitry.

5.4 Exteroceptive Sensors
5.4.1 Force Sensors
Measurement of a force or torque is usually reduced to measurement of the
strain induced by the force (torque) applied to an extensible element of suitable features. Therefore, an indirect measurement of force is obtained by
means of measurements of small displacements. The basic component of a
force sensor is the strain gauge which uses the change of electric resistance of
a wire under strain.
Strain gauge
The strain gauge consists of a wire of low temperature coeﬃcient. The wire is
disposed on an insulated support (Fig. 5.15a) which is glued to the element
subject to strain under the action of a stress. Dimensions of the wire change
and then they cause a change of electric resistance.
The strain gauge is chosen in such a way that the resistance Rs changes
linearly in the range of admissible strain for the extensible element. To transform changes of resistance into an electric signal, the strain gauge is inserted in
one arm of a Wheatstone bridge which is balanced in the absence of stress on
the strain gauge itself. From Fig. 5.15b it can be understood that the voltage
balance in the bridge is described by


R2
Rs
−
(5.30)
Vo =
Vi .
R1 + R2
R3 + Rs

216

5 Actuators and Sensors

If temperature variations occur, the wire changes its dimension without
application of any external stress. To reduce the eﬀect of temperature variations on the measurement output, it is worth inserting another strain gauge
in an adjacent arm of the bridge, which is glued on a portion of the extensible
element not subject to strain.
Finally, to increase bridge sensitivity, two strain gauges may be used which
have to be glued on the extensible element in such a way that one strain gauge
is subject to traction and the other to compression; the two strain gauges then
have to be inserted in two adjacent arms of the bridge.
Shaft torque sensor
In order to employ a servomotor as a torque-controlled generator, an indirect
measurement of the driving torque is typically used, e.g., through the measurement of armature current in a permanent-magnet DC servomotor. If it is
desired to guarantee insensitivity to change of parameters relating torque to
the measured physical quantities, it is necessary to resort to a direct torque
measurement.
The torque delivered by the servomotor to the joint can be measured by
strain gauges mounted on an extensible apparatus interposed between the
motor and the joint, e.g., a hollow shafting. Such apparatus must have low
torsional stiﬀness and high bending stiﬀness, and it must ensure a proportional
relationship between the applied torque and the induced strain.
By connecting the strain gauges mounted on the hollow shafting (in a
Wheatstone bridge conﬁguration) to a slip ring by means of graphite brushes,
it is possible to feed the bridge and measure the resulting unbalanced signal
which is proportional to the applied torque.
The measured torque is that delivered by the servomotor to the joint, and
thus it does not coincide with the driving torque Cm in the block schemes of
the actuating systems in Fig. 5.2 and in Fig. 5.6. In fact, such measurement
does not account for the inertial and friction torque contributions as well as
for the transmission located upstream of the measurement point.
Wrist force sensor
When the manipulator’s end-eﬀector is in contact with the working environment, the force sensor allows the measurement of the three components of a
force and the three components of a moment with respect to a frame attached
to it.
As illustrated in Fig. 5.16, the sensor is employed as a connecting apparatus
at the wrist between the outer link of the manipulator and the end-eﬀector.
The connection is made by means of a suitable number of extensible elements
subject to strain under the action of a force and a moment. Strain gauges
are glued on each element which provide strain measurements. The elements

5.4 Exteroceptive Sensors

217

Fig. 5.16. Use of a force sensor on the outer link of a manipulator

have to be disposed in a keen way so that at least one element is appreciably
deformed for any possible orientation of forces and moments.
Furthermore, the single force component with respect to the frame attached to the sensor should induce the least possible number of deformations,
so as to obtain good structural decoupling of force components. Since a complete decoupling cannot be achieved, the number of signiﬁcant deformations
to reconstruct the six components of the force and moment vector is greater
than six.
A typical force sensor is that where the extensible elements are disposed as
in a Maltese cross; this is schematically indicated in Fig. 5.17. The elements
connecting the outer link with the end-eﬀector are four bars with a rectangular
parallelepiped shape. On the opposite sides of each bar, a pair of strain gauges
is glued that constitute two arms of a Wheatstone bridge; there is a total of
eight bridges and thus the possibility of measuring eight strains.
The matrix relating strain measurements to the force components expressed in a Frame s attached to the sensor is termed sensor calibration matrix .
Let wi , for i = 1, . . . , 8, denote the outputs of the eight bridges providing measurement of the strains induced by the applied forces on the bars according

218

5 Actuators and Sensors

Fig. 5.17. Schematic representation of a Maltese-cross force sensor

to the directions speciﬁed in Fig. 5.17. Then, the calibration matrix is given
by the transformation
⎡

fxs
⎢ fys
⎢ s
⎢ fz
⎢ s
⎢ μx
⎣ s
μy
μsz

⎤

⎡

0
⎥ ⎢ c21
⎥ ⎢
⎥ ⎢ 0
⎥=⎢
⎥ ⎢ 0
⎦ ⎣
0
c61

0
0
c32
0
c52
0

c13
0
0
0
0
c63

0
0
c34
c44
0
0

0
c25
0
0
0
c65

0
0
c36
0
c56
0

c17
0
0
0
0
c67

⎡

⎤
w1
0 ⎢ w2 ⎥
⎢ ⎥
0 ⎥ ⎢ w3 ⎥
⎥⎢ ⎥
c38 ⎥ ⎢ w4 ⎥
⎥⎢ ⎥.
c48 ⎥ ⎢ w5 ⎥
⎦⎢ ⎥
0 ⎢ w6 ⎥
⎣ ⎦
0
w7
w8
⎤

(5.31)

Reconstruction of force measurements through the calibration matrix is entrusted to suitable signal processing circuitry available in the sensor.
Typical sensors have a diameter of about 10 cm and a height of about 5 cm,
with a measurement range of 50 to 500 N for the forces and of 5 to 70 N·m for
the torques, and a resolution of the order of 0.1% of the maximum force and
of 0.05% of the maximum torque, respectively; the sampling frequency at the
output of the processing circuitry is of the order of 1 kHz.
Finally, it is worth noticing that force sensor measurements cannot be
directly used by a force/motion control algorithm, since they describe the
equivalent forces acting on the sensors which diﬀer from the forces applied to
the manipulator’s end-eﬀector (Fig. 5.16). It is therefore necessary to trans-

5.4 Exteroceptive Sensors

219

form those forces from the sensor Frame s into the constraint Frame c; in view
of the transformation in (3.116), one has
 s 
 c 
Rcs
fs
O
fc
=
(5.32)
μcc
μss
S(r ccs )Rcs Rcs
which requires knowledge of the position r ccs of the origin of Frame s with
respect to Frame c as well as of the orientation Rcs of Frame s with respect
to Frame c. Both such quantities are expressed in Frame c, and thus they are
constant only if the end-eﬀector is still, once contact has been achieved.
5.4.2 Range Sensors
The primary function of the exteroceptive sensors is to provide the robot with
the information needed to execute ‘intelligent’ actions in an autonomous way.
To this end, it is crucial to detect the presence of an object in the workspace
and eventually to measure its range from the robot along a given direction.
The former kind of data is provided by the proximity sensors, a simpliﬁed
type of range sensors, capable of detecting only the presence of objects nearby
the sensitive part of the sensor, without a physical contact. The distance
within which such sensors detect objects is deﬁned sensitive range.
In the more general case, range sensors are capable of providing structured
data, given by the distance of the measured object and the corresponding
measurement direction, i.e., the position in space of the detected object with
respect to the sensor.
The data provided by the range sensors are used in robotics to avoid
obstacles, build maps of the environment, recognize objects.
The most popular range sensors in robotics applications are those based
on sound propagation through an elastic ﬂuid, the so-called sonars (SOund
NAvigation and Ranging), and those exploiting light propagation features, the
so-called lasers (Light Ampliﬁcation by Stimulated Emission of Radiation).
In the following, the main features of these two sensors are illustrated.
Sonars
The sonars employ acoustic pulses and their echoes to measure the range to an
object. Since the sound speed is usually known for a given media (air, water),
the range to an object is proportional to the echo travel time, commonly called
time-of-ﬂight, i.e., the time which the acoustic wave takes to cover the distance
sensor-object-sensor. Sonars are widely utilized in robotics, and especially in
mobile and underwater robotics. Their popularity is due to their low cost, light
weight, low power consumption, and low computational eﬀort, compared to
other ranging sensors. In some applications, such as in underwater and lowvisibility environments, the sonar is often the only viable sensing modality.
Despite a few rare examples of sonars operating at audible frequencies
for human ears (about 20 Hz to 20 KHz), the ultrasound frequencies (higher

220

5 Actuators and Sensors

Fig. 5.18. Sonar ranging principle

than 20 KHz) are the most widely used to realize this type of sensor. Typical
frequencies in robotics range from 20 KHz to 200 KHz, even though higher
values (of the order of MHz) can be achieved utilizing piezoelectric quartz
crystals. In this range, the energy of the wave emitted by the sonar can be
regarded as concentrated in a conical volume whose beamwidth depends on
the frequency as well as on the transducer diameter. Further to measuring
range, sonars provide qualitative directional data on the object which has
generated the echo. For the most common sensors in robotics, the beamwidth
of the energy beam is typically not smaller than 15 deg. Obviously, for smaller
beamwidths, higher angular resolutions can be obtained.
The main components of a sonar measurement system are a transducer,
which is vibrated and transforms acoustic energy into electric energy and vice
versa, and a circuitry for the excitation of the transducer and the detection of
the reﬂected signal. Figure 5.18 schematically illustrates the operating principle: the pulse I emitted by the transducer, after hitting the object O found
in the emission cone of the sensor, is partly reﬂected (echo E) towards the
sound source and thus detected. The time-of-ﬂight tv is the time between the
emission of the ultrasound pulse and the reception of the echo. The object
range dO can be computed from tv using the relation
dO =

cs tv
2

(5.33)

where cs is sound speed, which in low-humidity air depends on the temperature T (measured in centigrade) according to the expression
√
cs ≈ 20.05 T + 273.16 m/s.
(5.34)
In the scheme of Fig. 5.18 the use of a sole transducer is represented for
the transmission of the pulse and the reception of the echo. This conﬁguration requires that the commutation from transmitter to receiver takes place
after a certain latency time which depends not only on the duration of the
transmitted pulse but also on the mechanical inertia of the transducer.
Despite the low cost and ease of use, however, these sensors have nonnegligible limits with respect to the angular and radial resolution, as well as
to the minimum and maximum measurement range that can be achieved. In
particular, the width of the radiation cone decreases as frequency increases
with improved angular resolution. A higher frequency leads to greater radial

5.4 Exteroceptive Sensors

221

Fig. 5.19. Reﬂector models on smooth surfaces: a) non-detected plane. b) nondetected corner. c) plane with false detection (O real object, Ov virtual object
detected)

resolution and contributes to reducing the minimum range that can be detected by the sonar. Nevertheless, there is a lower limit because of the lapse
time when reception is inhibited to avoid interference with the reﬂected signal — in certain cases better performance can be obtained by employing two
distinct transducers for the emission and the detection. On the other hand,
too high frequencies may exasperate absorbtion phenomena, depending on the
features of the surface generating the echo. Such phenomena further reduce
the power of the transmitted signal — decreasing with the square of the range
covered by the ultrasound wave — thus reducing the maximum limit of the
measurement time.
Piezoelectric and electrostatic transducers are the two major types available that operate in air and can in principle operate both as a transmitter
and receiver.
The piezoelectric transducers exploit the property of some crystal materials
to deform under the action of an electric ﬁeld and vibrate when a voltage is
applied at the resonant frequency of the crystal. The eﬃciency of the acoustic
match of these transducers with compressible ﬂuids such as air is rather low.
Often a conical concave horn is mounted on the crystal to match acoustically
the crystal acoustic impedance to that of air. Being of resonant type, these
transducers are characterized by a rather low bandwidth and show a signiﬁcant
mechanical inertia which severely limits the minimum detectable range, thus
justifying the use of two distinct transducers as transmitter and receiver.
The electrostatic transducers operate as capacitors whose capacitance
varies moving and/or deforming one of its plates. A typical construction consists of a gold-coated plastic foil membrane (mobile plate) stretched across a
round grooved aluminium back plate (ﬁxed plate). When the transducer operates as receiver, the change of capacitance, induced by the deformation of
the membrane under the acoustic pressure, produces a proportional change of
the voltage across the capacitor, assuming that the foil charge is constant. As
a transmitter, the transducer membrane is vibrated by applying a sequence
of electric pulses across the capacitor. The electric oscillations generate, as

222

5 Actuators and Sensors

Fig. 5.20. Time-of-ﬂight laser sensor operating principle

a result of the induced electric ﬁeld, a mechanical force which vibrates the
mobile plate.
Since the electrostatic transducers can operate at diﬀerent frequencies,
they are characterized by large bandwidth and high sensitivity, low mechanical inertia and rather eﬃcient acoustic match with air. As compared to the
piezoelectric transducers, however, they can operate at lower maximum frequencies (a few hundreds kHz vs a few MHz) and require a bias voltage which
complicates the control electronics. Among the ultrasound measurement systems with capacitive transducers, it is worth mentioning the Polaroid sonar,
initially developed for autofocus systems and later widely employed as range
sensors in several robotic applications. The 600 series sensor utilizes a capacitive transducer of the type described above with a diameter of almost
4 cm, operates at 50 kHz frequency and is characterized by a beamwidth of
15 deg, can detect a maximum range of about 10 m and a mimimum range
of about 15 cm with an accuracy of ±1% across the measurement range. The
bias voltage is 200 V with current absorbtion peaks of 2 A in transmission.
Accuracy of ultrasound range sensors depends on the features of the transducer and the excitation/detection circuitry, as well as on the reﬂective properties of the surfaces hit by the acoustic waves.
Smooth surfaces, i.e., those characterized by irregularities of comparable
size to that of the wavelength corresponding to the employed frequency, may
produce a non-detectable echo at the sensor (Figura 5.19a,b) if the incident
angle of the ultrasound beam exceeds a given critical angle which depends
on the operational frequency and the reﬂective material. In the case of the
Polaroid sensors, this angle is equal to 65 deg, i.e., 25 deg from the normal
to the reﬂective surface, for a smooth surface in plywood. When operating in
complex environments, such mirror reﬂections may give rise to multiple reﬂections, thus causing range measurement errors or false detection (Fig. 5.19c).
Lasers
In the construction of optical measurement systems, the laser beam is usually
preferred to other light sources for the following reasons:

5.4 Exteroceptive Sensors

•
•
•
•

223

They can easily generate bright beams with lightweight sources.
The infrared beams can be used unobtrusively.
They focus well to give narrow beams.
Single-frequency sources allow easier rejection ﬁltering of unwanted frequencies, and do not disperse from refraction as much as full spectrum
sources.

There are two types of laser-based range sensors in common use: the timeof-ﬂight sensors and the triangulation sensors.
The time-of-ﬂight sensors compute distance by measuring the time that a
pulse of light takes to travel from the source to the observed target and then to
the detector (usually collocated with the source). The travel time multiplied
by the speed of light (properly adjusted for the air temperature) gives the
distance measurement. The operating principle of a time-of-ﬂight laser sensor
is illustrated in Fig. 5.20.
Limitations on the accuracy of these sensors are based on the minimum
observation time — and thus the minimum distance observable, the temporal
accuracy (or quantization) of the receiver, and the temporal width of the
laser pulse. Such limitations are not only of a technological nature. In many
cases, cost is the limiting factor of these measurement devices. For instance, to
obtain 1 mm resolution, a time accuracy of about 3 ps, which can be achieved
only by using rather expensive technology.
Many time-of-ﬂight sensors used have what is called an ambiguity interval . The sensor emits pulses of light periodically, and computes an average
target distance from the time of the returning pulses. Typically, to simplify
the detection electronics of these sensors, the receiver only accepts signals
that arrive within time Δt, but this time window might also observe previous
pulses reﬂected by more distant surfaces. This means that a measurement is
ambiguous to the multiple of 12 cΔt, where c is the speed of light. Typical
values of 12 cΔt are 20–40 m.
In certain conditions, suitable algorithms can be employed to recover the
true depth by assuming that the distances should be changing smoothly.
The time-of-ﬂight sensors transmit only a single beam, thus range measurements are only obtained from a single surface point. In order to obtain
more information, the range data is usually supplied as a vector of range to
surfaces lying in a plane or as an image. To obtain these denser representations, the laser beam is swept across the scene. Normally the beam is swept
by a set of mirrors rather than moving the laser and detector themselves —
mirrors are lighter and less prone to motion damage.
Typical time-of-ﬂight sensors suitable for mobile robotics applications have
a range of 5–100 m, an accuracy of 5–10 mm, and a frequency of data acquisition per second of 1000–25000 Hz.

224

5 Actuators and Sensors

Fig. 5.21. Triangulation laser sensor operating principle

The operating principle of triangulation laser sensors 3 is illustrated in
Fig. 5.21.
The laser beam emitted by a photodiode is projected onto the observed
surface. The reﬂected beam is focused on a CCD sensor by means of a suitable
lens. Obviously, reﬂection must be diﬀused. The position of the focused beam
reﬂected to the receiver gives rise to a signal which is proportional to the
distance of the transmitter from the object. In fact, from the measurement
of the CCD sensor it is possible to resort to the angle at which the reﬂected
energy hits the sensor. Once the relative position and orientation of the CCD
sensor with respect to the photodiode are known, as e.g. through a suitable
calibration procedure, it is possible to compute the distance from the object
with simple geometry.
Accuracy can be inﬂuenced by certain object surfaces not favouring reﬂection, diﬀerences or changes of colour. Such occurrences can be mitigated or
even eliminated with modern electronic technology and automatic regulation
of light intensity.
The possibility of controlling the laser beam light brings the following
advantages:

3

The triangulation method is based on the trigonometric properties of triangles
and in particular on the cosine theorem. The method allows the computation
of the distance between two non-directly accessible points, i.e., once two angles
and one side of a triangle are known, it is possible to determine the other two
sides. For the case at issue, one side is given by the distance between the emitter
(laser) and the receiver (the CCD sensor), one angle is given by the orientation
of the emitter with respect to that side and the other angle can be computed
from the position of the laser beam on the image plane. In practice, it is not easy
to compute the above quantities, and suitable calibration techniques are to be
employed which avoid such computation to determine the distance measurement.

5.4 Exteroceptive Sensors

225

Fig. 5.22. Schematic representation of a vision system

• If the laser beam wavelength is known, e.g. that of the visible red 670 nm,
highly selective ﬁlters can be used which are set to the same frequency to
reduce the eﬀects of other light sources.
• The laser beam may be remodelled through lenses and mirrors so as to
create multiple beams or laser strips to measure multiple 3D points simultaneously.
• The direction of the laser beam can be controlled directly by the control
system to observe selectively only those portions of the scene of interest.
The main limitations of this type of sensors are the potential eye safety
risks from the power of lasers, particularly when invisible laser frequencies
are used (commonly infrared), as well as the false specular reﬂections from
metallic and polished objects.
5.4.3 Vision Sensors
The task of a camera as a vision sensor is to measure the intensity of the
light reﬂected by an object. To this end, a photosensitive element, termed
pixel (or photosite), is employed, which is capable of transforming light energy
into electric energy. Diﬀerent types of sensors are available depending on the
physical principle exploited to realize the energy transformation. The most
widely used devices are CCD and CMOS sensors based on the photoelectric
eﬀect of semiconductors.
CCD
A CCD (Charge Coupled Device) sensor consists of a rectangular array of photosites. Due to the photoelectric eﬀect, when a photon hits the semiconductor
surface, a number of free electrons are created, so that each element accumulates a charge depending on the time integral of the incident illumination over
the photosensitive element. This charge is then passed by a transport mechanism (similar to an analog shift register) to the output ampliﬁer, while at

226

5 Actuators and Sensors

Fig. 5.23. Perspective transformation

the same time the photosite is discharged. The electric signal is to be further
processed in order to produce the real video signal .
CMOS
A CMOS (Complementary Metal Oxide Semiconductor) sensor consists of
a rectangular array of photodiodes. The junction of each photodiode is
precharged and it is discharged when hit by photons. An ampliﬁer integrated
in each pixel can transform this charge into a voltage or current level. The
main diﬀerence with the CCD sensor is that the pixels of a CMOS sensor are
non-integrating devices; after being activated they measure throughput, not
volume. In this manner, a saturated pixel will never overﬂow and inﬂuence a
neighboring pixel. This prevents the eﬀect of blooming, which indeed aﬀects
CCD sensors.
Camera
As sketched in Fig. 5.22, a camera is a complex system comprising several
devices other than the photosensitive sensor, i.e., a shutter , a lens and analog
preprocessing electronics. The lens is responsible for focusing the light reﬂected
by the object on the plane where the photosensitive sensor lies, called the
image plane.
With reference to Fig. 5.23, consider a frame Oc –xc yc zc attached to the
camera, whose location with respect to the base frame is identiﬁed by the
homogeneous transformation matrix T bc . Take a point of the object of coorT
dinates pc = [ pcx pcy pcz ] ; typically, the centroid of the object is chosen.
Then, the coordinate transformation from the base frame to the camera frame
is described as
pc = T cb p,
(5.35)

5.4 Exteroceptive Sensors

227

where p denotes the object position with respect to the base frame and homogeneous representations of vectors have been used.
A reference frame can be introduced on the image plane, whose axes X
and Y are parallel to the axes xc and yc of the camera frame, and the origin is
at the intersection of the optical axis with the image plane, termed principal
point. Due to the refraction phenomenon, the point in the camera frame is
transformed into a point in the image plane via the perspective transformation,
i.e.,
f pcx
pcz
f pcy
Yf = − c
pz

Xf = −

where (Xf , Yf ) are the new coordinates in the frame deﬁned on the image
plane, and f is the focal length of the lens. Notice that these coordinates are
expressed in metric units and the above transformation is singular at pcz = 0.
The presence of the minus sign in the equations of the perspective transformation is consistent with the fact that the image of an object appears upside
down on the image plane of the camera. Such an eﬀect can be avoided, for
computational ease, by considering a virtual image plane positioned before
the lens, in correspondence of the plane zc = f of the camera frame. In this
way, the model represented in Fig. 5.24 is obtained, which is characterized by
the frontal perspective transformation
f pcx
pcz
f pcy
Yf = c
pz

Xf =

(5.36)
(5.37)

where, with abuse of notation, the name of the variables on the virtual plane
has not been changed.
These relationships hold only in theory, since the real lenses are always
aﬀected by imperfections, which cause image quality degradation. Two types
of distortions can be recognized, namely, aberrations and geometric distortion.
The former can be reduced by restricting the light rays to a small central
region of the lens; the eﬀects of the latter can be compensated on the basis of
a suitable model whose parameters are to be identiﬁed.
A visual information is typically elaborated by a digital processor, and
thus the measurement principle is to transform the light intensity I(X, Y ) of
each point in the image plane into a number. It is clear that a spatial sampling
is needed since an inﬁnite number of points in the image plane exist, as well
as a temporal sampling since the image can change during time. The CCD or
CMOS sensors play the role of spatial samplers, while the shutter in front of
the lens plays the role of the temporal sampler.

228

5 Actuators and Sensors

Fig. 5.24. Frontal perspective transformation

The spatial sampling unit is the pixel, and thus the coordinates (X, Y ) of a
point in the image plane are to be expressed in pixels, i.e., (XI , YI ). Due to the
photosite ﬁnite dimensions, the pixel coordinates of the point are related to
the coordinates in metric units through two scale factors αx and αy , namely,
αx f pcx
+ X0
pcz
αy f pcy
+ Y0 ,
YI =
pcz

XI =

(5.38)
(5.39)

where X0 and Y0 are the oﬀsets which take into account the position of the
origin of the pixel coordinate system with respect to the optical axis. This
nonlinear transformation can be written in a linear form by resorting to the
homogeneous representation of the point (xI , yI , zI ) via the relationships
xI
λ
yI
YI =
λ

XI =

where λ > 0. As a consequence, (5.38), (5.39) can be rewritten as
⎡ c⎤
⎡ ⎤
⎡
⎤
px
xI
XI
c
⎢
⎣ yI ⎦ = λ ⎣ YI ⎦ = ΩΠ ⎣ pyc ⎥
⎦
pz
λ
1
1
where

⎡

f αx
Ω=⎣ 0
0
⎡
1 0
Π = ⎣0 1
0 0

0
f αy
0

⎤
X0
Y0 ⎦
1
⎤

0 0
0 0⎦.
1 0

(5.40)

(5.41)

(5.42)

5.4 Exteroceptive Sensors

229

At this point, the overall transformation from the Cartesian space of the
observed object to the image space of its image in pixels is characterized by
composing the transformations in (5.35), (5.40) as
Ξ = ΩΠT cb

(5.43)

which represents the so-called camera calibration matrix. It is worth pointing
out that such a matrix contains intrinsic parameters (αx , αy , X0 , Y0 , f ) in Ω
depending on the sensor and lens characteristics as well as extrinsic parameters in T bc depending on the relative position and orientation of the camera
with respect to the base frame. Several calibration techniques exist to identify these parameters in order to compute the transformation between the
Cartesian space and the image space as accurately as possible.
If the intrinsic parameters of a camera are known, from a computationally viewpoint, it is convenient to refer to the normalized coordinates (X, Y ),
deﬁned by the normalized perspective transformation
⎡ c⎤
⎡ ⎤
px
X
⎢ pcy ⎥
⎥
(5.44)
λ⎣ Y ⎦ = Π ⎢
⎣ pc ⎦ .
z
1
1
These coordinates are deﬁned in metrical units and coincide with the coordinates (5.36), (5.37) in the case when f = 1. Comparing (5.40) with (5.44)
yields the invertible transformation
⎡
⎤
⎡ ⎤
XI
X
⎣ YI ⎦ = Ω ⎣ Y ⎦
(5.45)
1
1
relating the normalized coordinates to those expressed in pixels through the
matrix of intrinsic parameters.
If a monochrome CCD camera4 is of concern, the output ampliﬁer of the
sensor produces a signal which is processed by a timing analog electronics
in order to generate an electric signal according to one of the existing video
standards, i.e., the CCIR European and Australian standard, or the RS170
American and Japanese standard. In any case, the video signal is a voltage of
1 V peak-to-peak whose amplitude represents sequentially the image intensity.
The entire image is divided into a number of lines (625 for the CCIR
standard and 525 for the RS170 standard) to be sequentially scanned. The
raster scan proceeds horizontally across each line and each line from top to
bottom, but ﬁrst all the even lines, forming the ﬁrst ﬁeld , and then all the odd
lines, forming the second ﬁeld , so that a frame is composed of two successive
4

Colour cameras are equipped with special CCDs sensitive to three basic colours
(RGB); the most sophisticated cameras have three separate sensors, one per each
basic colour.

230

5 Actuators and Sensors

ﬁelds. This technique, called interlacing, allows the image to be updated either
at frame rate or at ﬁeld rate; in the former case the update frequency is that
of the entire frame (25 Hz for the CCIR standard and 30 Hz for the RS170
standard), while in the latter case the update frequency can be doubled as
long as half the vertical resolution can be tolerated.
The last step of the measurement process is to digitize the analog video
signal. The special analog-to-digital converters adopted for video signal acquisition are called frame grabbers. By connecting the output of the camera to the
frame grabber, the video waveform is sampled and quantized and the values
stored in a two-dimensional memory array representing the spatial sample of
the image, known as framestore; this array is then updated at ﬁeld or frame
rate.
In the case of CMOS cameras (currently available only for monochrome
images), thanks to CMOS technology which allows the integration of the
analog-to-digital converter in each pixel, the output of the camera is directly
a two-dimensional array, whose elements can be accessed randomly. Such advantage, with respect to CCD cameras, leads to the possibility of higher frame
rates if only parts of the entire frame are accessed.
The sequence of steps from image formation to image acquisition described
above can be classiﬁed as a process of low-level vision; this includes the extraction of elementary image features, e.g., centroid and intensity discontinuities.
On the other hand, a robotic system can be considered really autonomous
only if procedures for emulating cognition are available, e.g., recognizing an
observed object among a set of CAD models stored into a data base. In this
case, the artiﬁcial vision process can be referred to as high-level vision.

Bibliography
Scientiﬁc literature on actuating systems and sensors is wide and continuously
updated. The mechanical aspects on the joint actuating systems can be probed
further in e.g. [186]. Details about electric servomotors can be found in [22],
while in [156] construction and control problems for hydraulic motors are
extensively treated. Control of electric drives is discussed in [128]; for direct
drives see [12]. Joint control problems are discussed in [89].
A wide and detailed survey on sensors and in particular on proprioceptive
sensors is given in [81]. In [220] force sensors are accurately described, with
special attention to wrist force sensors. Further details about range sensors,
with reference to mobile robotics applications, are available in [210]. Finally,
a general introduction on vision sensors is contained in [48], while in [233] one
of the most common calibration techniques for vision systems is described.

Problems

231

Problems
5.1. Prove (5.7)–(5.10).
5.2. Consider the DC servomotor with the data: Im = 0.0014 kg·m2 , Fm =
0.01 N·m·s/rad, La = 2 mH, Ra = 0.2 ohm, kt = 0.2 N·m/A, kv = 0.2 V·s/rad,
Ci Gv = 1, Tv = 0.1 ms, ki = 0. Perform a computer simulation of the current
and velocity response to a unit step voltage input Vc . Adopt a sampling time
of 1 ms.
5.3. For the servomotor of the previous problem, design the controller of the
current loop Ci (s) so that the current response to a unit step voltage input
Vc is characterized by a settling time of 2 ms. Compare the velocity response
with that obtained in Problem 5.2.
5.4. Find the control voltage/output position and reaction torque/output position transfer functions for the scheme of Fig. 5.6.
5.5. For a Gray-code optical encoder, ﬁnd the interconversion logic circuit
which yields a binary-coded output word.
5.6. With reference to a contact situation of the kind illustrated in Fig. 5.16,
let
⎡
⎤
0 0 1
r ccs = [ −0.3 0 0.2 ]T m
Rcs = ⎣ 0 −1 0 ⎦
1 0 0
and let the force sensor measurement be
f ss = [ 20

0

0 ]T N

μss = [ 0

6

0 ]T N·m.

Compute the equivalent force and moment in the contact frame.
5.7. Consider the SCARA manipulator in Fig. 2.34 with link lengths a1 =
a2 = 0.5 m. Let the base frame be located at the intersection between the
ﬁrst link and the base link with axis z pointing downward and axis x in
the direction of the ﬁrst link when ϑ1 = 0. Assume that a CCD camera
is mounted on the wrist so that the camera frame is aligned with the endeﬀector frame. The camera parameters are f = 8 mm, αx = 79.2 pixel/mm,
αy = 120.5 pixel/mm, X0 = 250, Y0 = 250. An object is observed by the
camera and is described by the point of coordinates p = [ 0.8 0.5 0.9 ]T m.
Compute the pixel coordinates of the point when the manipulator is at the
conﬁguration q = [ 0 π/4 0.1 0 ]T .

6
Control Architecture

This chapter is devoted to presenting a reference model for the functional
architecture of an industrial robot’s control system. The hierarchical structure
and its articulation into functional modules allows the determination of the
requirements and characteristics of the programming environment and the
hardware architecture. The architecture refers to robot manipulators, yet its
articulation in levels also holds for mobile robots.

6.1 Functional Architecture
The control system to supervise the activities of a robotic system should be
endowed with a number of tools providing the following functions:
• capability of moving physical objects in the working environment, i.e.,
manipulation ability;
• capability of obtaining information on the state of the system and working
environment, i.e., sensory ability;
• capability of exploiting information to modify system behaviour in a preprogrammed manner, i.e., intelligence ability;
• capability of storing, elaborating and providing data on system activity,
i.e., data processing ability.
An eﬀective implementation of these functions can be obtained by means
of a functional architecture which is thought of as the superposition of several
activity levels arranged in a hierarchical structure. The lower levels of the
structure are oriented to physical motion execution, whereas the higher levels
are oriented to logical action planning. The levels are connected by data ﬂows;
those directed towards the higher levels regard measurements and/or results
of actions, while those directed towards the lower levels regard transmission
of directions.
With reference to the control system functions implementing management
of the above listed system activities, in general it is worth allocating three

234

6 Control Architecture

functional models at each level. A ﬁrst module is devoted to sensory data
management (sensory module). A second module is devoted to provide knowledge of the relevant world (modelling module). A third module is devoted to
decide the policy of the action (decision module).
More speciﬁcally, the sensory modules acquire, elaborate, correlate and
integrate sensory data in time and space, in order to recognize and measure
the system state and environment characteristic; clearly, the functions of each
module are oriented to the management of the relevant sensory data for that
level.
On the other hand, the modelling modules contain models derived on the
basis of a priori knowledge of system and environment; these models are updated by the information coming from the sensory modules, while the activation of the required functions is entrusted to the decision modules.
Finally, the decision modules perform breakdown of high-level tasks into
low-level actions; such task breakdown concerns both breakdown in time of
sequential actions and breakdown in space of concurrent actions. Each decision
module is entrusted with the functions concerning management of elementary
action assignments, task planning and execution.
The functions of a decision module characterize the level of the hierarchy
and determine the functions required to the modelling and sensory modules
operating at the same level. This implies that the contents of these two modules do not uniquely allow the determination of the hierarchical level, since
the same function may be present at more levels depending on the needs of
the decision modules at the relative levels.
The functional architecture needs an operator interface at each level of the
hierarchy, so as to allow an operator to perform supervisory and intervention
functions on the robotic system.
The instructions imparted to the decision module at a certain level may
be provided either by the decision module at the next higher level or by
the operator interface, or else by a combination of the two. Moreover, the
operator, by means of suitable communication tools, can be informed on the
system state and thus can contribute his/her own knowledge and decisions to
the modelling and sensory modules.
In view of the high data ﬂow concerning the exchange of information between the various levels and modules of the functional architecture, it is worth
allocating a shared global memory which contains the updated estimates on
the state of the whole system and environment.
The structure of the reference model for the functional architecture is
represented in Fig. 6.1, where the four hierarchical levels potentially relevant
for robotic systems in industrial applications are illustrated. Such levels regard
deﬁnition of the task , its breakdown into elementary actions, assignment of
primitives to the actions, and implementation of control actions on the servomanipulator. In the following, the general functions of the three modules at
each level are described.

6.1 Functional Architecture

235

Fig. 6.1. Reference model for a control system functional architecture

At the task level , the user speciﬁes the task which the robotic system
should execute; this speciﬁcation is performed at a high level of abstraction.
The goal of the desired task is analyzed and broken down into a sequence of
actions which are coordinated in space and time and allow implementation of
the task. The choice of actions is performed on the basis of knowledge models
as well as of the scene of interest for the task. For instance, consider the
application of a robot installed in an assembly line which is required to perform
a speciﬁc assembly task. To deﬁne the elementary actions that have to be
transmitted to the decision module at the next lower level, the decision module
should consult its knowledge base available in the modelling module, e.g., type
of assembly, components of the object to assembly, assembly sequence, and
choice of tools. This knowledge base should be continuously updated by the
information provided by the sensory module concerning location of the parts
to assembly; such information is available by means of a high-level vision
system operating in a scarcely structured environment, or else by means of
simple sensors detecting the presence of an object in a structured environment.
At the action level , the symbolic commands coming from the task level
are translated into sequences of intermediate conﬁgurations which characterize a motion path for each elementary action. The choice of the sequences is
performed on the basis of models of the manipulator and environment where
the action is to take place. With reference to one of the actions generated by
the above assembly task, the decision module chooses the most appropriate
coordinate system to compute manipulator’s end-eﬀector poses, by separating
translation from rotation if needed; it decides whether to operate in the joint
or operational space, it computes the path or via points, and for the latter it
deﬁnes the interpolation functions. By doing so, the decision module should
compare the sequence of conﬁgurations with a model of the manipulator as

236

6 Control Architecture

well as with a geometric description of the environment, which are both available in the modelling model. In this way, action feasibility is ascertained in
terms of obstacle-collision avoidance, motion in the neighbourhood of kinematically singular conﬁgurations, occurrence of mechanical joint limits, and
eventually utilization of available redundant DOFs. The knowledge base is updated by the information on the portion of scene where the single action takes
place which is provided by the sensory module, e.g., by means of a low-level
vision system or range sensors.
At the primitive level , on the basis of the sequence of conﬁgurations received by the action level, admissible motion trajectories are computed and
the control strategy is decided. The motion trajectory is interpolated so as to
generate the references for the servo level. The choice of motion and control
primitives is conditioned by the features of the mechanical structure and its
degree of interaction with the environment. Still with reference to the above
case study, the decision module computes the geometric path and the relative trajectory on the basis of the knowledge of the manipulator dynamic
model available in the modelling module. Moreover, it deﬁnes the type of control algorithm, e.g., decentralized control, centralized control, or interaction
control; it speciﬁes the relative gains; and it performs proper coordinate transformations, e.g., kinematic inversion if needed. The sensory module provides
information on the occurrence of conﬂicts between motion planning and execution, by means of, e.g., force sensors, low-level vision systems and proximity
sensors.
At the servo level , on the basis of the motion trajectories and control
strategies imparted by the primitive level, control algorithms are implemented
which provide the driving signals to the joint servomotors. The control algorithm operates on error signals between the reference and the actual values
of the controlled quantities, by utilizing knowledge of manipulator dynamic
model, and of kinematics if needed. In particular, the decision module performs a microinterpolation on the reference trajectory to exploit fully the
dynamic characteristic of the drives; it computes the control law, and it generates the (voltage or current) signals for controlling the speciﬁc drives. The
modelling module elaborates the terms of the control law depending on the
manipulator current conﬁguration and pass them to the decision module; such
terms are computed on the basis of knowledge of manipulator dynamic model.
Finally, the sensory module provides measurements of the proprioceptive sensors (position, velocity and contact force if needed); these measurements are
used by the decision module to compute the servo errors and, if required,
by the modelling module to update the conﬁguration-dependent terms in the
model.
The speciﬁcation of the functions associated with each level points out that
the implementation of such functions should be performed at diﬀerent time
rates, in view of their complexity and requirements. On one hand, the functions associated with the higher levels are not subject to demanding real-time
constraints, since they regard planning activities. On the other hand, their

6.1 Functional Architecture

237

Fig. 6.2. Hierarchical levels of a functional architecture for industrial robots

complexity is notable, since scheduling, optimization, resource management
and high-level sensory system data processing are required to update complex
models.
At the lowest level, demanding real-time operation prevails in order to
obtain high dynamic performance of the mechanical structure. The above
remarks lead to the conclusion that, at the servo level, it is necessary to
provide the driving commands to the motors and to detect the proprioceptive
sensory data at sampling rates of the order of the millisecond, while sampling
rates of the order of the minute are admissible at the task level.
With respect to this reference model of functional architecture, current
industrial robot’s control systems are not endowed with all the functions illustrated, because of both technology and cost limitations. In this regard,
the task level is not implemented at all since there do not yet exist eﬀective
and reliable application software packages allowing support of the complex
functions required at this level.
It is worth characterizing those functional levels of the reference models
which are typically implemented in advanced industrial robot’s control systems. The details of Fig. 6.2 show what follows:

238

6 Control Architecture

• The modelling and sensory modules are always present at the lowest level,
because of demanding requirements at the servo level for high dynamic
performance robots to be employed even in relatively simple applications.
• At the primitive level, the modelling module is usually present while the
sensory module is present only in a reduced number of applications that
require robot interaction with a less structured environment.
• At the action level, the decision module is present only as an interpreter of
the high-level commands imparted by the operator. All the task breakdown
functions are entrusted to the operator, and thus the modelling and sensory
module are absent at this level. Possible checking of action feasibility is
moved down to the primitive level where a modelling module exists.
In view of the highly-structured reference model of functional architecture
illustrated above, evolution of the control system towards more and more powerful capabilities is possible. In fact, one may foresee that information technology progress may allow the addition of hierarchically higher levels than the
task level. These should functionally characterize complex tasks to be broken down into elementary tasks and yet, at an even higher level, missions to
be broken down into complex tasks. A six-level hierarchical structure of the
above kind has been proposed as the reference model for the functional architecture of the control system of a service robotic system for space applications
(NASREM). In this framework, one may allocate the functions required to advanced robotics systems devoted to ﬁeld or service applications, as discussed
in Sect. 1.4.

6.2 Programming Environment
Programming a robotic system requires deﬁnition of a programming environment supported by suitable languages, which allows the operator imparting
the task directions that the robot should execute. The programming environment is entrusted not only with the function of translating statements by
means of a suitable language, but also with the function of checking correct
execution of a task being executed by the robot. Therefore, robot programming environments, besides having some features in common with computer
programming environments, present a number of issues related to the observation that program execution produces eﬀects on the physical world. In other
words, even if a very accurate description of physical reality is available in
the programming environment, a number of situations will unavoidably occur
which have not been or cannot be predicted.
As a consequence, a robot programming environment should be endowed
with the following features:
• real-time operating system,
• world modelling,
• motion control,

6.2 Programming Environment

•
•
•
•
•

239

sensory data reading,
interaction with physical system,
error detection capability,
recovery of correct operational functions,
speciﬁc language structure.

Therefore, the requirements on a programming environment may naturally stem from the articulation into models of the preceding reference model
of functional architecture. Such an environment will be clearly conditioned
by the level of the architecture at which operator access is allowed. In the
following, the requirements imposed on the programming environment by the
functions respectively characterizing the sensory, modelling and decision modules are presented with reference to the hierarchical levels of the functional
architecture.
Sensory data handling is the determining factor which qualiﬁes a programming environment. At the servo level, real-time proprioceptive sensory data
conditioning is required. At the primitive level, sensory data have to be expressed in the relevant reference frames. At the action level, geometric features
of the objects interested to the action have to be extracted by high-level sensory data. At the task level, tools allowing recognition of the objects present
in the scene are required.
The ability of consulting knowledge models is a support for a programming
environment. At the servo level, on-line numerical computation of the models
utilized by control algorithms is to be performed on the basis of sensory data.
At the primitive level, coordinate transformations have to be operated. At
the action level, it is crucial to have tools allowing system simulation and
CAD modelling of elementary objects. At the task level, the programming
environment should assume the functions of an expert system.
Decision functions play a fundamental role in a programming environment, since they allow the deﬁnition of the ﬂow charts. At the servo level,
on-line computation ability is required to generate the driving signals for the
mechanical system. At the primitive level, logic conditioning is to be present.
At the action level, process synchronization options should be available in
order to implement nested loops, parallel computation and interrupt system.
At the task level, the programming environment should allow management of
concurrent processes, and it should be endowed with tools to test for, locate
and remove mistakes from a program (debuggers) at a high-interactive level.
The evolution of programming environments has been conditioned by
technology development of computer science. An analysis of this evolution
leads to ﬁnding three generations of environments with respect to their functional characteristics, namely, teaching-by-showing, robot-oriented programming, and object-oriented programming. In the evolution of the environments,
the next generation usually incorporates the functional characteristics of the
previous generation.

240

6 Control Architecture

This classiﬁcation regards those features of the programming environment
relative to the operator interface, and thus it has a direct correspondence
with the hierarchical levels of the reference model of functional architecture.
The functions associated with the servo level lead to understanding that a
programming environment problem does not really exist for the operator.
In fact, low-level programming concerns the use of traditional programming
languages (Assembly, C) for development of real-time systems. The operator
is only left with the possibility of intervening by means of simple command
actuation (point-to-point, reset), reading of proprioceptive sensory data, and
limited editing capability.
6.2.1 Teaching-by-Showing
The ﬁrst generation has been characterized by programming techniques of
teaching-by-showing type. The operator guides the manipulator manually or
by means of a teach pendant along the desired motion path. During motion
execution, the data read by joint position transducers are stored and thus
they can be utilized later as references for the joint drive servos; in this way,
the mechanical structure is capable of executing (playing back) the motion
taught by a direct acquisition on the spot.
The programming environment does not allow implementation of logic
conditioning and queuing, and thus the associated computational hardware
plays elementary functions. The operator is not required to have special programming skill, and thus he/she can be a plant technician. The set-up of a
working program obviously requires the robot to be available to the operator
at the time of teaching, and thus the robot itself has to be taken oﬀ production. Typical applications that can be solved by this programming technique
include spot welding, spray painting and, in general, simple palletizing.
With regard to the reference model of functional architecture, a programming environment based on the teaching-by-showing technique allows operator access at the primitive level.
The drawbacks of such an environment may be partially overcome by the
adoption of simple programming languages which allow:
• the acquisition of a meaningful posture by teaching,
• the computation of the end-eﬀector pose with respect to a reference frame,
by means of a direct kinematics transformation,
• the assignment of a motion primitive and the trajectory parameters (usually, velocity as a percentage of the maximum velocity),
• the computation of the servo references, by means of an inverse kinematics
transformation,
• the teaching sequences to be conditioned to the use of simple external
sensors (presence of an object at the gripper),
• the correction of motion sequences by using simple text editors,
• simple connections to be made between subsets of elementary sequences.

6.2 Programming Environment

241

Providing a teaching-by-showing environment with the the above-listed
functions can be framed as an attempt to develop a structured programming
environment.
6.2.2 Robot-oriented Programming
Following the advent of eﬃcient low-cost computational means, robot-oriented
programming environments have been developed. The need for interaction
of the environment with physical reality has imposed integration of several
functions, typical of high-level programming languages (BASIC, PASCAL),
with those speciﬁcally required by robotic applications. In fact, many robotoriented languages have retained the teaching-by-showing programming mode,
in view of its natural characteristic of accurate interface with the physical
world.
Since the general framework is that of a computer programming environment, two alternatives have been considered:
• to develop ad hoc languages for robotic applications,
• to develop robot program libraries supporting standard programming languages.
The current situation features the existence of numerous new proprietary
languages, whereas it would be desirable to develop either robotic libraries
to be used in the context of consolidated standards, or new general-purpose
languages for industrial automation applications.
Robot-oriented languages are structured programming languages which incorporate high-level statements and have the characteristic of an interpreted
language, in order to obtain an interactive environment allowing the programmer to check the execution of each source program statement before
proceeding to the next one. Common features of such languages are:
•
•
•
•
•
•
•
•
•
•

text editor,
complex data representation structures,
extensive use of predeﬁned state variable,
execution of matrix algebra operations,
extensive use of symbolic representations for coordinate frames,
possibility to specify the coordinated motion of more frames rigidly attached to objects by means of a single frame,
inclusion of subroutines with data and parameter exchange,
use of logic conditioning and queuing by means of ﬂags,
capability of parallel computing,
functions of programmable logic controller (PLC).

With respect to the reference model of functional architecture, it can be
recognized that a robot-oriented programming environment allows operator
access at the action level.

242

6 Control Architecture

In view of the structured language characteristic, the operator in this case
should be an expert language programmer. Editing an application program
may be performed oﬀ line, i.e., without physical availability of the robot to the
operator; oﬀ-line programming demands a perfectly structured environment,
though. A robotic system endowed with a robot-oriented programming language allows execution of complex applications where the robot is inserted in
a work cell and interacts with other machines and devices to perform complex
tasks, such as part assembly.
Finally, a programming environment that allows access at the task level
of a reference model of functional architecture is characterized by an objectoriented language. Such an environment should have the capability of specifying a task by means of high-level statements allowing automatic execution of
a number of actions on the objects present in the scene. Robot programming
languages belonging to this generation are currently under development and
thus they are not yet available on the market. They can be framed in the ﬁeld
of expert systems and artiﬁcial intelligence.

6.3 Hardware Architecture
The hierarchical structure of the functional architecture adopted as a reference model for an industrial robot’s control system, together with its articulation into diﬀerent functional modules, suggests hardware implementation
which exploits distributed computational resources interconnected by means
of suitable communication channels. To this end, it is worth recalling that the
functions implemented in current control systems regard the three levels from
servo to action, with a typically limited development of the functions implemented at the action level. At the servo and primitive levels, computational
capabilities are required with demanding real-time constraints.
A general model of the hardware architecture for the control system of
an industrial robot is illustrated in Fig. 6.3. In this ﬁgure, proper boards
with autonomous computational capabilities have been associated with the
functions indicated in the reference model of functional architecture of Fig. 9.2.
The boards are connected to a bus, e.g., a VME bus, which allows support of
the communication data ﬂow; the bus bandwidth should be wide enough so
as to satisfy the requirements imposed by real-time constraints.
The system board is typically a CPU endowed with:
•
•
•
•
•

a microprocessor with mathematical coprocessor,
a bootstrap EPROM memory,
a local RAM memory,
a RAM memory shared with the other boards through the bus,
a number of serial and parallel ports interfacing the bus and the external
world,
• counters, registers and timers,

6.3 Hardware Architecture

243

Fig. 6.3. General model of the hardware architecture of an industrial robot’s control
system

• an interrupt system.
The following functions are to be implemented in the system board:
• operator interface through teach pendant, keyboard, video and printer,
• interface with an external memory (hard disk) used to store data and
application programs,
• interface with workstations and other control systems by means of a local
communication network, e.g., Ethernet,
• I/O interface with peripheral devices in the working area, e.g., feeders,
conveyors and ON/OFF sensors,
• system bootstrap,
• programming language interpreter,
• bus arbiter.
The other boards facing the bus may be endowed, besides the basic components of the system board, with a supplementary or alternative processor

244

6 Control Architecture

(DSP, Transputer) for implementation of computationally demanding or dedicated functions. With reference to the architecture in Fig. 6.3, the following
functions are implemented in the kinematics board:
•
•
•
•

computation of motion primitives,
computation of direct kinematics, inverse kinematics and Jacobian,
test for trajectory feasibility,
handling of kinematic redundancy.
The dynamics board is devoted to

• computation of inverse dynamics.
The servo board has the functions of:
•
•
•
•
•

microinterpolation of references,
computation of control algorithm,
digital-to-analog conversion and interface with power ampliﬁers,
handling of position and velocity transducer data,
motion interruption in case of malfunction.

The remaining boards in the ﬁgure have been considered for the sake of an
example to illustrate how the use of sensors may require local processing capabilities to retrieve signiﬁcant information from the given data which can be
eﬀectively used in the sensory system. The force board performs the following
operations:
• conditioning of data provided by the force sensor,
• representation of forces in a given coordinate frame.
The vision board is in charge of:
• processing data provided by the camera,
• extracting geometric features of the scene,
• localizing objects in given coordinate frames.
Although the boards face the same bus, the frequency at which data are
exchanged needs not to be the same for each board. Those boards connected
to the proprioceptive sensors indeed need to exchange date with the robot at
the highest possible frequency (from 100 to 1000 Hz) to ensure high dynamic
performance to motion control as well as to reveal end-eﬀector contact in a
very short time.
On the other hand, the kinematics and dynamics boards implement modelling functions and, as such, they do not require data update at a rate as high
as that required by the servo board. In fact, manipulator postures do not vary
appreciably in a very short time, at least with respect to typical operational
velocities and/or accelerations of current industrial robots. Common sampling
frequencies are in the range of 10 to 100 Hz.

Problems

245

Fig. 6.4. Object pick-and-place task

Also the vision board does not require a high update rate, both because the
scene is generally quasi-static, and because processing of interpretive functions
are typically complex. Typical frequencies are in the range of 1 to 10 Hz.
In summary, the board access to the communication bus of a hardware
control architecture may be performed according to a multirate logic which
allows the solution of bus data overﬂow problems.

Bibliography
The features of robot control architectures are presented in [230, 25]. The
NASREM architecture model has been proposed in [3]. For robot programming see [225, 139, 91]. More advanced control architectures based on artiﬁcial
intelligence concepts are discussed in [8, 158].

Problems
6.1. With reference to the situation illustrated in Fig. 6.4, describe the sequence of actions required from the manipulator to pick up an object at
location A and place it at location B.
6.2. For the situation of Problem 6.1, ﬁnd the motion primitives in the cases
of given via points and given path points.
6.3. The planar arm indicated in Fig. 6.5 is endowed with a wrist force sensor
which allows the measurement of the relevant force and moment components
for the execution of a peg-in-hole task. Draw the ﬂow chart for writing a
program to execute the described task.
6.4. A palletizing problem is represented in Fig. 6.6. Sixteen equal objects
have to be loaded on the pallet. The manipulator’s end-eﬀector has to pick

246

6 Control Architecture

Fig. 6.5. Peg-in-hole task

Fig. 6.6. Palletizing task of objects available on a conveyor

up the objects from a conveyor, whose feeding is commanded by the robot
in such a way that the objects are always found in the same location to be
picked. Write a PASCAL program to execute the task.

7
Dynamics

Derivation of the dynamic model of a manipulator plays an important role
for simulation of motion, analysis of manipulator structures, and design of
control algorithms. Simulating manipulator motion allows control strategies
and motion planning techniques to be tested without the need to use a physically available system. The analysis of the dynamic model can be helpful for
mechanical design of prototype arms. Computation of the forces and torques
required for the execution of typical motions provides useful information for
designing joints, transmissions and actuators. The goal of this chapter is to
present two methods for derivation of the equations of motion of a manipulator in the joint space. The ﬁrst method is based on the Lagrange formulation
and is conceptually simple and systematic. The second method is based on the
Newton–Euler formulation and yields the model in a recursive form; it is computationally more eﬃcient since it exploits the typically open structure of the
manipulator kinematic chain. Then, a technique for dynamic parameter identiﬁcation is presented. Further, the problems of direct dynamics and inverse
dynamics are formalized, and a technique for trajectory dynamic scaling is introduced, which adapts trajectory planning to the dynamic characteristics of
the manipulator. The chapter ends with the derivation of the dynamic model
of a manipulator in the operational space and the deﬁnition of the dynamic
manipulability ellipsoid .

7.1 Lagrange Formulation
The dynamic model of a manipulator provides a description of the relationship
between the joint actuator torques and the motion of the structure.
With Lagrange formulation, the equations of motion can be derived in
a systematic way independently of the reference coordinate frame. Once a
set of variables qi , i = 1, . . . , n, termed generalized coordinates, are chosen
which eﬀectively describe the link positions of an n-DOF manipulator, the

248

7 Dynamics

Lagrangian of the mechanical system can be deﬁned as a function of the
generalized coordinates:
L=T −U
(7.1)
where T and U respectively denote the total kinetic energy and potential
energy of the system.
The Lagrange equations are expressed by
∂L
d ∂L
−
= ξi
dt ∂ q̇i
∂qi

i = 1, . . . , n

(7.2)

where ξi is the generalized force associated with the generalized coordinate qi .
Equations (7.2) can be written in compact form as
d
dt



∂L
∂ q̇



T
−

∂L
∂q

T
=ξ

(7.3)

where, for a manipulator with an open kinematic chain, the generalized coordinates are gathered in the vector of joint variables q. The contributions to
the generalized forces are given by the nonconservative forces, i.e., the joint
actuator torques, the joint friction torques, as well as the joint torques induced
by end-eﬀector forces at the contact with the environment.1
The equations in (7.2) establish the relations existing between the generalized forces applied to the manipulator and the joint positions, velocities and
accelerations. Hence, they allow the derivation of the dynamic model of the
manipulator starting from the determination of kinetic energy and potential
energy of the mechanical system.

Example 7.1
In order to understand the Lagrange formulation technique for deriving the dynamic
model, consider again the simple case of the pendulum in Example 5.1. With reference to Fig. 5.8, let ϑ denote the angle with respect to the reference position of
the body hanging down (ϑ = 0). By choosing ϑ as the generalized coordinate, the
kinetic energy of the system is given by
T =

1 2 1
I ϑ̇ + Im kr2 ϑ̇2 .
2
2

The system potential energy, deﬁned at less than a constant, is expressed by
U = mg (1 − cos ϑ).
Therefore, the Lagrangian of the system is
L=
1

1 2 1
I ϑ̇ + Im kr2 ϑ̇2 − mg (1 − cos ϑ).
2
2

The term torque is used as a synonym of joint generalized force.

7.1 Lagrange Formulation

249

Substituting this expression in the Lagrange equation in (7.2) yields
(I + Im kr2 )ϑ̈ + mg sin ϑ = ξ.
The generalized force ξ is given by the contributions of the actuation torque τ at
the joint and of the viscous friction torques −F ϑ̇ and −Fm kr2 ϑ, where the latter has
been reported to the joint side. Hence, it is
ξ = τ − F ϑ̇ − Fm kr2 ϑ
leading to the complete dynamic model of the system as the second-order diﬀerential
equation
(I + Im kr2 )ϑ̈ + (F + Fm kr2 )ϑ̇ + mg sin ϑ = τ .
It is easy to verify how this equation is equivalent to (5.25) when reported to the
joint side.

7.1.1 Computation of Kinetic Energy
Consider a manipulator with n rigid links. The total kinetic energy is given
by the sum of the contributions relative to the motion of each link and the
contributions relative to the motion of each joint actuator:2
T =

n


(T i + Tmi ),

(7.4)

i=1

where T i is the kinetic energy of Link i and Tmi is the kinetic energy of the
motor actuating Joint i.
The kinetic energy contribution of Link i is given by
"
1
ṗ∗ T ṗ∗ ρdV ,
(7.5)
Ti=
2 Vi i i
where ṗ∗i denotes the linear velocity vector and ρ is the density of the elementary particle of volume dV ; V i is the volume of Link i.
Consider the position vector p∗i of the elementary particle and the position
vector pCi of the link centre of mass, both expressed in the base frame. One
has
(7.6)
r i = [ rix riy riz ]T = p∗i − p i
with

2

1
pi=
mi

"

p∗i ρdV

Vi

Link 0 is ﬁxed and thus gives no contribution.

(7.7)

250

7 Dynamics

Fig. 7.1. Kinematic description of Link i for Lagrange formulation

where m i is the link mass. As a consequence, the link point velocity can be
expressed as
ṗ∗i = ṗ i + ω i × r i

(7.8)

= ṗ i + S(ω i )r i ,
where ṗ i is the linear velocity of the centre of mass and ω i is the angular
velocity of the link (Fig. 7.1).
By substituting the velocity expression (7.8) into (7.5), it can be recognized
that the kinetic energy of each link is formed by the following contributions.
Translational
The contribution is
1
2

"
Vi

ṗTi ṗ i ρdV =

1
m ṗT ṗ .
2 i i i

(7.9)

Mutual
The contribution is
& "
'
&
'
"
1
1 T
T
∗
2
ṗ S(ω i )r i ρdV = 2
ṗ S(ω i )
(pi − p i )ρdV = 0
2 Vi i
2 i
Vi
since, by virtue of (7.7), it is
"
Vi

p∗i ρdV = p

"
i

ρdV .
Vi

7.1 Lagrange Formulation

251

Rotational
The contribution is
'
&"
"
1
1 T
T
T T
r S (ω i )S(ω i )r i ρdV = ω i
S (r i )S(r i )ρdV ω i
2 Vi i
2
Vi
where the property S(ω i )r i = −S(r i )ω i has been exploited. In view of the
expression of the matrix operator S(·)
⎤
⎡
0
−riz riy
S(r i ) = ⎣ riz
0
−rix ⎦ ,
−riy rix
0
it is

1
2

The matrix
I

i

⎡%

⎢
=⎣
⎡

I

"
r Ti S T (ω i )S(ω i )r i ρdV =
Vi

∗

%
− rix riy ρdV
% 2
2
(rix + riz
)ρdV

∗

∗

2
2
+ riz
)ρdV
(riy

i xx

⎢
=⎣ ∗
∗

−I
I

i xy

i yy

∗

−I

⎤

1 T
ω I ωi .
2 i i

(7.10)

%
⎤
− rix riz ρdV
%
⎥
− riy riz ρdV ⎦ (7.11)
% 2
2
(rix + riy
)ρdV

i xz

⎥
−I i yz ⎦ .
I i zz

is symmetric3 and represents the inertia tensor relative to the centre of mass
of Link i when expressed in the base frame. Notice that the position of Link i
depends on the manipulator conﬁguration and thus the inertia tensor, when
expressed in the base frame, is conﬁguration-dependent. If the angular velocity
of Link i is expressed with reference to a frame attached to the link (as in the
Denavit–Hartenberg convention), it is
ω ii = RTi ω i
where Ri is the rotation matrix from Link i frame to the base frame. When
referred to the link frame, the inertia tensor is constant. Let I i i denote such
tensor; then it is easy to verify the following relation:
I

i

= Ri I i i RTi .

(7.12)

If the axes of Link i frame coincide with the central axes of inertia, then the
inertia products are null and the inertia tensor relative to the centre of mass
is a diagonal matrix.
3

The symbol ‘∗’ has been used to avoid rewriting the symmetric elements.

252

7 Dynamics

By summing the translational and rotational contributions (7.9) and (7.10),
the kinetic energy of Link i is
1
1
m ṗT ṗ + ω T Ri I i i RTi ω i .
2 i i i 2 i

Ti=

(7.13)

At this point, it is necessary to express the kinetic energy as a function
of the generalized coordinates of the system, that are the joint variables. To
this end, the geometric method for Jacobian computation can be applied to
the intermediate link other than the end-eﬀector, yielding
ṗ

( )

i

( )

( )

(7.14)

( )
jOii q̇i

( )
J O i q̇,

(7.15)

= jP 1i q̇1 + . . . + jP ii q̇i = J P i q̇

ωi =

( )
jO1i q̇1

+ ... +

=

where the contributions of the Jacobian columns relative to the joint velocities
have been taken into account up to current Link i. The Jacobians to consider
are then:
!
( )
(7.16)
J P i = j(P 1i ) . . . j(P ii ) 0 . . . 0
!
( i)
( i)
( i)
J O = jO1 . . . jOi 0 . . . 0 ;
(7.17)
the columns of the matrices in (7.16) and (7.17) can be computed according
to (3.30), giving

z j−1
for a prismatic joint
( i)
jP j =
(7.18)
for a revolute joint
z j−1 × (p i − pj−1 )

( )
jOji

=

0
z j−1

for a prismatic joint
for a revolute joint.

(7.19)

where pj−1 is the position vector of the origin of Frame j − 1 and z j−1 is the
unit vector of axis z of Frame j − 1. It follows that the kinetic energy of Link
i in (7.13) can be written as
Ti=

1
1
( )T ( )
( )T
( )
m i q̇ T J P i J P i q̇ + q̇ T J O i Ri I i i RTi J O i q̇.
2
2

(7.20)

The kinetic energy contribution of the motor of Joint i can be computed
in a formally analogous way to that of the link. Consider the typical case of
rotary electric motors (that can actuate both revolute and prismatic joints by
means of suitable transmissions). It can be assumed that the contribution of
the ﬁxed part (stator) is included in that of the link on which such motor is
located, and thus the sole contribution of the rotor is to be computed.
With reference to Fig. 7.2, the motor of Joint i is assumed to be located
on Link i − 1. In practice, in the design of the mechanical structure of an open
kinematic chain manipulator one attempts to locate the motors as close as
possible to the base of the manipulator so as to lighten the dynamic load of

7.1 Lagrange Formulation

253

Fig. 7.2. Kinematic description of Motor i

the ﬁrst joints of the chain. The joint actuator torques are delivered by the
motors by means of mechanical transmissions (gears).4 The contribution of
the gears to the kinetic energy can be suitably included in that of the motor.
It is assumed that no induced motion occurs, i.e., the motion of Joint i does
not actuate the motion of other joints.
The kinetic energy of Rotor i can be written as
Tmi =

1
1
mmi ṗTmi ṗmi + ω Tmi I mi ω mi ,
2
2

(7.21)

where mmi is the mass of the rotor, ṗmi denotes the linear velocity of the
centre of mass of the rotor, I mi is the inertia tensor of the rotor relative to
its centre of mass, and ω mi denotes the angular velocity of the rotor.
Let ϑmi denote the angular position of the rotor. On the assumption of a
rigid transmission, one has
(7.22)
kri q̇i = ϑ̇mi
where kri is the gear reduction ratio. Notice that, in the case of actuation of
a prismatic joint, the gear reduction ratio is a dimensional quantity.
According to the angular velocity composition rule (3.18) and the relation (7.22), the total angular velocity of the rotor is
ω mi = ω i−1 + kri q̇i z mi

(7.23)

where ω i−1 is the angular velocity of Link i − 1 on which the motor is located,
and z mi denotes the unit vector along the rotor axis.
4

Alternatively, the joints may be actuated by torque motors directly coupled to
the rotation axis without gears.

254

7 Dynamics

To express the rotor kinetic energy as a function of the joint variables, it
is worth expressing the linear velocity of the rotor centre of mass — similarly
to (7.14) — as
(m )
(7.24)
ṗmi = J P i q̇.
The Jacobian to compute is then
(mi )

(mi )
= jP
1

JP

(m )

i
jP,i−1

...

whose columns are given by

z j−1
(mi )
jP j =
z j−1 × (pmi − pj−1 )

0 ...

0

!

(7.25)

for a prismatic joint
for a revolute joint

(7.26)

where pj−1 is the position vector of the origin of Frame j − 1. Notice that
(m )

jP i i = 0 in (7.25), since the centre of mass of the rotor has been taken along
its axis of rotation.
The angular velocity in (7.23) can be expressed as a function of the joint
variables, i.e.,
(m )
ω mi = J O i q̇.
(7.27)
The Jacobian to compute is then
(mi )

JO

(mi )
= jO1

...

(m )

i
jO,i−1

(m )

jOi i

0 ...

0

!

(7.28)

whose columns, in view of (7.23), (7.15), are respectively given by

( )
jOji
j = 1, . . . , i − 1
(mi )
jOj =
kri z mi
j = i.

(7.29)

To compute the second relation in (7.29), it is suﬃcient to know the components of the unit vector of the rotor rotation axis z mi with respect to the base
frame. Hence, the kinetic energy of Rotor i can be written as
1
1
(m )T (m )
(m )T
(mi )
T
i
mmi q̇ T J P i J P i q̇ + q̇ T J O i Rmi I m
q̇.
(7.30)
mi R mi J O
2
2
Finally, by summing the various contributions relative to the single links
(7.20) and single rotors (7.30) as in (7.4), the total kinetic energy of the
manipulator with actuators is given by the quadratic form
Tmi =

1
1 
bij (q)q̇i q̇j = q̇ T B(q)q̇
2 i=1 j=1
2
n

T =

n

(7.31)

where
B(q) =

n 


( )T

( )

( )T

( )

m i J P i J P i + J O i Ri I i i RTi J O i

(7.32)

i=1
(mi )T

+mmi J P

(mi )

JP

is the (n × n) inertia matrix which is:

(mi )T

+ JO

(mi )

T
i
R mi I m
mi R mi J O



7.1 Lagrange Formulation

255

• symmetric,
• positive deﬁnite,
• (in general) conﬁguration-dependent.
7.1.2 Computation of Potential Energy
As done for kinetic energy, the potential energy stored in the manipulator is
given by the sum of the contributions relative to each link as well as to each
rotor:
n

(U i + Umi ).
(7.33)
U=
i=1

On the assumption of rigid links, the contribution due only to gravitational
forces5 is expressed by
"
g T0 p∗i ρdV = −m i g T0 p i
(7.34)
Ui =−
Vi

where g 0 is the gravity acceleration vector in the base frame (e.g., g 0 =
[ 0 0 −g ]T if z is the vertical axis), and (7.7) has been utilized for the
coordinates of the centre of mass of Link i. As regards the contribution of
Rotor i, similarly to (7.34), one has
Umi = −mmi g T0 pmi .

(7.35)

By substituting (7.34), (7.35) into (7.33), the potential energy is given by
U =−

n


(m i g T0 p i + mmi g T0 pmi )

(7.36)

i=1

which reveals that potential energy, through the vectors p i and pmi is a
function only of the joint variables q, and not of the joint velocities q̇.
7.1.3 Equations of Motion
Having computed the total kinetic and potential energy of the system as
in (7.31), (7.36), the Lagrangian (7.1) for the manipulator can be written as
L(q, q̇) = T (q, q̇) − U(q).

(7.37)

Taking the derivatives required by Lagrange equations in (7.3) and recalling
that U does not depend on q̇ yields
B(q)q̈ + n(q, q̇) = ξ
5

(7.38)

In the case of link ﬂexibility, one would have an additional contribution due to
elastic forces.

256

7 Dynamics

where
1
n(q, q̇) = Ḃ(q)q̇ −
2




∂
q̇ T B(q)q̇
∂q



T
+

∂U(q)
∂q

T
.

In detail, noticing that U in (7.36) does not depend on q̇ and accounting
for (7.31) yields

 


n
n

d ∂L
dbij (q)
d ∂T
q̇j
bij (q)q̈j +
=
=
dt ∂ q̇i
dt ∂ q̇i
dt
j=1
j=1
=

n


bij (q)q̈j +

j=1

and

n 
n

∂bij (q)
j=1 k=1

∂qk

q̇k q̇j

∂T
1   ∂bjk (q)
=
q̇k q̇j
∂qi
2 j=1
∂qi
n

n

k=1

where the indices of summation have been conveniently switched. Further, in
view of (7.14), (7.24), it is

n 

∂p j
∂pmj
∂U
T
T
=−
+ mm j g 0
m j g0
(7.39)
∂qi
∂qi
∂qi
j=1
=−

n 



( )
(m )
m j g T0 jP ij (q) + mmj g T0 jP i j (q) = gi (q)

j=1

where, again, the index of summation has been changed.
As a result, the equations of motion are
n

j=1

bij (q)q̈j +

n 
n


hijk (q)q̇k q̇j + gi (q) = ξi

i = 1, . . . , n.

(7.40)

j=1 k=1

where
hijk =

∂bij
1 ∂bjk
−
.
∂qk
2 ∂qi

(7.41)

A physical interpretation of (7.40) reveals that:
• For the acceleration terms:
– The coeﬃcient bii represents the moment of inertia at Joint i axis,
in the current manipulator conﬁguration, when the other joints are
blocked.
– The coeﬃcient bij accounts for the eﬀect of acceleration of Joint j on
Joint j.
• For the quadratic velocity terms:
– The term hijj q̇j2 represents the centrifugal eﬀect induced on Joint i by
velocity of Joint j; notice that hiii = 0, since ∂bii /∂qi = 0.

7.2 Notable Properties of Dynamic Model

257

–

The term hijk q̇j q̇k represents the Coriolis eﬀect induced on Joint i by
velocities of Joints j and k.
• For the conﬁguration-dependent terms:
– The term gi represents the moment generated at Joint i axis of the
manipulator, in the current conﬁguration, by the presence of gravity.
Some joint dynamic couplings, e.g., coeﬃcients bij and hijk , may be reduced or zeroed when designing the structure, so as to simplify the control
problem.
Regarding the nonconservative forces doing work at the manipulator joints,
these are given by the actuation torques τ minus the viscous friction torques
F v q̇ and the static friction torques f s (q, q̇): F v denotes the (n × n) diagonal
matrix of viscous friction coeﬃcients. As a simpliﬁed model of static friction
torques, one may consider the Coulomb friction torques F s sgn (q̇), where F s
is an (n × n) diagonal matrix and sgn (q̇) denotes the (n × 1) vector whose
components are given by the sign functions of the single joint velocities.
If the manipulator’s end-eﬀector is in contact with an environment, a
portion of the actuation torques is used to balance the torques induced at
the joints by the contact forces. According to a relation formally analogous
to (3.111), such torques are given by J T (q)he where he denotes the vector of
force and moment exerted by the end-eﬀector on the environment.
In summary, the equations of motion (7.38) can be rewritten in the compact matrix form which represents the joint space dynamic model :
B(q)q̈ + C(q, q̇)q̇ + F v q̇ + F s sgn (q̇) + g(q) = τ − J T (q)he

(7.42)

where C is a suitable (n × n) matrix such that its elements cij satisfy the
equation
n
n 
n


cij q̇j =
hijk q̇k q̇j .
(7.43)
j=1

j=1 k=1

7.2 Notable Properties of Dynamic Model
In the following, two notable properties of the dynamic model are presented
which will be useful for dynamic parameter identiﬁcation as well as for deriving
control algorithms.
7.2.1 Skew-symmetry of Matrix Ḃ − 2C
The choice of the matrix C is not unique, since there exist several matrices C whose elements satisfy (7.43). A particular choice can be obtained by

258

7 Dynamics

elaborating the term on the right-hand side of (7.43) and accounting for the
expressions of the coeﬃcients hijk in (7.41). To this end, one has
n


cij q̇j =

j=1

=

n 
n


hijk q̇k q̇j
j=1 k=1
n 
n 

∂bij
j=1 k=1

1 ∂bjk
−
∂qk
2 ∂qi


q̇k q̇j .

Splitting the ﬁrst term on the right-hand side by an opportune switch of
summation between j and k yields

n
n
n
n
n 

1   ∂bij
1   ∂bik
∂bjk
cij q̇j =
q̇k q̇j +
−
q̇k q̇j .
2 j=1
∂qk
2 j=1
∂qj
∂qi
j=1
k=1

k=1

As a consequence, the generic element of C is
cij =

n


cijk q̇k

(7.44)

k=1

where the coeﬃcients
cijk =

1
2



∂bij
∂bik
∂bjk
+
−
∂qk
∂qj
∂qi


(7.45)

are termed Christoﬀel symbols of the ﬁrst type. Notice that, in view of the
symmetry of B, it is
(7.46)
cijk = cikj .
This choice for the matrix C leads to deriving the following notable property of the equations of motion (7.42). The matrix
N (q, q̇) = Ḃ(q) − 2C(q, q̇)

(7.47)

is skew-symmetric; that is, given any (n × 1) vector w, the following relation
holds:
(7.48)
wT N (q, q̇)w = 0.
In fact, substituting the coeﬃcient (7.45) into (7.44) gives

n
n 
1  ∂bij
1  ∂bik
∂bjk
cij =
q̇k +
−
q̇k
2
∂qk
2
∂qj
∂qi
k=1
k=1

n 
1
1  ∂bik
∂bjk
= ḃij +
−
q̇k
2
2
∂qj
∂qi
k=1

and then the expression of the generic element of the matrix N in (7.47) is

n 

∂bjk
∂bik
−
nij = ḃij − 2cij =
q̇k .
∂qi
∂qj
k=1

7.2 Notable Properties of Dynamic Model

259

The result follows by observing that
nij = −nji .
An interesting property which is a direct implication of the skew-symmetry
of N (q, q̇) is that, by setting w = q̇,
q̇ T N (q, q̇)q̇ = 0;

(7.49)

notice that (7.49) does not imply (7.48), since N is a function of q̇, too.
It can be shown that (7.49) holds for any choice of the matrix C, since it
is a result of the principle of conservation of energy (Hamilton). By virtue of
this principle, the total time derivative of kinetic energy is balanced by the
power generated by all the forces acting on the manipulator joints. For the
mechanical system at issue, one may write


1 d T
q̇ B(q)q̇ = q̇ T τ − F v q̇ − F s sgn (q̇) − g(q) − J T (q)he .
2 dt

(7.50)

Taking the derivative on the left-hand side of (7.50) gives
1 T
q̇ Ḃ(q)q̇ + q̇ T B(q)q̈
2
and substituting the expression of B(q)q̈ in (7.42) yields

 1
1 d T
q̇ B(q)q̇ = q̇ T Ḃ(q) − 2C(q, q̇) q̇
(7.51)
2 dt
2

+q̇ T τ − F v q̇ − F s sgn (q̇) − g(q) − J T (q)he .
A direct comparison of the right-hand sides of (7.50) and (7.51) leads to the
result established by (7.49).
To summarize, the relation (7.49) holds for any choice of the matrix C,
since it is a direct consequence of the physical properties of the system,
whereas the relation (7.48) holds only for the particular choice of the elements of C as in (7.44), (7.45).
7.2.2 Linearity in the Dynamic Parameters
An important property of the dynamic model is the linearity with respect to
the dynamic parameters characterizing the manipulator links and rotors.
In order to determine such parameters, it is worth associating the kinetic
and potential energy contributions of each rotor with those of the link on
which it is located. Hence, by considering the union of Link i and Rotor i + 1
(augmented Link i), the kinetic energy contribution is given by
Ti = T i + Tmi+1

(7.52)

260

7 Dynamics

where
Ti=

1
1
m i ṗTi ṗ i + ω Ti I i ω i
2
2

(7.53)

and

1
1
mmi+1 ṗTmi+1 ṗmi+1 + ω Tmi+1 I mi+1 ω mi+1 .
(7.54)
2
2
With reference to the centre of mass of the augmented link, the linear velocities
of the link and rotor can be expressed according to (3.26) as
Tmi+1 =

ṗ

i

ṗmi+1

= ṗCi + ω i × r Ci , i
= ṗCi + ω i × r Ci ,mi+1

(7.55)
(7.56)

= p i − pCi

(7.57)

with
r Ci ,

i

r Ci ,mi+1 = pmi+1 − pCi ,

(7.58)

where pCi denotes the position vector of the centre of mass of augmented
Link i.
Substituting (7.55) into (7.53) gives
T

i

=

1
m i ṗTCi ṗCi + ṗTCi S(ω i )m i r Ci , i
2
1
1
+ m i ω Ti S T (r Ci , i )S(r Ci , i )ω i + ω Ti I i ω i .
2
2

(7.59)

By virtue of Steiner theorem, the matrix
Ī

i

=I

i

+ m i S T (r Ci , i )S(r Ci , i )

(7.60)

represents the inertia tensor relative to the overall centre of mass pCi , which
contains an additional contribution due to the translation of the pole with
respect to which the tensor is evaluated, as in (7.57). Therefore, (7.59) can be
written as
Ti=

1
1
m i ṗTCi ṗCi + ṗTCi S(ω i )m i r Ci , i + ω Ti Ī i ω i .
2
2

(7.61)

In a similar fashion, substituting (7.56) into (7.54) and exploiting (7.23)
yields
Tmi+1 =

1
1
mmi+1 ṗTCi ṗCi + ṗTCi S(ω i )mmi+1 r Ci ,mi+1 + ω Ti Ī mi+1 ω i (7.62)
2
2
1 2
2
+kr,i+1 q̇i+1 z Tmi+1 I mi+1 ω i + kr,i+1
q̇i+1
z Tmi+1 I mi+1 z mi+1 ,
2

where
Ī mi+1 = I mi+1 + mmi+1 S T (r Ci ,mi+1 )S(r Ci ,mi+1 ).

(7.63)

7.2 Notable Properties of Dynamic Model

261

Summing the contributions in (7.61), (7.62) as in (7.52) gives the expression of the kinetic energy of augmented Link i in the form
Ti =

1
1
mi ṗTCi ṗCi + ω Ti Ī i ω i + kr,i+1 q̇i+1 z Tmi+1 I mi+1 ω i
2
2
1 2
2
q̇i+1
z Tmi+1 I mi+1 z mi+1 ,
+ kr,i+1
2

(7.64)

where mi = m i + mmi+1 and Ī i = Ī i + Ī mi+1 are respectively the overall
mass and inertia tensor. In deriving (7.64), the relations in (7.57), (7.58) have
been utilized as well as the following relation between the positions of the
centres of mass:
(7.65)
m i p i + mmi+1 pmi+1 = mi pCi .
Notice that the ﬁrst two terms on the right-hand side of (7.64) represent
the kinetic energy contribution of the rotor when this is still, whereas the
remaining two terms account for the rotor’s own motion.
On the assumption that the rotor has a symmetric mass distribution about
its axis of rotation, its inertia tensor expressed in a frame Rmi with origin at
the centre of mass and axis zmi aligned with the rotation axis can be written
as
⎤
⎡
Imi xx
0
0
i
⎣ 0
Im
(7.66)
0 ⎦
Imi yy
mi =
0
0
Imi zz
where Imi yy = Imi xx . As a consequence, the inertia tensor is invariant with
respect to any rotation about axis zmi and is, anyhow, constant when referred
to any frame attached to Link i − 1.
Since the aim is to determine a set of dynamic parameters independent of
the manipulator joint conﬁguration, it is worth referring the inertia tensor of
the link Ī i to frame Ri attached to the link and the inertia tensor I mi+1 to
frame Rmi+1 so that it is diagonal. In view of (7.66) one has
T
i+1
I mi+1 z mi+1 = Rmi+1 I m
mi+1 Rmi+1 z mi+1 = Imi+1 z mi+1

(7.67)

where Imi+1 = Imi+1 zz denotes the constant scalar moment of inertia of the
rotor about its rotation axis.
Therefore, the kinetic energy (7.64) becomes
Ti =

1
1 iT i i
i
iT
i
mi ṗiT
Ci ṗCi + ω i Ī i ω i + kr,i+1 q̇i+1 Imi+1 z mi+1 ω i
2
2
1 2
2
+ kr,i+1
q̇i+1
Imi+1 .
2

(7.68)

According to the linear velocity composition rule for Link i in (3.15), one
may write
(7.69)
ṗiCi = ṗii + ω ii × r ii,Ci ,

262

7 Dynamics

where all the vectors have been referred to Frame i; note that r ii,Ci is ﬁxed in
such a frame. Substituting (7.69) into (7.68) gives
Ti =

1
1 iT )i i
i
iT
i
i
mi ṗiT
i ṗi + ṗi S(ω i )mi r i,Ci + ω i I i ω i
2
2
1 2
iT
i
2
+kr,i+1 q̇i+1 Imi+1 z mi+1 ω i + kr,i+1 q̇i+1
Imi+1 ,
2

where

i
i
I)i = Ī i + mi S T (r ii,Ci )S(r ii,Ci )

(7.70)

(7.71)

represents the inertia tensor with respect to the origin of Frame i according
to Steiner theorem.
Let r ii,Ci = [ Ci x Ci y Ci z ]T . The ﬁrst moment of inertia is
⎤
mi Ci x
= ⎣ mi Ci y ⎦ .
mi Ci z
⎡

mi r ii,Ci

(7.72)

From (7.71) the inertia tensor of augmented Link i is
⎤
⎡¯
−I¯ixz − mi Ci x Ci z
Iixx + mi (2Ci y + 2Ci z ) −I¯ixy − mi Ci x Ci y
i
⎥
⎢
I)i = ⎣
∗
I¯iyy + mi (2Ci x + 2Ci z ) −I¯iyz − mi Ci y Ci z ⎦
∗
∗
I¯izz + mi (2 + 2 )
⎡)
Iixx
⎢
=⎣ ∗
∗

−I)ixy
I)iyy
∗

Ci x

⎤

−I)ixz
⎥
−I)iyz ⎦ .

Ci y

(7.73)

I)izz

Therefore, the kinetic energy of the augmented link is linear with respect to
the dynamic parameters, namely, the mass, the three components of the ﬁrst
moment of inertia in (7.72), the six components of the inertia tensor in (7.73),
and the moment of inertia of the rotor .
As regards potential energy, it is worth referring to the centre of mass of
augmented Link i deﬁned as in (7.65), and thus the single contribution of
potential energy can be written as
i
Ui = −mi g iT
0 p Ci

(7.74)

where the vectors have been referred to Frame i. According to the relation
piCi = pii + r ii,Ci .
The expression in (7.74) can be rewritten as
i
i
Ui = −g iT
0 (mi pi + mi r i,Ci )

(7.75)

7.2 Notable Properties of Dynamic Model

263

that is, the potential energy of the augmented link is linear with respect to
the mass and the three components of the ﬁrst moment of inertia in (7.72).
By summing the contributions of kinetic energy and potential energy for
all augmented links, the Lagrangian of the system (7.1) can be expressed in
the form
n

(β TT i − β TU i )π i
(7.76)
L=
i=1

where π i is the (11 × 1) vector of dynamic parameters
π i = [ mi mi Ci x mi Ci y mi Ci z I)ixx I)ixy I)ixz I)iyy I)iyz I)izz Imi ]T ,
(7.77)
in which the moment of inertia of Rotor i has been associated with the parameters of Link i so as to simplify the notation.
In (7.76), β T i and β U i are two (11 × 1) vectors that allow the Lagrangian to be written as a function of π i . Such vectors are a function
of the generalized coordinates of the mechanical system (and also of their
derivatives as regards β T i ). In particular, it can be shown that β T i =
β T i (q1 , q2 , . . . , qi , q̇1 , q̇2 , . . . , q̇i ) and β U i = β U i (q1 , q2 , . . . , qi ), i.e., they do not
depend on the variables of the joints subsequent to Link i.
At this point, it should be observed how the derivations required by the
Lagrange equations in (7.2) do not alter the property of linearity in the parameters, and then the generalized force at Joint i can be written as
ξi =

n


y Tij π j

(7.78)

j=1

where
y ij =

∂β T j
∂β U j
d ∂β T j
−
+
.
dt ∂ q̇i
∂qi
∂qi

(7.79)

Since the partial derivatives of β T j and β U j appearing in (7.79) vanish for
j < i, the following notable result is obtained:
⎡ ⎤ ⎡ y T11 y T12 . . . y T1n ⎤ ⎡ π 1 ⎤
ξ1
⎢
⎥
0T y T22 . . . y T2n ⎥
⎢ ξ2 ⎥ ⎢
⎥ ⎢ π2 ⎥
⎢ . ⎥=⎢
(7.80)
⎥⎢ . ⎥
.
.
.
.
⎣ . ⎦ ⎢
..
..
.. ⎦ ⎣ .. ⎦
⎣ ..
.
ξn
πn
0T 0T . . . y Tnn
which yields the property of linearity of the model of a manipulator with
respect to a suitable set of dynamic parameters.
In the simple case of no contact forces (he = 0), it may be worth including
the viscous friction coeﬃcient Fvi and Coulomb friction coeﬃcient Fsi in the
parameters of the vector π i , thus leading to a total number of 13 parameters
per joint. In summary, (7.80) can be compactly written as
τ = Y (q, q̇, q̈)π

(7.81)

264

7 Dynamics

Fig. 7.3. Two-link Cartesian arm

where π is a (p × 1) vector of constant parameters and Y is an (n × p) matrix
which is a function of joint positions, velocities and accelerations; this matrix
is usually called regressor . Regarding the dimension of the parameter vector,
notice that p ≤ 13n since not all the thirteen parameters for each joint may
explicitly appear in (7.81).

7.3 Dynamic Model of Simple Manipulator Structures
In the following, three examples of dynamic model computation are illustrated
for simple two-DOF manipulator structures. Two DOFs, in fact, are enough
to understand the physical meaning of all dynamic terms, especially the joint
coupling terms. On the other hand, dynamic model computation for manipulators with more DOFs would be quite tedious and prone to errors, when
carried out by paper and pencil. In those cases, it is advisable to perform it
with the aid of a symbolic programming software package.
7.3.1 Two-link Cartesian Arm
Consider the two-link Cartesian arm in Fig. 7.3, for which the vector of generalized coordinates is q = [ d1 d2 ]T . Let m 1 , m 2 be the masses of the two
links, and mm1 , mm2 the masses of the rotors of the two joint motors. Also let
Im1 , Im2 be the moments of inertia with respect to the axes of the two rotors.
It is assumed that pmi = pi−1 and z mi = z i−1 , for i = 1, 2, i.e., the motors
are located on the joint axes with centres of mass located at the origins of the
respective frames.
With the chosen coordinate frames, computation of the Jacobians in (7.16),
(7.18) yields
⎡
⎤
⎡
⎤
0 0
0 1
( 1)
(
)
JP 2 = ⎣ 0 0 ⎦ .
JP = ⎣ 0 0 ⎦
1 0
1 0

7.3 Dynamic Model of Simple Manipulator Structures

265

Obviously, in this case there are no angular velocity contributions for both
links.
Computation of the Jacobians in (7.25), (7.26) e (7.28), (7.29) yields
⎡
⎤
⎡
⎤
0 0
0 0
(m )
(m )
JP 2 = ⎣ 0 0 ⎦
JP 1 = ⎣ 0 0 ⎦
0 0
1 0
⎤
⎡
⎡
⎤
0 0
0 kr2
(m )
(m )
JO 1 = ⎣ 0 0 ⎦
JO 2 = ⎣ 0 0 ⎦
kr1 0
0 0
where kri is the gear reduction ratio of Motor i. It is obvious to see that
z 1 = [ 1 0 0 ]T , which greatly simpliﬁes computation of the second term
in (4.34).
From (7.32), the inertia matrix is


2
m 1 + mm2 + kr1
Im1 + m 2
0
B=
.
2
Im2
0
m 2 + kr2
It is worth observing that B is constant, i.e., it does not depend on the arm
conﬁguration. This implies also that C = O, i.e., there are no contributions
of centrifugal and Coriolis forces. As for the gravitational terms, since g 0 =
[ 0 0 −g ]T (g is gravity acceleration), (7.39) with the above Jacobians gives
g1 = (m 1 + mm2 + m 2 )g

g2 = 0.

In the absence of friction and tip contact forces, the resulting equations of
motion are
2
(m 1 + mm2 + kr1
Im1 + m 2 )d¨1 + (m 1 + mm2 + m 2 )g = τ1
2
(m 2 + kr2
Im2 )d¨2 = τ2

where τ1 and τ2 denote the forces applied to the two joints. Notice that a
completely decoupled dynamics has been obtained. This is a consequence not
only of the Cartesian structures but also of the particular geometry; in other
words, if the second joint axis were not at a right angle with the ﬁrst joint
axis, the resulting inertia matrix would not be diagonal (see Problem 7.1).
7.3.2 Two-link Planar Arm
Consider the two-link planar arm in Fig. 7.4, for which the vector of generalized coordinates is q = [ ϑ1 ϑ2 ]T . Let 1 , 2 be the distances of the centres
of mass of the two links from the respective joint axes. Also let m 1 , m 2 be
the masses of the two links, and mm1 , mm2 the masses of the rotors of the two
joint motors. Finally, let Im1 , Im2 be the moments of inertia with respect to
the axes of the two rotors, and I 1 , I 2 the moments of inertia relative to the

266

7 Dynamics

Fig. 7.4. Two-link planar arm

centres of mass of the two links, respectively. It is assumed that pmi = pi−1
and z mi = z i−1 , for i = 1, 2, i.e., the motors are located on the joint axes
with centres of mass located at the origins of the respective frames.
With the chosen coordinate frames, computation of the Jacobians in (7.16),
(7.18) yields
⎤
⎡
⎤
⎡
−1 s1 0
−a1 s1 − 2 s12 −2 s12
( )
( )
J P 2 = ⎣ a1 c1 + 2 c12
2 c12 ⎦ ,
J P 1 = ⎣ 1 c1 0 ⎦
0
0
0
0
whereas computation of the Jacobians in (7.17), (7.19) yields
⎡
⎤
⎡
⎤
0 0
0 0
( )
( )
J O2 = ⎣ 0 0 ⎦ .
J O1 = ⎣ 0 0 ⎦
1 0
1 1
Notice that ω i , for i = 1, 2, is aligned with z0 , and thus Ri has no eﬀect. It
is then possible to refer to the scalar moments of inertia I i .
Computation of the Jacobians in (7.25), (7.26) yields
⎡
⎤
⎡
⎤
0 0
−a1 s1 0
(m1 )
(m
)
= ⎣0 0⎦
J P 2 = ⎣ a1 c1 0 ⎦ ,
JP
0 0
0
0
whereas computation of the Jacobians in (7.28), (7.29) yields
⎤
⎤
⎡
⎡
0 0
0 0
(m )
(m )
JO 2 = ⎣ 0 0 ⎦
JO 1 = ⎣ 0 0 ⎦
kr1 0
1 kr2
where kri is the gear reduction ratio of Motor i.
From (7.32), the inertia matrix is


b (ϑ ) b12 (ϑ2 )
B(q) = 11 2
b21 (ϑ2 )
b22

7.3 Dynamic Model of Simple Manipulator Structures

267

2
b11 = I 1 + m 1 21 + kr1
Im1 + I 2 + m 2 (a21 + 22 + 2a1 2 c2 )
+Im2 + mm2 a21

b12 = b21 = I 2 + m 2 (22 + a1 2 c2 ) + kr2 Im2
2
b22 = I 2 + m 2 22 + kr2
Im2 .
Compared to the previous example, the inertia matrix is now conﬁgurationdependent. Notice that the term kr2 Im2 in the oﬀ-diagonal term of the inertia
matrix derives from having considered the rotational part of the motor kinetic energy as due to the total angular velocity, i.e., its own angular velocity
and that of the preceding link in the kinematic chain. At ﬁrst approximation,
especially in the case of high values of the gear reduction ratio, this contribution could be neglected; in the resulting reduced model, motor inertias would
appear uniquely in the elements on the diagonal of the inertia matrix with
2
Imi .
terms of the type kri
The computation of Christoﬀel symbols as in (7.45) gives
1 ∂b11
=0
2 ∂q1
1 ∂b11
= c121 =
= −m 2 a1 2 s2 = h
2 ∂q2
∂b12
1 ∂b22
=
−
=h
∂q2
2 ∂q1
∂b21
1 ∂b11
=
−
= −h
∂q1
2 ∂q2
1 ∂b22
= c221 =
=0
2 ∂q1
1 ∂b22
=
= 0,
2 ∂q2

c111 =
c112
c122
c211
c212
c222
leading to the matrix


C(q, q̇) =

hϑ̇2
−hϑ̇1


h(ϑ̇1 + ϑ̇2 )
.
0

Computing the matrix N in (7.47) gives
N (q, q̇) = Ḃ(q) − 2C(q, q̇)




2hϑ̇2 hϑ̇2
hϑ̇2 h(ϑ̇1 + ϑ̇2 )
=
−2
−hϑ̇1
hϑ̇2
0
0


0
−2hϑ̇1 − hϑ̇2
=
2hϑ̇1 + hϑ̇2
0
that allows the veriﬁcation of the skew-symmetry property expressed by (7.48).
See also Problem 7.2.

268

7 Dynamics

As for the gravitational terms, since g 0 = [ 0 −g
above Jacobians gives

0 ]T , (7.39) with the

g1 = (m 1 1 + mm2 a1 + m 2 a1 )gc1 + m 2 2 gc12
g2 = m 2 2 gc12 .
In the absence of friction and tip contact forces, the resulting equations of
motion are

2
I 1 + m 1 21 + kr1
Im1 + I 2 + m 2 (a21 + 22 + 2a1 2 c2 ) + Im2 + mm2 a21 ϑ̈1

+ I 2 + m 2 (22 + a1 2 c2 ) + kr2 Im2 ϑ̈2
−2m 2 a1 2 s2 ϑ̇1 ϑ̇2 − m 2 a1 2 s2 ϑ̇22
+(m 1 1 + mm2 a1 + m 2 a1 )gc1 + m 2 2 gc12 = τ1


2
I 2 + m 2 (22 + a1 2 c2 ) + kr2 Im2 ϑ̈1 + I 2 + m 2 22 + kr2
Im2 ϑ̈2

(7.82)

+m 2 a1 2 s2 ϑ̇21 + m 2 2 gc12 = τ2
where τ1 and τ2 denote the torques applied to the joints.
Finally, it is wished to derive a parameterization of the dynamic model
(7.82) according to the relation (7.81). By direct inspection of the expressions
of the joint torques, it is possible to ﬁnd the following parameter vector:
π = [ π1

π2

π3

π4

π5

π6

π7

π8 ]T

π1 = m1 = m 1 + mm2
π2 = m1 C1 = m 1 (1 − a1 )
π3 = I)1 = I + m (1 − a1 )2 + Im
1

1

(7.83)

2

π4 = Im1
π5 = m2 = m 2
π6 = m2 C2 = m 2 (2 − a2 )
π7 = I)2 = I + m (2 − a2 )2
2

2

π8 = Im2 ,
where the parameters for the augmented links have been found according
to (7.77). It can be recognized that the number of non-null parameters is less
than the maximum number of twenty-two parameters allowed in this case.6
The regressor in (7.81) is


y11 y12 y13 y14 y15 y16 y17 y18
Y =
(7.84)
y21 y22 y23 y24 y25 y26 y27 y28
6

The number of parameters can be further reduced by resorting to a more accurate
inspection, which leads to ﬁnding a minimum number of ﬁve parameters; those
turn out to be a linear combination of the parameters in (7.83) (see Problem 7.4).

7.3 Dynamic Model of Simple Manipulator Structures

269

y11 = a21 ϑ̈1 + a1 gc1
y12 = 2a1 ϑ̈1 + gc1
y13 = ϑ̈1
2
y14 = kr1
ϑ̈1
2
y15 = (a1 + 2a1 a2 c2 + a22 )ϑ̈1 + (a1 a2 c2 + a22 )ϑ̈2 − 2a1 a2 s2 ϑ̇1 ϑ̇2
y16

−a1 a2 s2 ϑ̇22 + a1 gc1 + a2 gc12
= (2a1 c2 + 2a2 )ϑ̈1 + (a1 c2 + 2a2 )ϑ̈2 − 2a1 s2 ϑ̇1 ϑ̇2 − a1 s2 ϑ̇22

y17

+gc12
= ϑ̈1 + ϑ̈2

y18 = kr2 ϑ̈2
y21 = 0
y22 = 0
y23 = 0
y24 = 0
y25 = (a1 a2 c2 + a22 )ϑ̈1 + a22 ϑ̈2 + a1 a2 s2 ϑ̇21 + a2 gc12
y26 = (a1 c2 + 2a2 )ϑ̈1 + 2a2 ϑ̈2 + a1 s2 ϑ̇21 + gc12
y27 = ϑ̈1 + ϑ̈2
2
y28 = kr2 ϑ̈1 + kr2
ϑ̈2 .

Example 7.2
In order to understand the relative weight of the various torque contributions in the
dynamic model (7.82), consider a two-link planar arm with the following data:
a1 = a 2 = 1 m

1

=

kr1 = kr2 = 100

2

= 0.5 m

m1 = m2 = 50 kg

mm1 = mm2 = 5 kg

I1 = I2 = 10 kg·m2

Im1 = Im2 = 0.01 kg·m2 .

The two links have been chosen equal to illustrate better the dynamic interaction
between the two joints.
Figure 7.5 shows the time history of positions, velocities, accelerations and
torques resulting from joint trajectories with typical triangular velocity proﬁle and
equal time duration. The initial arm conﬁguration is so that the tip is located at the
point (0.2, 0) m with a lower elbow posture. Both joints make a rotation of π/2 rad
in a time of 0.5 s.
From the time history of the single torque contributions in Fig. 7.6 it can be
recognized that:
• The inertia torque at Joint 1 due to Joint 1 acceleration follows the time history
of the acceleration.
• The inertia torque at Joint 2 due to Joint 2 acceleration is piecewise constant,
since the inertia moment at Joint 2 axis is constant.

270

7 Dynamics
joint 2 pos

4

4

3

3

2

2

[rad]

[rad]

joint 1 pos

1

1

0

0

−1

−1

−2
0

0.1

0.2

0.3
[s]

0.4

−2
0

0.5

0.1

6

6

4

4

2
0
0

0.1

0.3
[s]

0.4

0.5

0

joint 1 acc

0.1

20

10

10

0
−10

0.2

0.3
[s]

0.4

0.5

joint 2 acc

30

[rad/s^2]

[rad/s^2]

0.2

−20

0
−10
−20

0.1

0.2

0.3
[s]

0.4

−30
0

0.5

0.1

joint 1 torque
6000

4000

4000

2000

2000

0

−2000

−4000

−4000
0.2

0.3
[s]

0.4

0.3
[s]

0.4

0.5

0

−2000

0.1

0.2

joint 2 torque

6000

[Nm]

[Nm]

0.5

2

20

−6000
0

0.4

0

30

−30
0

0.3
[s]

joint 2 vel

[rad/s]

[rad/s]

joint 1 vel

0.2

0.5

−6000
0

0.1

0.2

0.3
[s]

0.4

0.5

Fig. 7.5. Time history of positions, velocities, accelerations and torques with joint
trajectories of equal duration

7.3 Dynamic Model of Simple Manipulator Structures
inert_11

0

−5000
0

0.1

0.2

0.4

0.5

0

−5000
0

inert_12

100
0

0

−100

−100

−200

−300

−400

−400
0.1

0.2

0.3
[s]

0.2

0.4

−500
0

0.5

0.3
[s]

0.4

0.5

0.4

0.5

0.4

0.5

0.4

0.5

inert_21

−200

−300

−500
0

0.1

100

[Nm]

[Nm]

0.3
[s]

inert_22

5000

[Nm]

[Nm]

5000

271

0.1

0.2

centrif_2 + coriol_12

0.3
[s]
centrif_1

1000

1000

500

[Nm]

[Nm]

12
2
0
−500
0

500
0

0.1

0.2

0.3
[s]

0.4

−500
0

0.5

0.1

0.2

grav_2

800

800

600

600

400

400

[Nm]

[Nm]

grav_1

200
0

200
0

−200
0

0.3
[s]

−200
0.1

0.2

0.3
[s]

0.4

0.5

0

0.1

0.2

0.3
[s]

Fig. 7.6. Time history of torque contributions with joint trajectories of equal duration

272

7 Dynamics
joint 1 pos

3

3

2

2

1
0

0
−1
0.2

0.4

−2
0

0.6

4

4

2

2

0

−2

−4

−4
0.2

[s]

0.4

−6
0

0.6

joint 1 acc

40

[rad/s^2]

−20

[s]

0.4

0.6

joint 2 acc

−20

0.2

[s]

0.4

−40
0

0.6

0.2

[s]

0.4

0.6

joint 2 torque
5000

[Nm]

5000

[Nm]

0.2

0

joint 1 torque

0

−5000
0

0.6

20

0

−40
0

0.4

joint 2 vel

40

20

[s]

0

−2

−6
0

0.2

6

[rad/s]

[rad/s]

[s]

joint 1 vel

6

[rad/s^2]

1

−1
−2
0

joint 2 pos

4

[rad]

[rad]

4

0

−5000
0.2

[s]

0.4

0.6

0

0.2

[s]

0.4

0.6

Fig. 7.7. Time history of positions, velocities, accelerations and torques with joint
trajectories of diﬀerent duration

7.3 Dynamic Model of Simple Manipulator Structures
inert_11

4000

4000

2000

2000

0
−2000
−4000

−6000
0.2

[s]

0.4

0.6

0

inert_12

1500
1000

1000

500

500

0
−500
0

0.2

0.4

0.6

0
−500

0.2

[s]

0.4

0.6

0

0.2

centrif_2 + coriol_12

[s]

0.4

0.6

centrif_1

1000

1000
12

500
[Nm]

500
[Nm]

[s]

inert_21

1500

[Nm]

[Nm]

0
−2000
−4000

−6000
0

inert_22

6000

[Nm]

[Nm]

6000

273

0

0

2
−500
0

−500
0.2

0.6

0

grav_1

1000
800

800

600

600

400
200

[s]

0.4

0.6

grav_2

400
200

0
0

0.2

1000

[Nm]

[Nm]

[s]

0.4

0
0.2

[s]

0.4

0.6

0

0.2

[s]

0.4

0.6

Fig. 7.8. Time history of torque contributions with joint trajectories of diﬀerent
duration

274

7 Dynamics
tip pos

2

[m]

1.5
1
0.5
0
0

0.1

0.2

0.3
[s]

0.4

0.5

0.4

0.5

0.4

0.5

tip vel
5

[m/s]

4
3
2
1
0
0

0.1

0.2

0.3
[s]
tip acc

40

[m/s^2]

20
0
−20
−40
0

0.1

0.2

0.3
[s]

Fig. 7.9. Time history of tip position, velocity and acceleration with a straight line
tip trajectory along the horizontal axis
•

The inertia torques at each joint due to acceleration of the other joint conﬁrm
the symmetry of the inertia matrix, since the acceleration proﬁles are the same
for both joints.
• The Coriolis eﬀect is present only at Joint 1, since the arm tip moves with respect
to the mobile frame attached to Link 1 but is ﬁxed with respect to the frame
attached to Link 2.
• The centrifugal and Coriolis torques reﬂect the above symmetry.
Figure 7.7 shows the time history of positions, velocities, accelerations and
torques resulting from joint trajectories with typical trapezoidal velocity proﬁle and
diﬀerent time duration. The initial conﬁguration is the same as in the previous case.
The two joints make a rotation so as to take the tip to the point (1.8, 0) m. The
acceleration time is 0.15 s and the maximum velocity is 5 rad/s for both joints.

7.3 Dynamic Model of Simple Manipulator Structures
joint 1 pos

3

3

2

2

1

1

0

0

−1

−1

−2
0

0.1

0.2

0.3
[s]

0.4

joint 2 pos

4

[rad]

[rad]

4

−2
0

0.5

0.1

4

4

2

2

0

0

−2

−4

−6

−6
0.2

0.3
[s]

0.4

−8
0

0.5

0.1

60

60

40

40

20
0
−20

0.2

0.3
[s]

0.4

0.5

20
0
−40

0.1

0.2

0.3
[s]

0.4

0.5

0

0.1

joint 1 torque

0.2

0.3
[s]

0.4

0.5

joint 2 torque

5000
[Nm]

5000
[Nm]

0.5

−20

−40

0

−5000
0

0.4

joint 2 acc
80

[rad/s^2]

[rad/s^2]

joint 1 acc
80

0

0.3
[s]

−2

−4

0.1

0.2

joint 2 vel

[rad/s]

[rad/s]

joint 1 vel

−8
0

275

0

−5000
0.1

0.2

0.3
[s]

0.4

0.5

0

0.1

0.2

0.3
[s]

0.4

0.5

Fig. 7.10. Time history of joint positions, velocities, accelerations, and torques with
a straight line tip trajectory along the horizontal axis

276

7 Dynamics
inert_11

4

1

x 10

1

0
−0.5
−1
0

inert_22

4

0.5
[Nm]

[Nm]

0.5

x 10

0
−0.5

0.1

0.2

0.3
[s]

0.4

−1
0

0.5

0.1

0.2

3000

2000

2000

1000

1000

0

0

−1000

−1000

0

0.1

0.2

0.3
[s]

0.4

0.5

0

0.1

0.2

centrif_2 + coriol_12

[Nm]

[Nm]

0.4

0.5

0.4

0.5

0.4

0.5

500

0
−500

0
−500

2

−1000

−1000
0.1

0.2

0.3
[s]

0.4

0.5

0

grav_1

1000

0.1

0.2

800

800

600

600

400
200

0.3
[s]
grav_2

1000

[Nm]

[Nm]

0.3
[s]

1000
12

500

400
200

0
0

0.5

centrif_1

1000

0

0.4

inert_21

3000

[Nm]

[Nm]

inert_12

0.3
[s]

0
0.1

0.2

0.3
[s]

0.4

0.5

0

0.1

0.2

0.3
[s]

Fig. 7.11. Time history of joint torque contributions with a straight line tip trajectory along the horizontal axis

7.3 Dynamic Model of Simple Manipulator Structures

277

From the time history of the single torque contributions in Fig. 7.8 it can be
recognized that:
• The inertia torque at Joint 1 due to Joint 2 acceleration is opposite to that at
Joint 2 due to Joint 1 acceleration in that portion of trajectory when the two
accelerations have the same magnitude but opposite sign.
• The diﬀerent velocity proﬁles imply that the centrifugal eﬀect induced at Joint
1 by Joint 2 velocity dies out later than the centrifugal eﬀect induced at Joint 2
by Joint 1 velocity.
• The gravitational torque at Joint 2 is practically constant in the ﬁrst portion
of the trajectory, since Link 2 is almost kept in the same posture. As for the
gravitational torque at Joint 1, instead, the centre of mass of the articulated
system moves away from the origin of the axes.
Finally, Fig. 7.9 shows the time history of tip position, velocity and acceleration
for a trajectory with a trapezoidal velocity proﬁle. Starting from the same initial
posture as above, the arm tip makes a translation of 1.6 m along the horizontal axis;
the acceleration time is 0.15 s and the maximum velocity is 5 m/s.
As a result of an inverse kinematics procedure, the time history of joint positions,
velocities and accelerations have been computed which are illustrated in Fig. 7.10,
together with the joint torques that are needed to execute the assigned trajectory.
It can be noticed that the time history of the represented quantities diﬀers from
the corresponding ones in the operational space, in view of the nonlinear eﬀects
introduced by kinematic relations.
For what concerns the time history of the individual torque contributions in
Fig. 7.11, it is possible to make a number of remarks similar to those made above
for trajectories assigned directly in the joint space.

7.3.3 Parallelogram Arm
Consider the parallelogram arm in Fig. 7.12. Because of the presence of the
closed chain, the equivalent tree-structured open-chain arm is initially taken
into account. Let 1 , 2 , 3 and 1 be the distances of the centres of mass
of the three links along one branch of the tree, and of the single link along
the other branch, from the respective joint axes. Also let m 1 , m 2 , m 3 and
m 1 be the masses of the respective links, and I 1 , I 2 , I 3 and I 1 the
moments of inertia relative to the centres of mass of the respective links. For
the sake of simplicity, the contributions of the motors are neglected.
With the chosen coordinate frames, computation of the Jacobians in (7.16)
(7.18) yields
⎡
⎤
⎡
⎤
−1 s1 0 0
−a1 s1 − 2 s1 2 −2 s1 2 0
( )
( )
0 0⎦
J P 2 = ⎣ a1 c1 + 2 c1 2
2 c1 2
0⎦
J P 1 = ⎣ 1 c1
0
0 0
0
0
0
⎡
⎤
−a1 s1 − a2 s1 2 − 3 s1 2 3 −a2 s1 2 − 3 s1 2 3 −3 s1 2 3
( 3 )
J P = ⎣ a1 c1 + a2 c1 2 + 3 c1 2 3
a2 c1 2 + 3 c1 2 3
3 c1 2 3 ⎦
0
0
0

278

7 Dynamics

Fig. 7.12. Parallelogram arm

⎤

⎡

and
(

J P 1

)

−1 s1
= ⎣ 1 c1
0

whereas computation of the Jacobians in
⎡
⎤
⎡
0 0 0
0
( 1 )
(
)
JO = ⎣ 0 0 0 ⎦
J O 2 = ⎣ 0
1 0 0
1
and
(
)
J O 1

⎦,

(7.17), (7.19) yields
⎤
⎡
0 0
0
(
)
0 0⎦
J O 3 = ⎣ 0
1 0
1

0
0
1

⎤
0
0⎦
1

⎡ ⎤
0
= ⎣0⎦.
1

From (7.32), the inertia matrix of the virtual arm composed of joints ϑ1 ,
ϑ2 , ϑ3 is
⎤
⎡
b1 1 (ϑ2 , ϑ3 ) b1 2 (ϑ2 , ϑ3 ) b1 3 (ϑ2 , ϑ3 )
b2 2 (ϑ3 )
b2 3 (ϑ3 ) ⎦
B  (q  ) = ⎣ b2 1 (ϑ2 , ϑ3 )
b3 2 (ϑ3 )
b3  3 
b3 1 (ϑ2 , ϑ3 )
b1  1  = I

1

+m

(a21
3

1

a22

+m
+
b1 2 = b2 1 = I

2

21 + I

2

+m

2

(a21 + 22 + 2a1 2 c2 ) + I

23

+
+ 2a1 a2 c2 + 2a1 3 c2 3 + 2a2 3 c3 )
+ m 2 (22 + a1 2 c2 ) + I 3

+m 3 (a22 + 23 + a1 a2 c2 + a1 3 c2 3 + 2a2 3 c3 )
b1 3 = b31 = I 3 + m 3 (23 + a1 3 c2 3 + a2 3 c3 )
b2  2  = I
b2  3  = I
b3  3  = I

2
3
3

+m
+m
+m

22 + I 3 + m 3 (a22 + 23 + 2a2 3 c3 )
(23 + a2 3 c3 )
3
2

3

3

23

7.3 Dynamic Model of Simple Manipulator Structures

279

while the moment of inertia of the virtual arm composed of just joint ϑ1 is
b1 1 = I

1

+m

1

21 .

Therefore, the inertial torque contributions of the two virtual arms are respectively:
3

τi =
bi j  ϑ̈j 
τ1 = b1 1 ϑ̈1 .
j  =1

At this point, in view of (2.64) and (3.121), the inertial torque contributions at the actuated joints for the closed-chain arm turn out to be
τ a = B a q̈ a
where q a = [ ϑ1

b
B a = a11
ba21

T

T

ϑ1 ] , τ a = [ τa1

ba12
ba22

ba11 = I 1 + m
−2a1 m 3 3

1

21 + m

ba12 = ba21 = a1 m
ba22 = I

1

+m

1

τa2 ] and

2

2

a21 + I

2 + a1 m

21 + I

2

+m

2

3

3

+m

3

23 + m

3

a21


(a1 − 3 ) cos (ϑ1 − ϑ1 )

22 + m

3

a21 .

This expression reveals the possibility of obtaining a conﬁguration-independent
and decoupled inertia matrix; to this end it is suﬃcient to design the four links
of the parallelogram so that
m
m

¯3
a1
=

a1
2 2
3

where ¯3 = 3 − a1 is the distance of the centre of mass of Link 3 from the
axis of Joint 4. If this condition is satisﬁed, then the inertia matrix is diagonal
(ba12 = ba21 = 0) with


2 ¯3
2
2
ba11 = I 1 + m 1 1 + m 2 a1 1 +
+ I 3
a1 a1


a1 a1
2
2
ba22 = I 1 + m 1 1 + I 2 + m 2 2 1 +
.
2 ¯3
As a consequence, no contributions of Coriolis and centrifugal torques are
obtained. Such a result could not be achieved with the previous two-link
planar arm, no matter how the design parameters were chosen.

280

7 Dynamics

As for the gravitational terms, since g 0 = [ 0 −g
above Jacobians gives
g1 (m

1

1 + m

2

a1 + m

3

a1 )gc1 + (m

g2

+m 3 3 gc1 2 3
(m 2 2 + m 3 a2 )gc1 2 + m

g

m

3

3

3

2

0 ]T , (7.39) with the

2 + m

3

a2 )gc1 2

3 gc1 2 3

 gc
3

1 2 3

and
g1 = m

1

1 gc1 .

Composing the various contributions as done above yields


(m 1 1 + m 2 a1 − m 3 ¯3 )gc1
ga =
(m 1 1 + m 2 2 + m 3 a1 )gc1
which, together with the inertial torques, completes the derivation of the
sought dynamic model.
A ﬁnal comment is in order. In spite of its kinematic equivalence with the
two-link planar arm, the dynamic model of the parallelogram is remarkably
lighter. This property is quite advantageous for trajectory planning and control purposes. For this reason, apart from obvious considerations related to
manipulation of heavy payloads, the adoption of closed kinematic chains in
the design of industrial robots has received a great deal of attention.

7.4 Dynamic Parameter Identiﬁcation
The use of the dynamic model for solving simulation and control problems demands the knowledge of the values of dynamic parameters of the manipulator
model.
Computing such parameters from the design data of the mechanical structure is not simple. CAD modelling techniques can be adopted which allow the
computation of the values of the inertial parameters of the various components
(links, actuators and transmissions) on the basis of their geometry and type of
materials employed. Nevertheless, the estimates obtained by such techniques
are inaccurate because of the simpliﬁcation typically introduced by geometric
modelling; moreover, complex dynamic eﬀects, such as joint friction, cannot
be taken into account.
A heuristic approach could be to dismantle the various components of the
manipulator and perform a series of measurements to evaluate the inertial
parameters. Such technique is not easy to implement and may be troublesome
to measure the relevant quantities.
In order to ﬁnd accurate estimates of dynamic parameters, it is worth
resorting to identiﬁcation techniques which conveniently exploit the property
of linearity (7.81) of the manipulator model with respect to a suitable set of

7.4 Dynamic Parameter Identiﬁcation

281

dynamic parameters. Such techniques allow the computation of the parameter
vector π from the measurements of joint torques τ and of relevant quantities
for the evaluation of the matrix Y , when suitable motion trajectories are
imposed to the manipulator.
On the assumption that the kinematic parameters in the matrix Y are
known with good accuracy, e.g., as a result of a kinematic calibration, measurements of joint positions q, velocities q̇ and accelerations q̈ are required.
Joint positions and velocities can be actually measured while numerical reconstruction of accelerations is needed; this can be performed on the basis of the
position and velocity values recorded during the execution of the trajectories.
The reconstructing ﬁlter does not work in real time and thus it can also be
anti-causal, allowing an accurate reconstruction of the accelerations.
As regards joint torques, in the unusual case of torque sensors at the
joint, these can be measured directly. Otherwise, they can be evaluated from
either wrist force measurements or current measurements in the case of electric
actuators.
If measurements of joint torques, positions, velocities and accelerations
have been obtained at given time instants t1 , . . . , tN along a given trajectory,
one may write
⎡
⎤ ⎡
⎤
τ (t1 )
Y (t1 )
⎢
⎥ ⎢
⎥
τ̄ = ⎣ ... ⎦ = ⎣ ... ⎦ π = Ȳ π.
(7.85)
τ (tN )

Y (tN )

The number of time instants sets the number of measurements to perform
and should be large enough (typically N n
p) so as to avoid ill-conditioning
of matrix Ȳ . Solving (7.85) by a least-squares technique leads to the solution
in the form
T
T
(7.86)
π = (Ȳ Ȳ )−1 Ȳ τ̄
T

T

where (Ȳ Ȳ )−1 Ȳ is the left pseudo-inverse matrix of Ȳ .
It should be noticed that, in view of the block triangular structure of
matrix Y in (7.80), computation of parameter estimates could be simpliﬁed
by resorting to a sequential procedure. Take the equation τn = y Tnn π n and
solve it for π n by specifying τn and y Tnn for a given trajectory on Joint n.
By iterating the procedure, the manipulator parameters can be identiﬁed on
the basis of measurements performed joint by joint from the outer link to the
base. Such procedure, however, may have the inconvenience to accumulate
any error due to ill-conditioning of the matrices involved step by step. It may
then be worth operating with a global procedure by imposing motions on all
manipulator joints at the same time.
Regarding the rank of matrix Ȳ , it is possible to identify only the dynamic
parameters of the manipulator that contribute to the dynamic model. Example 7.2 has indeed shown that for the two-link planar arm considered, only
8 out of the 22 possible dynamic parameters appear in the dynamic model.
Hence, there exist some dynamic parameters which, in view of the disposition

282

7 Dynamics

of manipulator links and joints, are non-identiﬁable, since for any trajectory
assigned to the structure they do not contribute to the equations of motion. A
direct consequence is that the columns of the matrix Y in (7.80) corresponding to such parameters are null and thus they have to be removed from the
matrix itself; e.g., the resulting (2 × 8) matrix in (7.84).
Another issue to consider about determination of the eﬀective number
of parameters that can be identiﬁed by (7.86) is that some parameters can
be identiﬁed in linear combinations whenever they do not appear isolated in
the equations. In such a case, it is necessary, for each linear combination, to
remove as many columns of the matrix Y as the number of parameters in the
linear combination minus one.
For the determination of the minimum number of identiﬁable parameters
that allow direct application of the least-squares technique based on (7.86),
it is possible to inspect directly the equations of the dynamic model, as long
as the manipulator has few joints. Otherwise, numerical techniques based on
singular value decomposition of matrix Ȳ have to be used. If the matrix Ȳ
resulting from a series of measurements is not full-rank, one has to resort to
a damped least-squares inverse of Ȳ where solution accuracy depends on the
weight of the damping factor.
In the above discussion, the type of trajectory imposed to the manipulator
joints has not been explicitly addressed. It can be generally ascertained that
the choice should be oriented in favor of polynomial type trajectories which are
suﬃciently rich to allow an accurate evaluation of the identiﬁable parameters.
T
This corresponds to achieving a low condition number of the matrix Ȳ Ȳ
along the trajectory. On the other hand, such trajectories should not excite
any unmodelled dynamic eﬀects such as joint elasticity or link ﬂexibility that
would naturally lead to unreliable estimates of the dynamic parameters to
identify.
Finally, it is worth observing that the technique presented above can also
be extended to the identiﬁcation of the dynamic parameters of an unknown
payload at the manipulator’s end-eﬀector. In such a case, the payload can be
regarded as a structural modiﬁcation of the last link and one may proceed to
identify the dynamic parameters of the modiﬁed link. To this end, if a force
sensor is available at the manipulator’s wrist, it is possible to characterize
directly the dynamic parameters of the payload starting from force sensor
measurements.

7.5 Newton–Euler Formulation
In the Lagrange formulation, the manipulator dynamic model is derived starting from the total Lagrangian of the system. On the other hand, the Newton–
Euler formulation is based on a balance of all the forces acting on the generic
link of the manipulator. This leads to a set of equations whose structure allows
a recursive type of solution; a forward recursion is performed for propagating

7.5 Newton–Euler Formulation

283

Fig. 7.13. Characterization of Link i for Newton–Euler formulation

link velocities and accelerations, followed by a backward recursion for propagating forces.
Consider the generic augmented Link i (Link i plus motor of Joint i + 1) of
the manipulator kinematic chain (Fig. 7.13). According to what was presented
in Sect. 7.2.2, one can refer to the centre of mass Ci of the augmented link to
characterize the following parameters:
•
•
•
•
•
•

mi mass of augmented link,
Ī i inertia tensor of augmented link,
Imi moment of inertia of rotor,
r i−1,Ci vector from origin of Frame (i − 1) to centre of mass Ci ,
r i,Ci vector from origin of Frame i to centre of mass Ci ,
r i−1,i vector from origin of Frame (i − 1) to origin of Frame i.
The velocities and accelerations to be considered are:

•
•
•
•
•
•
•
•
•

ṗCi linear velocity of centre of mass Ci ,
ṗi linear velocity of origin of Frame i,
ω i angular velocity of link,
ω mi angular velocity of rotor,
p̈Ci linear acceleration of centre of mass Ci ,
p̈i linear acceleration of origin of Frame i,
ω̇ i angular acceleration of link,
ω̇ mi angular acceleration of rotor,
g 0 gravity acceleration.
The forces and moments to be considered are:

• f i force exerted by Link i − 1 on Link i,
• −f i+1 force exerted by Link i + 1 on Link i,

284

7 Dynamics

• μi moment exerted by Link i − 1 on Link i with respect to origin of
Frame i − 1,
• −μi+1 moment exerted by Link i + 1 on Link i with respect to origin of
Frame i.
Initially, all the vectors and matrices are assumed to be expressed with
reference to the base frame.
As already anticipated, the Newton–Euler formulation describes the motion of the link in terms of a balance of forces and moments acting on it.
The Newton equation for the translational motion of the centre of mass
can be written as
(7.87)
f i − f i+1 + mi g 0 = mi p̈Ci .
The Euler equation for the rotational motion of the link (referring moments to the centre of mass) can be written as
d
(Ī i ω i + kr,i+1 q̇i+1 Imi+1 z mi+1 ),
dt
(7.88)
where (7.67) has been used for the angular momentum of the rotor. Notice
that the gravitational force mi g 0 does not generate any moment, since it is
concentrated at the centre of mass.
As pointed out in the above Lagrange formulation, it is convenient to
express the inertia tensor in the current frame (constant tensor). Hence, aci
cording to (7.12), one has Ī i = Ri Ī i RTi , where Ri is the rotation matrix from
Frame i to the base frame. Substituting this relation in the ﬁrst term on the
right-hand side of (7.88) yields
μi + f i × r i−1,Ci − μi+1 − f i+1 × r i,Ci =

d
i
i T
i
(Ī i ω i ) = Ṙi Ī i RTi ω i + Ri Ī i Ṙi ω i + Ri Ī i RTi ω̇ i
(7.89)
dt
i
i
i
= S(ω i )Ri Ī i RTi ω i + Ri Ī i RTi S T (ω i )ω i + Ri Ī i RTi ω̇ i
= Ī i ω̇ i + ω i × (Ī i ω i )
where the second term represents the gyroscopic torque induced by the dependence of Ī i on link orientation.7 Moreover, by observing that the unit vector
z mi+1 rotates accordingly to Link i, the derivative needed in the second term
on the right-hand side of (7.88) is
d
(q̇i+1 Imi+1 z mi+1 ) = q̈i+1 Imi+1 z mi+1 + q̇i+1 Imi+1 ω i × z mi+1
dt

(7.90)

By substituting (7.89), (7.90) in (7.88), the resulting Euler equation is
μi + f i × r i−1,Ci −μi+1 − f i+1 × r i,Ci = Ī i ω̇ i + ω i × (Ī i ω i )

(7.91)

+kr,i+1 q̈i+1 Imi+1 z mi+1 + kr,i+1 q̇i+1 Imi+1 ω i × z mi+1 .
7

In deriving (7.89), the operator S has been introduced to compute the derivative
of Ri , as in (3.8); also, the property S T (ω i )ω i = 0 has been utilized.

7.5 Newton–Euler Formulation

285

The generalized force at Joint i can be computed by projecting the force
f i for a prismatic joint, or the moment μi for a revolute joint, along the
joint axis. In addition, there is the contribution of the rotor inertia torque
kri Imi ω̇ Tmi z mi . Hence, the generalized force at Joint i is expressed by
 T
f i z i−1 + kri Imi ω̇ Tmi z mi for a prismatic joint
τi =
(7.92)
μTi z i−1 + kri Imi ω̇ Tmi z mi for a revolute joint.
7.5.1 Link Accelerations
The Newton–Euler equations in (7.87), (7.91) and the equation in (7.92) require the computation of linear and angular acceleration of Link i and Rotor
i. This computation can be carried out on the basis of the relations expressing
the linear and angular velocities previously derived. The equations in (3.21),
(3.22), (3.25), (3.26) can be brieﬂy rewritten as

ω i−1
for a prismatic joint
(7.93)
ωi =
ω i−1 + ϑ̇i z i−1
for a revolute joint
and


ṗi =

ṗi−1 + d˙i z i−1 + ω i × r i−1,i
ṗi−1 + ω i × r i−1,i

for a prismatic joint
for a revolute joint.

(7.94)

As for the angular acceleration of the link, it can be seen that, for a
prismatic joint, diﬀerentiating (3.21) with respect to time gives
ω̇ i = ω̇ i−1 ,

(7.95)

whereas, for a revolute joint, diﬀerentiating (3.25) with respect to time gives
ω̇ i = ω̇ i−1 + ϑ̈i z i−1 + ϑ̇i ω i−1 × z i−1 .

(7.96)

As for the linear acceleration of the link, for a prismatic joint, diﬀerentiating (3.22) with respect to time gives
p̈i = p̈i−1 + d¨i z i−1 + d˙i ω i−1 × z i−1 + ω̇ i × r i−1,i
+ω i × d˙i z i−1 + ω i × (ω i−1 × r i−1,i )

(7.97)

where the relation ṙ i−1,i = d˙i z i−1 + ω i−1 × r i−1,i has been used. Hence, in
view of (3.21), the equation in (7.97) can be rewritten as
p̈i = p̈i−1 + d¨i z i−1 + 2d˙i ω i × z i−1 + ω̇ i × r i−1,i + ω i × (ω i × r i−1,i ). (7.98)
Also, for a revolute joint, diﬀerentiating (3.26) with respect to time gives
p̈i = p̈i−1 + ω̇ i × r i−1,i + ω i × (ω i × r i−1,i ).

(7.99)

286

7 Dynamics

In summary, the equations in (7.95), (7.96), (7.98), (7.99) can be compactly
rewritten as

ω̇ i−1
for a prismatic joint
ω̇ i =
(7.100)
ω̇ i−1 + ϑ̈i z i−1 + ϑ̇i ω i−1 × z i−1
for a revolute joint
and

⎧
⎪
p̈
+ d¨i z i−1 + 2d˙i ω i × z i−1
⎪
⎪ i−1
⎨
+ω̇ i × r i−1,i + ω i × (ω i × r i−1,i )
p̈=
i
⎪
⎪ p̈i−1 + ω̇ i × r i−1,i
⎪
⎩
+ω i × (ω i × r i−1,i )

for a prismatic joint
(7.101)
for a revolute joint.

The acceleration of the centre of mass of Link i required by the Newton
equation in (7.87) can be derived from (3.15), since ṙ ii,Ci = 0; by diﬀerentiating (3.15) with respect to time, the acceleration of the centre of mass Ci
can be expressed as a function of the velocity and acceleration of the origin
of Frame i, i.e.,
p̈Ci = p̈i + ω̇ i × r i,Ci + ω i × (ω i × r i,Ci ).

(7.102)

Finally, the angular acceleration of the rotor can be obtained by time
diﬀerentiation of (7.23), i.e.,
ω̇ mi = ω̇ i−1 + kri q̈i z mi + kri q̇i ω i−1 × z mi .

(7.103)

7.5.2 Recursive Algorithm
It is worth remarking that the resulting Newton–Euler equations of motion
are not in closed form, since the motion of a single link is coupled to the
motion of the other links through the kinematic relationship for velocities
and accelerations.
Once the joint positions, velocities and accelerations are known, one can
compute the link velocities and accelerations, and the Newton–Euler equations
can be utilized to ﬁnd the forces and moments acting on each link in a recursive fashion, starting from the force and moment applied to the end-eﬀector.
On the other hand, also link and rotor velocities and accelerations can be
computed recursively starting from the velocity and acceleration of the base
link. In summary, a computationally recursive algorithm can be constructed
that features a forward recursion relative to the propagation of velocities and
accelerations and a backward recursion for the propagation of forces and moments along the structure.
For the forward recursion, once q, q̇, q̈, and the velocity and acceleration
of the base link ω 0 , p̈0 − g 0 , ω̇ 0 are speciﬁed, ω i , ω̇ i , p̈i , p̈Ci , ω̇ mi can be
computed using (7.93), (7.100), (7.101), (7.102), (7.103), respectively. Notice
that the linear acceleration has been taken as p̈0 − g 0 so as to incorporate the

7.5 Newton–Euler Formulation

287

term −g 0 in the computation of the acceleration of the centre of mass p̈Ci
via (7.101), (7.102).
Having computed the velocities and accelerations with the forward recursion from the base link to the end-eﬀector, a backward recursion can be carried
out for the forces. In detail, once he = [ f Tn+1 μTn+1 ]T is given (eventually
he = 0), the Newton equation in (7.87) to be used for the recursion can be
rewritten as
(7.104)
f i = f i+1 + mi p̈Ci
since the contribution of gravity acceleration has already been included in
p̈Ci . Further, the Euler equation gives
μi = −f i × (r i−1,i + r i,Ci ) + μi+1 + f i+1 × r i,Ci + Ī i ω̇ i + ω i × (Ī i ω i )
+kr,i+1 q̈i+1 Imi+1 z mi+1 + kr,i+1 q̇i+1 Imi+1 ω i × z mi+1

(7.105)

which derives from (7.91), where r i−1,Ci has been expressed as the sum of the
two vectors appearing already in the forward recursion. Finally, the generalized forces resulting at the joints can be computed from (7.92) as
⎧ T
⎪
f i z i−1 + kri Imi ω̇ Tmi z mi
⎪
⎪
⎨ +F d˙ + F sgn (d˙ )
for a prismatic joint
vi i
si
i
(7.106)
τi =
T
T
⎪
μ
z
+
k
I
z
ω̇
⎪
i−1
ri
m
m
m
i
i
i
i
⎪
⎩
+Fvi ϑ̇i + Fsi sgn (ϑ̇i )
for a revolute joint,
where joint viscous and Coulomb friction torques have been included.
In the above derivation, it has been assumed that all vectors were referred
to the base frame. To simplify greatly computation, however, the recursion is
computationally more eﬃcient if all vectors are referred to the current frame
on Link i. This implies that all vectors that need to be transformed from
Frame i + 1 into Frame i have to be multiplied by the rotation matrix Rii+1 ,
whereas all vectors that need to be transformed from Frame i − 1 into Frame i
T
. Therefore, the equations
have to be multiplied by the rotation matrix Ri−1
i
in (7.93), (7.100), (7.101), (7.102), (7.103), (7.104), (7.105), (7.106) can be
rewritten as:
 i−1 T i−1
Ri
ω i−1
for a prismatic joint
i
(7.107)
ωi =
i−1 T
i−1
Ri
(ω i−1 + ϑ̇i z 0 ) for a revolute joint
 i−1 T i−1
for a prismatic joint
Ri
ω̇ i−1
i
(7.108)
ω̇ i =
i−1
i−1
T
Ri−1
(
ω̇
+
ϑ̈
z
+
ϑ̇
ω
×
z
)
for
a
revolute
joint
i
0
i
0
i
i−1
i−1
⎧ i−1
i−1
T
T
⎪
R
(p̈i−1 + d¨i z 0 ) + 2d˙i ω ii × Ri−1
z0
i
⎪
⎪ i
⎪
⎪
i
⎨ +ω̇ i × r ii−1,i + ω ii × (ω ii × r ii−1,i )
for a prismatic joint
p̈ii =
(7.109)
⎪
T i−1
⎪
p̈i−1 + ω̇ ii × r ii−1,i
Ri−1
⎪
i
⎪
⎪
⎩
+ω ii × (ω ii × r ii−1,i )
for a revolute joint

288

7 Dynamics

Fig. 7.14. Computational structure of the Newton–Euler recursive algorithm

p̈iCi = p̈ii + ω̇ ii × r ii,Ci + ω ii × (ω ii × r ii,Ci )

(7.110)

i−1
i−1
i−1
i−1
ω̇ i−1
mi = ω̇ i−1 + kri q̈i z mi + kri q̇i ω i−1 × z mi

(7.111)

i
f ii = Rii+1 f i+1
i+1 + mi p̈Ci

(7.112)

i+1
i
i
μii = −f ii × (r ii−1,i +r ii,Ci ) + Rii+1 μi+1
i+1 + Ri+1 f i+1 × r i,Ci

(7.113)

i
i
+Ī i ω̇ ii + ω ii × (Ī i ω ii )
i
+ω ii × (Ī i ω ii ) + kr,i+1 q̈i+1 Imi+1 z imi+1

⎧ i T i−1 T
T i−1
z 0 + kri Imi ω̇ i−1
⎪
mi z mi
⎪ f i Ri
⎪
⎨ +Fvi d˙i + Fsi sgn (d˙i )
τi =
⎪ μi T Ri−1 T z 0 + kri Im ω̇ i−1 T z i−1
⎪
mi
mi
i
i
⎪
⎩ i
+Fvi ϑ̇i + Fsi sgn (ϑ̇i )

+ kr,i+1 q̇i+1 Imi+1 ω ii × z imi+1
for a prismatic joint
(7.114)
for a revolute joint.
i

The above equations have the advantage that the quantities Ī i , r ii,Ci , z i−1
mi
are constant; further, it is z 0 = [ 0 0 1 ]T .
To summarize, for given joint positions, velocities and accelerations, the
recursive algorithm is carried out in the following two phases:

7.5 Newton–Euler Formulation

289

• With known initial conditions ω 00 , p̈00 − g 00 , and ω̇ 00 , use (7.107), (7.108),
(7.109), (7.110), (7.111), for i = 1, . . . , n, to compute ω ii , ω̇ ii , p̈ii , p̈iCi , ω̇ i−1
mi .
n+1
n+1
• With known terminal conditions f n+1 and μn+1 , use (7.112), (7.113), for
i = n, . . . , 1, to compute f ii , μii , and then (7.114) to compute τi .
The computational structure of the algorithm is schematically illustrated
in Fig. 7.14.
7.5.3 Example
In the following, an example to illustrate the single steps of the Newton–
Euler algorithm is developed. Consider the two-link planar arm whose dynamic model has already been derived in Example 7.2.
Start by imposing the initial conditions for the velocities and accelerations:
p̈00 − g 00 = [ 0

g

ω 00 = ω̇ 00 = 0,

0 ]T

and the terminal conditions for the forces:
f 33 = 0

μ33 = 0.

All quantities are referred to the current link frame. As a consequence, the
following constant vectors are obtained:
⎡
⎤
⎡ ⎤
⎡
⎤
⎡ ⎤
C1
a1
C2
a2
r 11,C1 = ⎣ 0 ⎦ r 10,1 = ⎣ 0 ⎦ r 22,C2 = ⎣ 0 ⎦ r 21,2 = ⎣ 0 ⎦
0
0
0
0
where C1 and C2 are both negative quantities. The rotation matrices needed
for vector transformation from one frame to another are
⎡
⎤
ci −si 0
= ⎣ si ci 0 ⎦ i = 1, 2
R23 = I.
Ri−1
i
0
0
1
Further, it is assumed that the axes of rotation of the two rotors coincide with
T
the respective joint axes, i.e., z i−1
mi = z 0 = [ 0 0 1 ] for i = 1, 2.
According to (7.107)–(7.114), the Newton–Euler algorithm requires the
execution of the following steps:
• Forward recursion: Link 1
⎡

0

⎤

⎢ ⎥
ω 11 = ⎣ 0 ⎦
ϑ̇1

290

7 Dynamics

⎡

⎤

0

⎢ ⎥
ω̇ 11 = ⎣ 0 ⎦
ϑ̈1
⎡

−a1 ϑ̇21 + gs1

⎤

⎥
⎢
p̈11 = ⎣ a1 ϑ̈1 + gc1 ⎦
0
⎡

−(C1 + a1 )ϑ̇21 + gs1

⎤

⎥
⎢
p̈1C1 = ⎣ (C1 + a1 )ϑ̈1 + gc1 ⎦
0
⎡

0

⎢
ω̇ 0m1 = ⎣

0

⎤
⎥
⎦.

kr1 ϑ̈1
• Forward recursion: Link 2
⎡

0

⎢
ω 22 = ⎣

0

⎤
⎥
⎦

ϑ̇1 + ϑ̇2
⎡

0

⎢
ω̇ 22 = ⎣

0

⎤
⎥
⎦

ϑ̈1 + ϑ̈2
⎡

a1 s2 ϑ̈1 − a1 c2 ϑ̇21 − a2 (ϑ̇1 + ϑ̇2 )2 + gs12

⎤

⎥
⎢
p̈22 = ⎣ a1 c2 ϑ̈1 + a2 (ϑ̈1 + ϑ̈2 ) + a1 s2 ϑ̇21 + gc12 ⎦
0
⎡

a1 s2 ϑ̈1 − a1 c2 ϑ̇21 − (C2 + a2 )(ϑ̇1 + ϑ̇2 )2 + gs12

⎤

⎥
⎢
p̈2C2 = ⎣ a1 c2 ϑ̈1 + (C2 + a2 )(ϑ̈1 + ϑ̈2 ) + a1 s2 ϑ̇21 + gc12 ⎦
0
⎡
⎢
ω̇ 1m2 = ⎣

0
0
ϑ̈1 + kr2 ϑ̈2

⎤
⎥
⎦.

7.5 Newton–Euler Formulation

291

• Backward recursion: Link 2
⎤
m2 a1 s2 ϑ̈1 − a1 c2 ϑ̇21 − (C2 + a2 )(ϑ̇1 + ϑ̇2 )2 + gs12
 ⎥
⎢
f 22 = ⎣ m2 a1 c2 ϑ̈1 + (C2 + a2 )(ϑ̈1 + ϑ̈2 ) + a1 s2 ϑ̇21 + gc12 ⎦
⎡

0
⎡

⎤

∗

⎢
⎥
∗
⎢
⎥
μ22 = ⎢
⎥
⎣ I¯2zz (ϑ̈1 + ϑ̈2 ) + m2 (C2 + a2 )2 (ϑ̈1 + ϑ̈2 ) + m2 a1 (C2 + a2 )c2 ϑ̈1 ⎦
+m2 a1 (C2 + a2 )s2 ϑ̇21 + m2 (C2 + a2 )gc12


τ2 = I¯2zz + m2 (C2 + a2 )2 + a1 (C2 + a2 )c2 + kr2 Im2 ϑ̈1

+ I¯2zz + m2 (C + a2 )2 + k 2 Im ϑ̈2
2

r2

2

+m2 a1 (C2 + a2 )s2 ϑ̇21 + m2 (C2 + a2 )gc12 .
• Backward recursion: Link 1
⎡

⎤
−m2 (C2 + a2 )s2 (ϑ̈1 + ϑ̈2 ) − m1 (C1 + a1 )ϑ̇21 − m2 a1 ϑ̇21
⎢
−m2 (C2 + a2 )c2 (ϑ̇1 + ϑ̇2 )2 + (m1 + m2 )gs1 ⎥
⎢
⎥
⎢
⎥
1
f 1 = ⎢ m1 (C1 + a1 )ϑ̈1 + m2 a1 ϑ̈1 + m2 (C2 + a2 )c2 (ϑ̈1 + ϑ̈2 ) ⎥
⎢
⎥
⎣
−m2 (C2 + a2 )s2 (ϑ̇1 + ϑ̇2 )2 + (m1 + m2 )gc1 ⎦
0
⎡

∗

⎤

⎥
⎢
∗
⎥
⎢
⎥
⎢
⎢ I¯1zz ϑ̈1 + m2 a21 ϑ̈1 + m1 (C + a1 )2 ϑ̈1 + m2 a1 (C + a2 )c2 ϑ̈1 ⎥
1
2
⎥
⎢
⎥
μ11 = ⎢
+I¯2zz (ϑ̈1 + ϑ̈2 ) + m2 a1 (C2 + a2 )c2 (ϑ̈1 + ϑ̈2 )
⎥
⎢
⎥
⎢
⎥
⎢
+m2 (C2 + a2 )2 (ϑ̈1 + ϑ̈2 ) + kr2 Im2 ϑ̈2
⎥
⎢
⎣
+m2 a1 (C2 + a2 )s2 ϑ̇21 − m2 a1 (C2 + a2 )s2 (ϑ̇1 + ϑ̇2 )2 ⎦
+m1 (C1 + a1 )gc1 + m2 a1 gc1 + m2 (C2 + a2 )gc12

2
τ1 = I¯1zz + m1 (C1 + a1 )2 + kr1
Im1 + I¯2zz


+m2 a21 + (C2 + a2 )2 + 2a1 (C2 + a2 )c2 ϑ̈1



+ I¯2zz + m2 (C2 + a2 )2 + a1 (C2 + a2 )c2 + kr2 Im2 ϑ̈2
−2m2 a1 (C2 + a2 )s2 ϑ̇1 ϑ̇2 − m2 a1 (C2 + a2 )s2 ϑ̇22

+ m1 (C1 + a1 ) + m2 a1 gc1 + m2 (C2 + a2 )gc12 .

292

7 Dynamics

As for the moment components, those marked by the symbol ‘∗’ have not
been computed, since they are not related to the joint torques τ2 and τ1 .
Expressing the dynamic parameters in the above torques as a function of
the link and rotor parameters as in (7.83) yields

I¯1zz

I¯2zz

m1 = m 1 + mm2
m1 C1 = m 1 (1 − a1 )
2
+ m1 C1 = I)1 = I 1 + m 1 (1 − a1 )2 + Im2
m2 = m 2
m2 C2 = m 2 (2 − a2 )
+ m2 2C2 = I)2 = I 2 + m 2 (2 − a2 )2 .

On the basis of these relations, it can be veriﬁed that the resulting dynamic
model coincides with the model derived in (7.82) with Lagrange formulation.

7.6 Direct Dynamics and Inverse Dynamics
Both Lagrange formulation and Newton–Euler formulation allow the computation of the relationship between the joint torques — and, if present, the
end-eﬀector forces — and the motion of the structure. A comparison between
the two approaches reveals what follows. The Lagrange formulation has the
following advantages:
• It is systematic and of immediate comprehension.
• It provides the equations of motion in a compact analytical form containing
the inertia matrix, the matrix in the centrifugal and Coriolis forces, and
the vector of gravitational forces. Such a form is advantageous for control
design.
• It is eﬀective if it is wished to include more complex mechanical eﬀects
such as ﬂexible link deformation.
The Newton–Euler formulation has the following fundamental advantage:
• It is an inherently recursive method that is computationally eﬃcient.
In the study of dynamics, it is relevant to ﬁnd a solution to two kinds of
problems concerning computation of direct dynamics and inverse dynamics.
The direct dynamics problem consists of determining, for t > t0 , the joint
accelerations q̈(t) (and thus q̇(t), q(t)) resulting from the given joint torques
τ (t) — and the possible end-eﬀector forces he (t) — once the initial positions
q(t0 ) and velocities q̇(t0 ) are known (initial state of the system).
The inverse dynamics problem consists of determining the joint torques
τ (t) which are needed to generate the motion speciﬁed by the joint accelerations q̈(t), velocities q̇(t), and positions q(t) — once the possible end-eﬀector
forces he (t) are known.

7.6 Direct Dynamics and Inverse Dynamics

293

Solving the direct dynamics problem is useful for manipulator simulation.
Direct dynamics allows the motion of the real physical system to be described
in terms of the joint accelerations, when a set of assigned joint torques is
applied to the manipulator; joint velocities and positions can be obtained by
integrating the system of nonlinear diﬀerential equations.
Since the equations of motion obtained with Lagrange formulation give the
analytical relationship between the joint torques (and the end-eﬀector forces)
and the joint positions, velocities and accelerations, these can be computed
from (7.42) as
(7.115)
q̈ = B −1 (q)(τ − τ  )
where
τ  (q, q̇) = C(q, q̇)q̇ + F v q̇ + F s sgn (q̇) + g(q) + J T (q)he

(7.116)

denotes the torque contributions depending on joint positions and velocities.
Therefore, for simulation of manipulator motion, once the state at the time
instant tk is known in terms of the position q(tk ) and velocity q̇(tk ), the acceleration q̈(tk ) can be computed by (7.115). Then using a numerical integration
method, e.g., Runge–Kutta, with integration step Δt, the velocity q̇(tk+1 ) and
position q(tk+1 ) at the instant tk+1 = tk + Δt can be computed.
If the equations of motion are obtained with Newton–Euler formulation,
it is possible to compute direct dynamics by using a computationally more
eﬃcient method. In fact, for given q and q̇, the torques τ  (q, q̇) in (7.116) can
be computed as the torques given by the algorithm of Fig. 7.14 with q̈ = 0.
Further, column bi of matrix B(q) can be computed as the torque vector
given by the algorithm of Fig. 7.14 with g 0 = 0, q̇ = 0, q̈i = 1 and q̈j = 0
for j = i; iterating this procedure for i = 1, . . . , n leads to constructing the
matrix B(q). Hence, from the current values of B(q) and τ  (q, q̇), and the
given τ , the equations in (7.115) can be integrated as illustrated above.
Solving the inverse dynamics problem is useful for manipulator trajectory
planning and control algorithm implementation. Once a joint trajectory is
speciﬁed in terms of positions, velocities and accelerations (typically as a result of an inverse kinematics procedure), and if the end-eﬀector forces are
known, inverse dynamics allows computation of the torques to be applied to
the joints to obtain the desired motion. This computation turns out to be
useful both for verifying feasibility of the imposed trajectory and for compensating nonlinear terms in the dynamic model of a manipulator. To this
end, Newton–Euler formulation provides a computationally eﬃcient recursive
method for on-line computation of inverse dynamics. Nevertheless, it can be
shown that also Lagrange formulation is liable to a computationally eﬃcient
recursive implementation, though with a nonnegligible reformulation eﬀort.
For an n-joint manipulator the number of operations required is:8

8

See Sect. E.1 for the deﬁnition of computational complexity of an algorithm.

294

7 Dynamics

• O(n2 ) for computing direct dynamics,
• O(n) for computing inverse dynamics.

7.7 Dynamic Scaling of Trajectories
The existence of dynamic constraints to be taken into account for trajectory
generation has been mentioned in Sect. 4.1. In practice, with reference to the
given trajectory time or path shape (segments with high curvature), the trajectories that can be obtained with any of the previously illustrated methods
may impose too severe dynamic performance for the manipulator. A typical
case is that when the required torques to generate the motion are larger than
the maximum torques the actuators can supply. In this case, an infeasible
trajectory has to be suitably time-scaled.
Suppose a trajectory has been generated for all the manipulator joints
as q(t), for t ∈ [0, tf ]. Computing inverse dynamics allows the evaluation of
the time history of the torques τ (t) required for the execution of the given
motion. By comparing the obtained torques with the torque limits available
at the actuators, it is easy to check whether or not the trajectory is actually
executable. The problem is then to seek an automatic trajectory dynamic
scaling technique — avoiding inverse dynamics recomputation — so that the
manipulator can execute the motion on the speciﬁed path with a proper timing
law without exceeding the torque limits.
Consider the manipulator dynamic model as given in (7.42) with F v =
O, F s = O and he = 0, for simplicity. The term C(q, q̇) accounting for
centrifugal and Coriolis forces has a quadratic dependence on joint velocities,
and thus it can be formally rewritten as
C(q, q̇)q̇ = Γ (q)[q̇ q̇],

(7.117)

where [q̇ q̇] indicates the symbolic notation of the (n(n + 1)/2 × 1) vector
[q̇ q̇] = [ q̇12

q̇1 q̇2

...

q̇n−1 q̇n

q̇n2 ]T ;

Γ (q) is a proper (n × n(n + 1)/2) matrix that satisﬁes (7.117). In view of such
position, the manipulator dynamic model can be expressed as
B(q(t))q̈(t) + Γ (q(t))[q̇(t)q̇(t)] + g(q(t)) = τ (t),

(7.118)

where the explicit dependence on time t has been shown.
Consider the new variable q̄(r(t)) satisfying the equation
q(t) = q̄(r(t)),

(7.119)

where r(t) is a strictly increasing scalar function of time with r(0) = 0 and
r(tf ) = t̄f .

7.7 Dynamic Scaling of Trajectories

295

Diﬀerentiating (7.119) twice with respect to time provides the following
relations:
q̇ = ṙq̄  (r)
q̈ = ṙ2 q̄  (r) + r̈q̄  (r)

(7.120)
(7.121)

where the prime denotes the derivative with respect to r. Substituting (7.120),
(7.121) into (7.118) yields


ṙ2 B(q̄(r))q̄  (r) + Γ (q̄(r))[q̄  (r)q̄  (r)] + r̈B(q̄(r))q̄  (r) + g(q̄(r)) = τ .
(7.122)
In (7.118) it is possible to identify the term
τ s (t) = B(q(t))q̈(t) + Γ (q(t))[q̇(t)q̇(t)],

(7.123)

representing the torque contribution that depends on velocities and accelerations. Correspondingly, in (7.122) one can set


τ s (t) = ṙ2 B(q̄(r))q̄  (r) + Γ (q̄(r))[q̄  (r)q̄  (r)] + r̈B(q̄(r))q̄  (r). (7.124)
By analogy with (7.123), it can be written
τ̄ s (r) = B(q̄(r))q̄  (r) + Γ (q̄(r))[q̄  (r)q̄  (r)]

(7.125)

and then (7.124) becomes
τ s (t) = ṙ2 τ̄ s (r) + r̈B(q̄(r))q̄  (r).

(7.126)

The expression in (7.126) gives the relationship between the torque contributions depending on velocities and accelerations required by the manipulator
when this is subject to motions having the same path but diﬀerent timing
laws, obtained through a time scaling of joint variables as in (7.119).
Gravitational torques have not been considered, since they are a function
of the joint positions only, and thus their contribution is not inﬂuenced by
time scaling.
The simplest choice for the scaling function r(t) is certainly the linear
function
r(t) = ct
with c a positive constant. In this case, (7.126) becomes
τ s (t) = c2 τ̄ s (ct),
which reveals that a linear time scaling by c causes a scaling of the magnitude
of the torques by the coeﬃcient c2 . Let c > 1: (7.119) shows that the trajectory
described by q̄(r(t)), assuming r = ct as the independent variable, has a
duration t̄f > tf to cover the entire path speciﬁed by q. Correspondingly, the

296

7 Dynamics

torque contributions τ̄ s (ct) computed as in (7.125) are scaled by the factor c2
with respect to the torque contributions τ s (t) required to execute the original
trajectory q(t).
With the use of a recursive algorithm for inverse dynamics computation,
it is possible to check whether the torques exceed the allowed limits during
trajectory execution; obviously, limit violation should not be caused by the
sole gravity torques. It is necessary to ﬁnd the joint for which the torque
has exceeded the limit more than the others, and to compute the torque
contribution subject to scaling, which in turn determines the factor c2 . It
is then possible to compute the time-scaled trajectory as a function of the
new time variable r = ct which no longer exceeds torque limits. It should be
pointed out, however, that with this kind of linear scaling the entire trajectory
may be penalized, even when a torque limit on a single joint is exceeded only
for a short interval of time.

7.8 Operational Space Dynamic Model
As an alternative to the joint space dynamic model, the equations of motion
of the system can be expressed directly in the operational space; to this end it
is necessary to ﬁnd a dynamic model which describes the relationship between
the generalized forces acting on the manipulator and the number of minimal
variables chosen to describe the end-eﬀector position and orientation in the
operational space.
Similar to kinematic description of a manipulator in the operational space,
the presence of redundant DOFs and/or kinematic and representation singularities deserves careful attention in the derivation of an operational space
dynamic model.
The determination of the dynamic model with Lagrange formulation using
operational space variables allows a complete description of the system motion
only in the case of a nonredundant manipulator, when the above variables
constitute a set of generalized coordinates in terms of which the kinetic energy,
the potential energy, and the nonconservative forces doing work on them can
be expressed.
This way of proceeding does not provide a complete description of dynamics for a redundant manipulator; in this case, in fact, it is reasonable to
expect the occurrence of internal motions of the structure caused by those
joint generalized forces which do not aﬀect the end-eﬀector motion.
To develop an operational space model which can be adopted for both
redundant and nonredundant manipulators, it is then convenient to start from
the joint space model which is in all general. In fact, solving (7.42) for the joint
accelerations, and neglecting the joint friction torques for simplicity, yields
q̈ = −B −1 (q)C(q, q̇)q̇ − B −1 (q)g(q) + B −1 (q)J T (q)(γ e − he ),

(7.127)

7.8 Operational Space Dynamic Model

297

where the joint torques τ have been expressed in terms of the equivalent endeﬀector forces γ according to (3.111). It is worth noting that h represents the
contribution of the end-eﬀector forces due to contact with the environment,
whereas γ expresses the contribution of the end-eﬀector forces due to joint
actuation.
On the other hand, the second-order diﬀerential kinematics equation
in (3.98) describes the relationship between joint space and operational space
accelerations, i.e.,
ẍe = J A (q)q̈ + J̇ A (q, q̇)q̇.
The solution in (7.127) features the geometric Jacobian J , whereas the analytical Jacobian J A appears in (3.98). For notation uniformity, in view of (3.66),
one can set
T TA (xe )he = hA
(7.128)
T TA (xe )γ e = γ A
where T A is the transformation matrix between the two Jacobians. Substituting (7.127) into (3.98) and accounting for (7.128) gives
ẍe = −J A B −1 C q̇ − J A B −1 g + J̇ A q̇ + J A B −1 J TA (γ A − hA ).

(7.129)

where the dependence on q and q̇ has been omitted. With the positions
B A = (J A B −1 J TA )−1
C A ẋe = B A J A B −1 C q̇ − B A J̇ A q̇
g A = B A J A B −1 g,

(7.130)
(7.131)
(7.132)

the expression in (7.129) can be rewritten as
B A (xe )ẍe + C A (xe , ẋe )ẋe + g A (xe ) = γ A − hA ,

(7.133)

which is formally analogous to the joint space dynamic model (7.42). Notice
that the matrix J A B −1 J TA is invertible if and only if J A is full-rank, that is,
in the absence of both kinematic and representation singularities.
For a nonredundant manipulator in a nonsingular conﬁguration, the expressions in (7.130)–(7.132) become:
−1
B A = J −T
A BJ A

C A ẋe =
gA =

J −T
A C q̇
J −T
A g.

− B A J̇ A q̇

(7.134)
(7.135)
(7.136)

As anticipated above, the main feature of the obtained model is its formal
validity also for a redundant manipulator, even though the variables xe do
not constitute a set of generalized coordinates for the system; in this case, the
matrix B A is representative of a kinetic pseudo-energy.
In the following, the utility of the operational space dynamic model
in (7.133) for solving direct and inverse dynamics problems is investigated. The

298

7 Dynamics

following derivation is meaningful for redundant manipulators; for a nonredundant manipulator, in fact, using (7.133) does not pose speciﬁc problems
as long as J A is nonsingular ((7.134)–(7.136)).
With reference to operational space, the direct dynamics problem consists
of determining the resulting end-eﬀector accelerations ẍe (t) (and thus ẋe (t),
xe (t)) from the given joint torques τ (t) and end-eﬀector forces he (t). For a
redundant manipulator, (7.133) cannot be directly used, since (3.111) has a
solution in γ e only if τ ∈ R(J T ). It follows that for simulation purposes,
the solution to the problem is naturally obtained in the joint space; in fact,
the expression in (7.42) allows the computation of q, q̇, q̈ which, substituted
into the direct kinematics equations in ((2.82), (3.62), (3.98), give xe , ẋe , ẍe ,
respectively.
Formulation of an inverse dynamics problem in the operational space requires the determination of the joint torques τ (t) that are needed to generate
a speciﬁc motion assigned in terms of ẍe (t), ẋe (t), xe (t), for given end-eﬀector
forces he (t). A possible way of solution is to solve a complete inverse kinematics problem for (2.82), (3.62), (3.98), and then compute the required torques
with the joint space inverse dynamics as in (7.42). Hence, for redundant manipulators, redundancy resolution is performed at kinematic level.
An alternative solution to the inverse dynamics problem consists of computing γ A as in (7.133) and the joint torques τ as in (3.111). In this way,
however, the presence of redundant DOFs is not exploited at all, since the
computed torques do not generate internal motions of the structure.
If it is desired to ﬁnd a formal solution that allows redundancy resolution
at dynamic level, it is necessary to determine those torques corresponding to
the equivalent end-eﬀector forces computed as in (7.133). By analogy with
the diﬀerential kinematics solution (3.54), the expression of the torques to be
determined will feature the presence of a minimum-norm term and a homogeneous term. Since the joint torques have to be computed, it is convenient to
express the model (7.133) in terms of q, q̇, q̈. By recalling the positions (7.131),
(7.132), the expression in (7.133) becomes
B A (ẍe − J̇ A q̇) + B A J A B −1 C q̇ + B A J A B −1 g = γ A − hA
and, in view of (3.98),
B A J A q̈ + B A J A B −1 C q̇ + B A J A B −1 g = γ A − hA .
By setting

J̄ A (q) = B −1 (q)J TA (q)B A (q),

(7.137)
(7.138)

the expression in (7.137) becomes
T

J̄ A (Bq̈ + C q̇ + g) = γ A − hA .

(7.139)

At this point, from the joint space dynamic model in (7.42), it is easy to
recognize that (7.139) can be written as
T

J̄ A (τ − J TA hA ) = γ A − hA

7.9 Dynamic Manipulability Ellipsoid

from which

T

J̄ A τ = γ A .

299

(7.140)

The general solution to (7.140) is of the form (see Problem 7.10)

T
τ = J TA (q)γ A + I n − J TA (q)J̄ A (q) τ 0 ,

(7.141)

that can be derived by observing that J TA in (7.138) is a right pseudo-inverse
T
of J̄ A weighted by the inverse of the inertia matrix B −1 . The (n × 1) vector of
arbitrary torques τ 0 in (7.141) does not contribute to the end-eﬀector forces,
T
since it is projected in the null space of J̄ A .
To summarize, for given xe , ẋe , ẍe and hA , the expression in (7.133)
allows the computation of γ A . Then, (7.141) gives the torques τ which, besides
executing the assigned end-eﬀector motion, generate internal motions of the
structure to be employed for handling redundancy at dynamic level through
a suitable choice of τ 0 .

7.9 Dynamic Manipulability Ellipsoid
The availability of the dynamic model allows formulation of the dynamic manipulability ellipsoid which provides a useful tool for manipulator dynamic
performance analysis. This can be used for mechanical structure design as
well as for seeking optimal manipulator conﬁgurations.
Consider the set of joint torques of constant (unit) norm
τTτ = 1

(7.142)

describing the points on the surface of a sphere. It is desired to describe the
operational space accelerations that can be generated by the given set of joint
torques.
For studying dynamic manipulability, suppose to consider the case of a
manipulator standing still (q̇ = 0), not in contact with the environment (he =
0). The simpliﬁed model is
B(q)q̈ + g(q) = τ .

(7.143)

The joint accelerations q̈ can be computed from the second-order diﬀerential kinematics that can be obtained by diﬀerentiating (3.39), and imposing
successively q̇ = 0, leading to
v̇ e = J (q)q̈.

(7.144)

Solving for minimum-norm accelerations only, for a nonsingular Jacobian, and
substituting in (7.143) yields the expression of the torques
τ = B(q)J † (q)v̇ e + g(q)

(7.145)

300

7 Dynamics
1

0.5

−1

[m]

JB g
0

−0.5
−0.5

0

[m]

0.5

1

Fig. 7.15. Eﬀect of gravity on the dynamic manipulability ellipsoid for a three-link
planar arm

needed to derive the ellipsoid. In fact, substituting (7.145) into (7.142) gives
T

B(q)J † (q)v̇ e + g(q) = 1.
B(q)J † (q)v̇ e + g(q)
The vector on the right-hand side of (7.145) can be rewritten as
BJ † v̇ e + g = B(J † v̇ e + B −1 g)

(7.146)

= B(J † v̇ e + B −1 g + J † J B −1 g − J † J B −1 g)

= B J † v̇ e + J † J B −1 g + (I n − J † J )B −1 g ,
where the dependence on q has been omitted. According to what was done
for solving (7.144), one can neglect the contribution of the accelerations given
by B −1 g which are in the null space of J and then produce no end-eﬀector
acceleration. Hence, (7.146) becomes
BJ † v̇ e + g = BJ † (v̇ e + J B −1 g)

(7.147)

and the dynamic manipulability ellipsoid can be expressed in the form
(v̇ e + J B −1 g)T J †T B T BJ † (v̇ e + J B −1 g) = 1.

(7.148)

The core of the quadratic form J † T B T BJ † depends on the geometrical and
inertial characteristics of the manipulator and determines the volume and
principal axes of the ellipsoid. The vector −J B −1 g, describing the contribution of gravity, produces a constant translation of the centre of the ellipsoid
(for each manipulator conﬁguration) with respect to the origin of the reference
frame; see the example in Fig. 7.15 for a three-link planar arm.
The meaning of the dynamic manipulability ellipsoid is conceptually similar to that of the ellipsoids considered with reference to kineto-statics duality.
In fact, the distance of a point on the surface of the ellipsoid from the endeﬀector gives a measure of the accelerations which can be imposed to the
end-eﬀector along the given direction, with respect to the constraint (7.142).
With reference to Fig. 7.15, it is worth noticing how the presence of gravity

Problems

301

acceleration allows the execution of larger accelerations downward, as natural
to predict.
In the case of a nonredundant manipulator, the ellipsoid reduces to
(v̇ e + J B −1 g)T J −T B T BJ −1 (v̇ e + J B −1 g) = 1.

(7.149)

Bibliography
The derivation of the dynamic model for rigid manipulators can be found in
several classical robotics texts, such as [180, 10, 248, 53, 217, 111].
The ﬁrst works on the computation of the dynamic model of open-chain
manipulators based on the Lagrange formulation are [234, 19, 221, 236]. A
computationally eﬃcient formulation is presented in [96].
Dynamic model computation for robotic systems having a closed-chain or
a tree kinematic structure can be found in [11, 144] and [112], respectively.
Joint friction models are analyzed in [9].
The notable properties of the dynamic model deriving from the principle of
energy conservation are underlined in [213], on the basis of the work in [119].
Algorithms to ﬁnd the parameterization of the dynamic model in terms of
a minimum number of parameters are considered in [115], which utilizes the
results in [166]. Methods for symbolic computation of those parameters are
presented in [85] for open kinematic chains and [110] for closed kinematic
chains. Parameter identiﬁcation methods based on least-squares techniques
are given in [13].
The Newton–Euler formulation is proposed in [172], and a computationally
eﬃcient version for inverse dynamics can be found in [142]; an analogous formulation is employed for direct dynamics computation in [237]. The Lagrange
and Newton–Euler formulations are compared by a computational viewpoint
in [211], while they are utilized in [201] for dynamic model computation with
inclusion of inertial and gyroscopic eﬀects of actuators. Eﬃcient algorithms
for direct dynamics computation are given in [76, 77].
The trajectory dynamic scaling technique is presented in [97]. The operational space dynamic model is illustrated in [114] and the concept of weighted
pseudo-inverse of the inertia matrix is introduced in [78]. The manipulability
ellipsoids are analyzed in [246, 38].

Problems
7.1. Find the dynamic model of a two-link Cartesian arm in the case when
the second joint axis forms an angle of π/4 with the ﬁrst joint axis; compare
the result with the model of the manipulator in Fig. 7.3.
7.2. For the two-link planar arm of Sect. 7.3.2, prove that with a diﬀerent
choice of the matrix C, (7.49) holds true while (7.48) does not.

302

7 Dynamics

Fig. 7.16. Two-link planar arm with a prismatic joint and a revolute joint

7.3. Find the dynamic model of the SCARA manipulator in Fig. 2.36.
7.4. For the planar arm of Sect. 7.3.2, ﬁnd a minimal parameterization of the
dynamic model in (7.82).
7.5. Find the dynamic model of the two-link planar arm with a prismatic
joint and a revolute joint in Fig. 7.16 with the Lagrange formulation. Then,
consider the addition of a concentrated tip payload of mass mL , and express
the resulting model in a linear form with respect to a suitable set of dynamic
parameters as in (7.81).
7.6. For the two-link planar arm of Fig. 7.4, ﬁnd the dynamic model with
the Lagrange formulation when the absolute angles with respect to the base
frame are chosen as generalized coordinates. Discuss the result in view of a
comparison with the model derived in (7.82).
7.7. Compute the joint torques for the two-link planar arm of Fig. 7.4 with
the data and along the trajectories of Example 7.2, in the case of tip forces
f = [ 500 500 ]T N.
7.8. Find the dynamic model of the two-link planar arm with a prismatic
joint and a revolute joint in Fig. 7.16 by using the recursive Newton–Euler
algorithm.
7.9. Show that for the operational space dynamic model (7.133) a skewsymmetry property holds which is analogous to (7.48).
7.10. Show how to obtain the general solution to (7.140) in the form (7.141).
7.11. For a nonredundant manipulator, compute the relationship between the
dynamic manipulability measure that can be deﬁned for the dynamic manipulability ellipsoid and the manipulability measure deﬁned in (3.56).

8
Motion Control

In Chap. 4, trajectory planning techniques have been presented which allow the generation of the reference inputs to the motion control system. The
problem of controlling a manipulator can be formulated as that to determine
the time history of the generalized forces (forces or torques) to be developed
by the joint actuators, so as to guarantee execution of the commanded task
while satisfying given transient and steady-state requirements. The task may
regard either the execution of speciﬁed motions for a manipulator operating
in free space, or the execution of speciﬁed motions and contact forces for a
manipulator whose end-eﬀector is constrained by the environment. In view of
problem complexity, the two aspects will be treated separately; ﬁrst, motion
control in free space, and then control of the interaction with the environment. The problem of motion control of a manipulator is the topic of this
chapter. A number of joint space control techniques are presented. These can
be distinguished between decentralized control schemes, i.e., when the single
manipulator joint is controlled independently of the others, and centralized
control schemes, i.e., when the dynamic interaction eﬀects between the joints
are taken into account. Finally, as a premise to the interaction control problem, the basic features of operational space control schemes are illustrated.

8.1 The Control Problem
Several techniques can be employed for controlling a manipulator. The technique followed, as well as the way it is implemented, may have a signiﬁcant
inﬂuence on the manipulator performance and then on the possible range of
applications. For instance, the need for trajectory tracking control in the operational space may lead to hardware/software implementations, which diﬀer
from those allowing point-to-point control, where only reaching of the ﬁnal
position is of concern.
On the other hand, the manipulator mechanical design has an inﬂuence
on the kind of control scheme utilized. For instance, the control problem of

304

8 Motion Control

Fig. 8.1. General scheme of joint space control

a Cartesian manipulator is substantially diﬀerent from that of an anthropomorphic manipulator.
The driving system of the joints also has an eﬀect on the type of control
strategy used. If a manipulator is actuated by electric motors with reduction
gears of high ratios, the presence of gears tends to linearize system dynamics, and thus to decouple the joints in view of the reduction of nonlinearity
eﬀects. The price to pay, however, is the occurrence of joint friction, elasticity and backlash that may limit system performance more than it is due to
conﬁguration-dependent inertia, Coriolis and centrifugal forces, and so forth.
On the other hand, a robot actuated with direct drives eliminates the drawbacks due to friction, elasticity and backlash, but the weight of nonlinearities
and couplings between the joints becomes relevant. As a consequence, diﬀerent
control strategies have to be thought of to obtain high performance.
Without any concern to the speciﬁc type of mechanical manipulator, it
is worth remarking that task speciﬁcation (end-eﬀector motion and forces) is
usually carried out in the operational space, whereas control actions (joint
actuator generalized forces) are performed in the joint space. This fact naturally leads to considering two kinds of general control schemes, namely, a
joint space control scheme (Fig. 8.1) and an operational space control scheme
(Fig. 8.2). In both schemes, the control structure has closed loops to exploit
the good features provided by feedback, i.e., robustness to modelling uncertainties and reduction of disturbance eﬀects. In general terms, the following
considerations should be made.
The joint space control problem is actually articulated in two subproblems. First, manipulator inverse kinematics is solved to transform the motion
requirements xd from the operational space into the corresponding motion q d
in the joint space. Then, a joint space control scheme is designed that allows
the actual motion q to track the reference inputs. However, this solution has
the drawback that a joint space control scheme does not inﬂuence the operational space variables xe which are controlled in an open-loop fashion through
the manipulator mechanical structure. It is then clear that any uncertainty of
the structure (construction tolerance, lack of calibration, gear backlash, elasticity) or any imprecision in the knowledge of the end-eﬀector pose relative

8.2 Joint Space Control

305

Fig. 8.2. General scheme of operational space control

to an object to manipulate causes a loss of accuracy on the operational space
variables.
The operational space control problem follows a global approach that requires a greater algorithmic complexity; notice that inverse kinematics is now
embedded into the feedback control loop. Its conceptual advantage regards the
possibility of acting directly on operational space variables; this is somewhat
only a potential advantage, since measurement of operational space variables
is often performed not directly, but through the evaluation of direct kinematics
functions starting from measured joint space variables.
On the above premises, in the following, joint space control schemes for
manipulator motion in the free space are presented ﬁrst. In the sequel, operational space control schemes will be illustrated which are logically at the
basis of control of the interaction with the environment.

8.2 Joint Space Control
In Chap. 7, it was shown that the equations of motion of a manipulator in
the absence of external end-eﬀector forces and, for simplicity, of static friction
(diﬃcult to model accurately) are described by
B(q)q̈ + C(q, q̇)q̇ + F v q̇ + g(q) = τ

(8.1)

with obvious meaning of the symbols. To control the motion of the manipulator in free space means to determine the n components of generalized forces —
torques for revolute joints, forces for prismatic joints — that allow execution
of a motion q(t) so that
q(t) = q d (t),
as closely as possible, where q d (t) denotes the vector of desired joint trajectory
variables.
The generalized forces are supplied by the actuators through proper transmissions to transform the motion characteristics. Let q m denote the vector
of joint actuator displacements; the transmissions — assumed to be rigid and
with no backlash — establish the following relationship:
K r q = qm ,

(8.2)

306

8 Motion Control

Fig. 8.3. Block scheme of the manipulator and drives system as a voltage-controlled
system

where K r is an (n × n) diagonal matrix, whose elements are deﬁned in (7.22)
and are much greater than unity.1
In view of (8.2), if τ m denotes the vector of actuator driving torques, one
can write
(8.3)
τ m = K −1
r τ.
With reference to (5.1)–(5.4), the n driving systems can be described in
compact matrix form by the equations:
K −1
r τ = K t ia
v a = Ra ia + K v q̇ m
v a = Gv v c .

(8.4)
(8.5)
(8.6)

In (8.4), K t is the diagonal matrix of torque constants and ia is the vector
of armature currents of the n motors; in (8.5), v a is the vector of armature
voltages, Ra is the diagonal matrix of armature resistances,2 and K v is the
diagonal matrix of voltage constants of the n motors; in (8.6), Gv is the
diagonal matrix of gains of the n ampliﬁers and v c is the vector of control
voltages of the n servomotors.
On reduction of (8.1), (8.2), (8.4), (8.5), (8.6), the dynamic model of the
system given by the manipulator and drives is described by
B(q)q̈ + C(q, q̇)q̇ + F q̇ + g(q) = u

(8.7)

where the following positions have been made:
F = F v + K r K t R−1
a KvKr
−1
u = K r K t R a Gv v c .

(8.8)
(8.9)

From (8.1), (8.7), (8.8), (8.9) it is
−1
K r K t R−1
a Gv v c = τ + K r K t Ra K v K r q̇
1

2

(8.10)

Assuming a diagonal K r leads to excluding the presence of kinematic couplings
in the transmission, that is the motion of each actuator does not induce motion
on a joint other than that actuated.
The contribution of the inductance has been neglected.

8.2 Joint Space Control

307

Fig. 8.4. Block scheme of the manipulator and drives system as a torque-controlled
system

and thus

τ = K r K t R−1
a (Gv v c − K v K r q̇).

(8.11)

The overall system is then voltage-controlled and the corresponding block
scheme is illustrated in Fig. 8.3. If the following assumptions hold:
• the elements of matrix K r , characterizing the transmissions, are much
greater than unity;
• the elements of matrix Ra are very small, which is typical in the case of
high-eﬃciency servomotors;
• the values of the torques τ required for the execution of the desired motions
are not too large;
then it can be assumed that
Gv v c ≈ K v K r q̇.

(8.12)

The proportionality relationship obtained between q̇ and v c is independent
of the values attained by the manipulator parameters; the smaller the joint
velocities and accelerations, the more valid this assumption. Hence, velocity
(or voltage) control shows an inherent robustness with respect to parameter
variations of the manipulator model, which is enhanced by the values of the
gear reduction ratios.
In this case, the scheme illustrated in Fig. 8.3 can be taken as the reference
structure for the design of the control system. Having assumed that
v c ≈ G−1
v K v K r q̇

(8.13)

implies that the velocity of the i-th joint depends only on the i-th control voltage, since the matrix G−1
v K v K r is diagonal. Therefore, the joint position
control system can be designed according to a decentralized control structure,
since each joint can be controlled independently of the others. The results,
evaluated in the terms of the tracking accuracy of the joint variables with
respect to the desired trajectories, are improved in the case of higher gear reduction ratios and less demanding values of required speeds and accelerations.
On the other hand, if the desired manipulator motion requires large joint
speeds and/or accelerations, the approximation (8.12) no longer holds, in view
of the magnitude of the required driving torques; this occurrence is even more
evident for direct-drive actuation (K r = I).

308

8 Motion Control

In this case, by resorting to an inverse dynamics technique, it is possible
to ﬁnd the joint torques τ (t) needed to track any speciﬁed motion in terms of
the joint accelerations q̈(t), velocities q̇(t) and positions q(t). Obviously, this
solution requires the accurate knowledge of the manipulator dynamic model.
The determination of the torques to be generated by the drive system can thus
refer to a centralized control structure, since to compute the torque history at
the i-th joint it is necessary to know the time evolution of the motion of all
the joints. By recalling that
τ = K r K t ia ,

(8.14)

to ﬁnd a relationship between the torques τ and the control voltages v c ,
using (8.5), (8.6) leads to
−1
τ = K r K t R−1
a Gv v c − K r K t Ra K v K r q̇.

(8.15)

If the actuators have to provide torque contributions computed on the basis
of the manipulator dynamic model, the control voltages — to be determined
according to (8.15) — depend on the torque values and also on the joint
velocities; this relationship depends on the matrices K t , K v and R−1
a , whose
elements are inﬂuenced by the operating conditions of the motors. To reduce
sensitivity to parameter variations, it is worth considering driving systems
characterized by a current control rather than by a voltage control. In this case
the actuators behave as torque-controlled generators; the equation in (8.5)
becomes meaningless and is replaced by
ia = Gi v c ,

(8.16)

which gives a proportional relation between the armature currents ia (and
thus the torques τ ) and the control voltages v c established by the constant
matrix Gi . As a consequence, (8.9) becomes
τ = u = K r K t Gi v c

(8.17)

which shows a reduced dependence of u on the motor parameters. The overall
system is now torque-controlled and the resulting block scheme is illustrated
in Fig. 8.4.
The above presentation suggests resorting for the decentralized structure
— where the need for robustness prevails — to feedback control systems, while
for the centralized structure — where the computation of inverse dynamics is
needed — it is necessary to refer to control systems with feedforward actions.
Nevertheless, it should be pointed out that centralized control still requires
the use of error contributions between the desired and the actual trajectory,
no matter whether they are implemented in a feedback or in a feedforward
fashion. This is a consequence of the fact that the considered dynamic model,
even though a quite complex one, is anyhow an idealization of reality which

8.3 Decentralized Control

309

does not include eﬀects, such as joint Coulomb friction, gear backlash, dimension tolerance, and the simplifying assumptions in the model, e.g., link
rigidity, and so on.
As already pointed out, the drive systems is anyhow inserted into a feedback control system. In the case of decentralized control, the drive will be
characterized by the model describing its behaviour as a velocity-controlled
generator. Instead, in the case of centralized control, since the driving torque
is to be computed on a complete or reduced manipulator dynamic model, the
drive will be characterized as a torque-controlled generator.

8.3 Decentralized Control
The simplest control strategy that can be thought of is one that regards the
manipulator as formed by n independent systems (the n joints) and controls each joint axis as a single-input/single-output system. Coupling eﬀects
between joints due to varying conﬁgurations during motion are treated as
disturbance inputs.
In order to analyze various control schemes and their performance, it is
worth considering the model of the system manipulator with drives in terms
of mechanical quantities at the motor side; in view of (8.2), (8.3), it is
−1
−1
−1
−1
−1
−1
K −1
r B(q)K r q̈ m + K r C(q, q̇)K r q̇ m + K r F v K r + K r g(q) = τ m .
(8.18)
By observing that the diagonal elements of B(q) are formed by constant terms
and conﬁguration-dependent terms (functions of sine and cosine for revolute
joints), one can set
B(q) = B̄ + ΔB(q)
(8.19)

where B̄ is the diagonal matrix whose constant elements represent the resulting average inertia at each joint. Substituting (8.19) into (8.1) yields

where

−1
K −1
r B̄K r q̈ m + F m q̇ m + d = τ m

(8.20)

−1
F m = K −1
r F vKr

(8.21)

represents the matrix of viscous friction coeﬃcients about the motor axes, and
−1
−1
−1
−1
d = K −1
r ΔB(q)K r q̈ m + K r C(q, q̇)K r q̇ m + K r g(q)

(8.22)

represents the contribution depending on the conﬁguration.
As illustrated by the block scheme of Fig. 8.5, the system of manipulator
with drives is actually constituted by two subsystems; one has τ m as input
and q m as output, the other has q m , q̇ m , q̈ m as inputs, and d as output. The
former is linear and decoupled , since each component of τ m inﬂuences only the
corresponding component of q m . The latter is nonlinear and coupled , since

310

8 Motion Control

Fig. 8.5. Block scheme of the system of manipulator with drives

it accounts for all those nonlinear and coupling terms of manipulator joint
dynamics.
On the basis of the above scheme, several control algorithms can be derived
with reference to the detail of knowledge of the dynamic model. The simplest
approach that can be followed, in case of high-gear reduction ratios and/or
limited performance in terms of required velocities and accelerations, is to
consider the component of the nonlinear interacting term d as a disturbance
for the single joint servo.
The design of the control algorithm leads to a decentralized control structure, since each joint is considered independently of the others. The joint
controller must guarantee good performance in terms of high disturbance rejection and enhanced trajectory tracking capabilities. The resulting control
structure is substantially based on the error between the desired and actual
output, while the input control torque at actuator i depends only on the error
of output i.
Therefore, the system to control is Joint i drive corresponding to the singleinput/single-output system of the decoupled and linear part of the scheme in
Fig. 8.5. The interaction with the other joints is described by component i of
the vector d in (8.22).

8.3 Decentralized Control

311

Fig. 8.6. Block scheme of general independent joint control

Assumed that the actuator is a rotary electric DC motor, the general
scheme of drive control is that in Fig. 5.9 where Im is the average inertia
2 3
reported to the motor axis (Imi = b̄ii /kri
).
8.3.1 Independent Joint Control
To guide selection of the controller structure, start noticing that an eﬀective
rejection of the disturbance d on the output ϑm is ensured by:
• a large value of the ampliﬁer gain before the point of intervention of the
disturbance,
• the presence of an integral action in the controller so as to cancel the eﬀect
of the gravitational component on the output at steady state (constant
ϑm ).
These requisites clearly suggest the use of a proportional-integral (PI) control action in the forward path whose transfer function is
C(s) = Kc

1 + sTc
;
s

(8.23)

this yields zero error at steady state for a constant disturbance, and the presence of the real zero at s = −1/Tc oﬀers a stabilizing action. To improve
dynamic performance, it is worth choosing the controller as a cascade of elementary actions with local feedback loops closed around the disturbance.
Besides closure of a position feedback loop, the most general solution is
obtained by closing inner loops on velocity and acceleration. This leads to
the scheme in Fig. 8.6, where CP (s), CV (s), CA (s) respectively represent
position, velocity, acceleration controllers, and the inmost controller should
3

Subscript i is to be dropped for notation compactness.

312

8 Motion Control

be of PI type as in (8.23) so as to obtain zero error at steady state for a
constant disturbance. Further, kT P , kT V , kT A are the respective transducer
constants, and the ampliﬁer gain Gv has been embedded in the gain of the
inmost controller. In the scheme of Fig. 8.6, notice that ϑr is the reference
input, which is related to the desired output ϑmd as
ϑr = kT P ϑmd .
Further, the disturbance torque D has been suitably transformed into a voltage by the factor Ra /kt .
In the following, a number of possible solutions that can be derived from
the general scheme of Fig. 8.6 are presented; at this stage, the issue arising
from possible lack of measurement of physical variables is not considered yet.
Three case studies are considered which diﬀer in the number of active feedback
loops.4
Position feedback
In this case, the control action is characterized by
CP (s) = KP

1 + sTP
s

CV (s) = 1

CA (s) = 1

kT V = kT A = 0.
With these positions, the structure of the control scheme in Fig. 8.6 leads to
the scheme illustrated in Fig. 5.10. From this scheme the transfer function of
the forward path is
km KP (1 + sTP )
,
P (s) =
s2 (1 + sTm )
while that of the return path is
H(s) = kT P .
A root locus analysis can be performed as a function of the gain of the position loop km KP kT P TP /Tm . Three situations are illustrated for the poles
of the closed-loop system with reference to the relation between TP and Tm
(Fig. 8.7). Stability of the closed-loop feedback system imposes some constraints on the choice of the parameters of the PI controller. If TP < Tm ,
the system is inherently unstable (Fig. 8.7a). Then, it must be TP > Tm
(Fig. 8.7b). As TP increases, the absolute value of the real part of the two
roots of the locus tending towards the asymptotes increases too, and the sysTm
tem has faster time response. Hence, it is convenient to render TP
(Fig. 8.7c). In any case, the real part of the dominant poles cannot be less
than −1/2Tm .
4

See Appendix C for a brief brush-up on control of linear single-input/single-output
systems.

8.3 Decentralized Control

313

Fig. 8.7. Root loci for the position feedback control scheme

The closed-loop input/output transfer function is
Θm (s)
=
Θr (s)

1
kT P
,
s2 (1 + sTm )
1+
km KP kT P (1 + sTP )

(8.24)

which can be expressed in the form
1
(1 + sTP )
kT P

,
W (s) = 
s2
2ζs
+ 2 (1 + sτ )
1+
ωn
ωn
where ωn and ζ are respectively the natural frequency and damping ratio of
the pair of complex poles and −1/τ locates the real pole. These values are
assigned to deﬁne the joint drive dynamics as a function of the constant TP ;
Tm (Fig. 8.7c), for
if TP > Tm , then 1/ζωn > TP > τ (Fig. 8.7b); if TP
large values of the loop gain, then ζωn > 1/τ ≈ 1/TP and the zero at −1/TP
in the transfer function W (s) tends to cancel the eﬀect of the real pole.

314

8 Motion Control

The closed-loop disturbance/output transfer function is
sRa
Θm (s)
kt KP kT P (1 + sTP )
=−
,
D(s)
s2 (1 + sTm )
1+
km KP kT P (1 + sTP )

(8.25)

which shows that it is worth increasing KP to reduce the eﬀect of disturbance
on the output during the transient. The function in (8.25) has two complex
poles (−ζωn , ±j 1 − ζ 2 ωn ), a real pole (−1/τ ), and a zero at the origin. The
zero is due to the PI controller and allows the cancellation of the eﬀects of
gravity on the angular position when ϑm is a constant.
In (8.25), it can be recognized that the term KP kT P is the reduction
factor imposed by the feedback gain on the amplitude of the output due to
disturbance; hence, the quantity
XR = KP kT P

(8.26)

can be interpreted as the disturbance rejection factor , which in turn is determined by the gain KP . However, it is not advisable to increase KP too
much, because small damping ratios would result leading to unacceptable oscillations of the output. An estimate TR of the output recovery time needed
by the control system to recover the eﬀects of the disturbance on the angular
position can be evaluated by analyzing the modes of evolution of (8.25). Since
τ ≈ TP , such estimate is expressed by

*
1
TR = max TP ,
.
(8.27)
ζωn
Position and velocity feedback
In this case, the control action is characterized by
CP (s) = KP

CV (s) = KV

1 + sTV
s

CA (s) = 1

kT A = 0;
with these positions, the structure of the control scheme in Fig. 8.6 leads to
scheme illustrated in Fig. 5.11. To carry out a root locus analysis as a function
of the velocity feedback loop gain, it is worth reducing the velocity loop in
parallel to the position loop by following the usual rules for moving blocks.
From the scheme in Fig. 5.11 the transfer function of the forward path is
P (s) =

km KP KV (1 + sTV )
,
s2 (1 + sTm )

8.3 Decentralized Control

315

Fig. 8.8. Root locus for the position and velocity feedback control scheme

while that of the return path is
H(s) = kT P



kT V
1+s
.
KP kT P

The zero of the controller at s = −1/TV can be chosen so as to cancel the
eﬀects of the real pole of the motor at s = −1/Tm . Then, by setting
TV = Tm ,
the poles of the closed-loop system move on the root locus as a function of the
loop gain km KV kT V , as shown in Fig. 8.8. By increasing the position feedback
gain KP , it is possible to conﬁne the closed-loop poles into a region of the
complex plane with large absolute values of the real part. Then, the actual
location can be established by a suitable choice of KV .
The closed-loop input/output transfer function is
Θm (s)
=
Θr (s)

1
kT P
1+

skT V
s2
+
KP kT P
km KP kT P KV

,

(8.28)

which can be compared with the typical transfer function of a second-order
system
1
kT P
W (s) =
.
(8.29)
2ζs
s2
1+
+ 2
ωn
ωn
It can be recognized that, with a suitable choice of the gains, it is possible to
obtain any value of natural frequency ωn and damping ratio ζ. Hence, if ωn
and ζ are given as design requirements, the following relations can be found:
KV kT V =

2ζωn
km

(8.30)

316

8 Motion Control

Fig. 8.9. Block scheme of position, velocity and acceleration feedback control

KP kT P KV =

ωn2
.
km

(8.31)

For given transducer constants kT P and kT V , once KV has been chosen to
satisfy (8.30), the value of KP is obtained from (8.31).
The closed-loop disturbance/output transfer function is
Θm (s)
=−
D(s)

sRa
kt KP kT P KV (1 + sTm )
,
skT V
s2
1+
+
KP kT P
km KP kT P KV

(8.32)

which shows that the disturbance rejection factor is
XR = KP kT P KV

(8.33)

and is ﬁxed, once KP and KV have been chosen via (8.30), (8.31). Concerning
disturbance dynamics, the presence of a zero at the origin introduced by the
PI, of a real pole at s = −1/Tm , and of a pair of complex poles having real
part −ζωn should be noticed. Hence, in this case, an estimate of the output
recovery time is given by the time constant

*
1
;
(8.34)
TR = max Tm ,
ζωn
which reveals an improvement with respect to the previous case in (8.27),
since Tm  TP and the real part of the dominant poles is not constrained by
the inequality ζωn < 1/2Tm .

8.3 Decentralized Control

317

Fig. 8.10. Root locus for the position, velocity and acceleration feedback control
scheme

Position, velocity and acceleration feedback
In this case, the control action is characterized by
CP (s) = KP

CV (s) = KV

CA (s) = KA

1 + sTA
.
s

After some manipulation, the block scheme of Fig. 8.6 can be reduced to that
of Fig. 8.9 where G (s) indicates the following transfer function:
G (s) =

km

⎛



TA
sTm 1 + km KA kT A
⎜
Tm
(1 + km KA kT A ) ⎜
⎝1 +
(1 + km KA kT A )

⎞.
⎟
⎟
⎠

The transfer function of the forward path is
P (s) =

KP KV KA (1 + sTA ) 
G (s),
s2

while that of the return path is

H(s) = kT P

skT V
1+
KP kT P


.

Also in this case, a suitable pole cancellation is worthy which can be achieved
either by setting
TA = Tm ,
or by making
km KA kT A TA

Tm

km KA kT A

1.

318

8 Motion Control

Fig. 8.11. Block scheme of a ﬁrst-order ﬁlter

The two solutions are equivalent as regards dynamic performance of the control system. In both cases, the poles of the closed-loop system are constrained
to move on the root locus as a function of the loop gain km KP KV KA /(1 +
km KA kT A ) (Fig. 8.10). A close analogy with the previous scheme can be
recognized, in that the resulting closed-loop system is again of second-order
type.
The closed-loop input/output transfer function is
1
Θm (s)
=
Θr (s)

kT P
,
skT V
s2 (1 + km KA kT A )
1+
+
KP kT P
km KP kT P KV KA

(8.35)

while the closed-loop disturbance/output transfer function is
Θm (s)
=−
D(s)

sRa
kt KP kT P KV KA (1 + sTA )
.
skT V
s2 (1 + km KA kT A )
1+
+
KP kT P
km KP kT P KV KA

(8.36)

The resulting disturbance rejection factor is given by
XR = KP kT P KV KA ,
while the output recovery time is given by the time constant

*
1
TR = max TA ,
ζωn

(8.37)

(8.38)

where TA can be made less than Tm , as pointed out above.
With reference to the transfer function in (8.29), the following relations
can be established for design purposes, once ζ, ωn , XR have been speciﬁed:
2KP kT P
ωn
=
kT V
ζ
km XR
km KA kT A =
−1
ωn2
KP kT P KV KA = XR .

(8.39)
(8.40)
(8.41)

8.3 Decentralized Control

319

For given kT P , kT V , kT A , KP is chosen to satisfy (8.39), KA is chosen to
satisfy (8.40), and then KV is obtained from (8.41). Notice how admissible
solutions for the controller typically require large values for the rejection factor XR . Hence, in principle, not only does the acceleration feedback allow the
achievement of any desired dynamic behaviour but, with respect to the previous case, it also allows the prescription of the disturbance rejection factor
as long as km XR /ωn2 > 1.
In deriving the above control schemes, the issue of measurement of feedback variables was not considered explicitly. With reference to the typical
position control servos that are implemented in industrial practice, there
is no problem of measuring position and velocity, while a direct measurement of acceleration, in general, either is not available or is too expensive to
obtain. Therefore, for the scheme of Fig. 8.9, an indirect measurement can
be obtained by reconstructing acceleration from direct velocity measurement
through a ﬁrst-order ﬁlter (Fig. 8.11). The ﬁlter is characterized by a bandwidth ω3f = kf . By choosing this bandwidth wide enough, the eﬀects due
to measurement lags are not appreciable, and then it is feasible to take the
acceleration ﬁlter output as the quantity to feed back. Some problem may
occur concerning the noise superimposed on the ﬁltered acceleration signal,
though.
Resorting to a ﬁltering technique may be useful when only the direct position measurement is available. In this case, by means of a second-order state
variable ﬁlter, it is possible to reconstruct velocity and acceleration. However,
the greater lags induced by the use of a second-order ﬁlter typically degrade
the performance with respect to the use of a ﬁrst-order ﬁlter, because of limitations imposed on the ﬁlter bandwidth by numerical implementation of the
controller and ﬁlter.
Notice that the above derivation is based on an ideal dynamic model, i.e.,
when the eﬀects of transmission elasticity as well as those of ampliﬁer and
motor electrical time constants are neglected. This implies that satisfaction
of design requirements imposing large values of feedback gains may not be
veriﬁed in practice, since the existence of unmodelled dynamics — such as
electric dynamics, elastic dynamics due to non-perfectly rigid transmissions,
ﬁlter dynamics for the third scheme — might lead to degrading the system and
eventually driving it to instability. In summary, the above solutions constitute
design guidelines whose limits should be emphasized with regard to the speciﬁc
application.
8.3.2 Decentralized Feedforward Compensation
When the joint control servos are required to track reference trajectories with
high values of speed and acceleration, the tracking capabilities of the scheme in
Fig. 8.6 are unavoidably degraded. The adoption of a decentralized feedforward
compensation allows a reduction of the tracking error. Therefore, in view
of the closed-loop input/output transfer functions in (8.24), (8.28), (8.35),

320

8 Motion Control

Fig. 8.12. Block scheme of position feedback control with decentralized feedforward
compensation

the reference inputs to the three control structures analyzed in the previous
section can be respectively modiﬁed into


s2 (1 + sTm )
(8.42)
Θr (s) = kT P +
Θmd (s)
km KP (1 + sTP )


skT V
s2
Θr (s) = kT P +
+
(8.43)
Θmd (s)
KP
km KP KV


skT V
s2 (1 + km KA kT A )
Θr (s) = kT P +
+
(8.44)
Θmd (s);
KP
km KP KV KA
in this way, tracking of the desired joint position Θmd (s) is achieved, if not
for the eﬀect of disturbances. Notice that computing time derivatives of the
desired trajectory is not a problem, once ϑmd (t) is known analytically. The
tracking control schemes, resulting from simple manipulation of (8.42), (8.43),
(8.44) are reported respectively in Figs. 8.12, 8.13, 8.14, where M (s) indicates
the motor transfer function in (5.11), with km and Tm as in (5.12).
All the solutions allow the input trajectory to be tracked within the range
of validity and linearity of the models employed. It is worth noticing that, as
the number of nested feedback loops increases, a less accurate knowledge of
the system model is required to perform feedforward compensation. In fact,
Tm and km are required for the scheme of Fig. 8.12, only km is required for
the scheme of Fig. 8.13, and km again — but with reduced weight — for the
scheme of Fig. 8.14.
It is worth recalling that perfect tracking can be obtained only under the
assumption of exact matching of the controller and feedforward compensation
parameters with the process parameters, as well as of exact modelling and
linearity of the physical system. Deviations from the ideal values cause a
performance degradation that should be analyzed case by case.

8.3 Decentralized Control

321

Fig. 8.13. Block scheme of position and velocity feedback control with decentralized
feedforward compensation

Fig. 8.14. Block scheme of position, velocity and acceleration feedback control with
decentralized feedforward compensation

The presence of saturation blocks in the schemes of Figs. 8.12, 8.13, 8.14
is to be intended as intentional nonlinearities whose function is to limit relevant physical quantities during transients; the greater the number of feedback
loops, the greater the number of quantities that can be limited (velocity, acceleration, and motor voltage). To this end, notice that trajectory tracking is
obviously lost whenever any of the above quantities saturates. This situation
often occurs for industrial manipulators required to execute point-to-point
motions; in this case, there is less concern about the actual trajectories followed, and the actuators are intentionally taken to operate at the current
limits so as to realize the fastest possible motions.
After simple block reduction on the above schemes, it is possible to determine equivalent control structures that utilize position feedback only and
regulators with standard actions. It should be emphasized that the two solutions are equivalent in terms of disturbance rejection and trajectory tracking.

322

8 Motion Control

Fig. 8.15. Equivalent control scheme of PI type

Fig. 8.16. Equivalent control scheme of PID type

However, tuning of regulator parameters is less straightforward, and the elimination of inner feedback loops prevents the possibility of setting saturations
on velocity and/or acceleration. The control structures equivalent to those
of Figs. 8.12, 8.13, 8.14 are illustrated in Figs. 8.15, 8.16, 8.17, respectively;
control actions of PI, PID, PIDD2 type are illustrated which are respectively
equivalent to the cases of: position feedback; position and velocity feedback;
position, velocity and acceleration feedback.
It is worth noticing that the equivalent control structures in Figs. 8.15–8.17
are characterized by the presence of the feedforward action (Tm /km )ϑ̈md +
(1/km )ϑ̇md . If the motor is current-controlled and not voltage-controlled, by
recalling (5.13), the feedforward action is equal to (ki /kt )(Im ϑ̈md + Fm ϑ̇md ).
If ϑ̇m ≈ ϑ̇md , ϑ̈m ≈ ϑ̈md and the disturbance is negligible, the term Im ϑ̈d +

8.3 Decentralized Control

323

Fig. 8.17. Equivalent control scheme of PIDD2 type

Fm ϑ̇d represents the driving torque providing the desired velocity and acceleration, as indicated by (5.3). By setting
iad =

1
(Im ϑ̈md + Fm ϑ̇md ),
kt

the feedforward action can be rewritten in the form ki iad . This shows that, in
the case the drive is current-controlled, it is possible to replace the acceleration
and velocity feedforward actions with a current and thus a torque feedforward
action, which is to be properly computed with reference to the desired motion.
This equivalence is illustrated in Fig. 8.18, where M (s) has been replaced
by the block scheme of an electric drive of Fig. 5.2, where the parameters of
the current loop are chosen so as to realize a torque-controlled generator. The
feedforward action represents a reference for the motor current, which imposes the generation of the nominal torque to execute the desired motion; the
presence of the position reference allows the closure of a feedback loop which,
in view of the adoption of a standard regulator with transfer function CR (s),
confers robustness to the presented control structure. In summary, the performance that can be achieved with velocity and acceleration feedforward actions
and voltage-controlled actuator can be achieved with a current-controlled actuator and a desired torque feedforward action.
The above schemes can incorporate the typical structure of the controllers
actually implemented in the control architectures of industrial robots. In these
systems it is important to choose the largest possible gains so that model
inaccuracy and coupling terms do not appreciably aﬀect positions of the single
joints. As pointed out above, the upper limit on the gains is imposed by

324

8 Motion Control

Fig. 8.18. Control scheme with current-controlled drive and current feedforward
action

all those factors that have not been modelled, such as implementation of
discrete-time controllers in lieu of the continuous-time controllers analyzed
in theory, presence of ﬁnite sampling time, neglected dynamic eﬀects (e.g.,
joint elasticity, structural resonance, ﬁnite transducer bandwidth), and sensor
noise. In fact, the inﬂuence of such factors in the implementation of the above
controllers may cause a severe system performance degradation for much too
large values of feedback gains.

8.4 Computed Torque Feedforward Control
Deﬁne the tracking error e(t) = ϑmd (t) − ϑm (t). With reference to the most
general scheme (Fig. 8.17), the output of the PIDD2 regulator can be written
as
"
t

a2 ë + a1 ė + a0 e + a−1

e(ς)dς

which describes the time evolution of the error. The constant coeﬃcients
a2 , a1 , a0 , a−1 are determined by the particular solution adopted. Summing
the contribution of the feedforward actions and of the disturbance to this
expression yields
Tm
1
Ra
d,
ϑ̈md +
ϑ̇md −
km
km
kt
where
Tm
Im Ra
1
=
km =
.
km
kt
kv
The input to the motor (Fig. 8.6) has then to satisfy the following equation:
" t
Tm
1
Ra
Tm
1
e(ς)dς +
d=
a2 ë + a1 ė + a0 e + a−1
ϑ̈md +
ϑ̇md −
ϑ̈m +
ϑ̇m .
km
km
kt
km
km
With a suitable change of coeﬃcients, this can be rewritten as
" t
Ra
a2 ë + a1 ė + a0 e + a−1
e(ς)dς =
d.
kt

8.4 Computed Torque Feedforward Control

325

Fig. 8.19. Block scheme of computed torque feedforward control

This equation describes the error dynamics and shows that any physically
executable trajectory is asymptotically tracked only if the disturbance term
d(t) = 0. With the term physically executable it is meant that the saturation
limits on the physical quantities, e.g., current and voltage in electric motors,
are not violated in the execution of the desired trajectory.
The presence of the term d(t) causes a tracking error whose magnitude is
reduced as much as the disturbance frequency content is located oﬀ to the left
of the lower limit of the bandwidth of the error system. The disturbance/error
transfer function is given by
Ra
s
E(s)
kt
=  3
,
D(s)
a2 s + a1 s2 + a0 s + a−1
and thus the adoption of loop gains which are not realizable for the above
discussed reasons is often required.
Nevertheless, even if the term d(t) has been introduced as a disturbance,
its expression is given by (8.22). It is then possible to add a further term to
the previous feedforward actions which is able to compensate the disturbance
itself rather than its eﬀects. In other words, by taking advantage of model
knowledge, the rejection eﬀort of an independent joint control scheme can be
lightened with notable simpliﬁcation from the implementation viewpoint.
Let q d (t) be the desired joint trajectory and q md (t) the corresponding
actuator trajectory as in (8.2). By adopting an inverse model strategy, the
feedforward action Ra K −1
t dd can be introduced with
−1
−1
−1
−1
dd = K −1
r ΔB(q d )K r q̈ md + K r C(q d , q̇ d )K r q̇ md + K r g(q d ), (8.45)

where Ra and K t denote the diagonal matrices of armature resistances and
torque constants of the actuators. This action tends to compensate the actual

326

8 Motion Control

disturbance expressed by (8.22) and in turn allows the control system to
operate in a better condition.
This solution is illustrated in the scheme of Fig. 8.19, which conceptually
describes the control system of a manipulator with computed torque control.
The feedback control system is representative of the n independent joint control servos; it is decentralized , since controller i elaborates references and measurements that refer to single Joint i. The interactions between the various
joints, expressed by d, are compensated by a centralized action whose function
is to generate a feedforward action that depends on the joint references as well
as on the manipulator dynamic model. This action compensates the nonlinear
coupling terms due to inertial, Coriolis, centrifugal, and gravitational forces
that depend on the structure and, as such, vary during manipulator motion.
Although the residual disturbance term d = dd − d vanishes only in the
ideal case of perfect tracking (q = q d ) and exact dynamic modelling, d is
representative of interaction disturbances of considerably reduced magnitude
with respect to d. Hence, the computed torque technique has the advantage to
alleviate the disturbance rejection task for the feedback control structure and
in turn allows limited gains. Notice that expression (8.45) in general imposes a
computationally demanding burden on the centralized part of the controller.
Therefore, in those applications where the desired trajectory is generated in
real time with regard to exteroceptive sensory data and commands from higher
hierarchical levels of the robot control architecture,5 on-line computation of
the centralized feedforward action may require too much time.6
Since the actual controller is to be implemented on a computer with a
ﬁnite sampling time, torque computation has to be carried out during this
interval of time; in order not to degrade dynamic system performance, typical
sampling times are of the order of the millisecond.
Therefore, it may be worth performing only a partial feedforward action
so as to compensate those terms of (8.45) that give the most relevant contributions during manipulator motion. Since inertial and gravitational terms
dominate velocity-dependent terms (at operational joint speeds not greater
than a few radians per second), a partial compensation can be achieved by
computing only the gravitational torques and the inertial torques due to the
diagonal elements of the inertia matrix. In this way, only the terms depending
on the global manipulator conﬁguration are compensated while those deriving
from motion interaction with the other joints are not.
Finally, it should be pointed out that, for repetitive trajectories, the above
compensating contributions can be computed oﬀ-line and properly stored on
the basis of a trade-oﬀ solution between memory capacity and computational
requirements of the control architecture.
5
6

See also Chap. 6.
In this regard, the problem of real-time computation of compensating torques can
be solved by resorting to eﬃcient recursive formulations of manipulator inverse
dynamics, such as the Newton–Euler algorithm presented in Chap. 7.

8.5 Centralized Control

327

8.5 Centralized Control
In the previous sections several techniques have been discussed that allow
the design of independent joint controllers. These are based on a singleinput/single-output approach, since interaction and coupling eﬀects between
the joints have been considered as disturbances acting on each single joint
drive system.
On the other hand, when large operational speeds are required or directdrive actuation is employed (K r = I), the nonlinear coupling terms strongly
inﬂuence system performance. Therefore, considering the eﬀects of the components of d as a disturbance may generate large tracking errors. In this case,
it is advisable to design control algorithms that take advantage of a detailed
knowledge of manipulator dynamics so as to compensate for the nonlinear
coupling terms of the model. In other words, it is necessary to eliminate the
causes rather than to reduce the eﬀects induced by them; that is, to generate
compensating torques for the nonlinear terms in (8.22). This leads to centralized control algorithms that are based on the (partial or complete) knowledge
of the manipulator dynamic model.
Whenever the robot is endowed with the torque sensors at the joint motors
presented in Sect. 5.4.1, those measurements can be conveniently utilized to
generate the compensation action, thus avoiding the on-line computation of
the terms of the dynamic model.
As shown by the dynamic model (8.1), the manipulator is not a set of
n decoupled system but it is a multivariable system with n inputs (joint
torques) and n outputs (joint positions) interacting between them by means
of nonlinear relations.7
In order to follow a methodological approach which is consistent with
control design, it is necessary to treat the control problem in the context of
nonlinear multivariable systems. This approach will obviously account for the
manipulator dynamic model and lead to ﬁnding nonlinear centralized control
laws, whose implementation is needed for high manipulator dynamic performance. On the other hand, the above computed torque control can be interpreted in this framework, since it provides a model-based nonlinear control
term to enhance trajectory tracking performance. Notice, however, that this
action is inherently performed oﬀ line, as it is computed on the time history
of the desired trajectory and not of the actual one.
In the following, the problem of the determination of the control law u
ensuring a given performance to the system of manipulator with drives is
tackles. Since (8.17) can be considered as a proportional relationship between
v c and u, the centralized control schemes below refer directly to the generation
of control toques u.

7

See Appendix C for the basic concepts on control of nonlinear mechanical systems.

328

8 Motion Control

8.5.1 PD Control with Gravity Compensation
Let a constant equilibrium posture be assigned for the system as the vector of
desired joint variables q d . It is desired to ﬁnd the structure of the controller
which ensures global asymptotic stability of the above posture.
The determination of the control input which stabilizes the system around
the equilibrium posture is based on the Lyapunov direct method.
Take the vector [ q T q̇ T ]T as the system state, where
q = qd − q

(8.46)

represents the error between the desired and the actual posture. Choose the
following positive deﬁnite quadratic form as Lyapunov function candidate:
V (q̇, q) =

1 T
1
q̇ B(q)q̇ + q T K P q > 0
2
2

∀q̇, q = 0

(8.47)

where K P is an (n × n) symmetric positive deﬁnite matrix. An energy-based
interpretation of (8.47) reveals a ﬁrst term expressing the system kinetic energy and a second term expressing the potential energy stored in the system
of equivalent stiﬀness K P provided by the n position feedback loops.
Diﬀerentiating (8.47) with respect to time, and recalling that q d is constant, yields
1
(8.48)
V̇ = q̇ T B(q)q̈ + q̇ T Ḃ(q)q̇ − q̇ T K P q.
2
Solving (8.7) for Bq̈ and substituting it in (8.48) gives
V̇ =



1 T
q̇ Ḃ(q) − 2C(q, q̇) q̇ − q̇ T F q̇ + q̇ T u − g(q) − K P q .
2

(8.49)

The ﬁrst term on the right-hand side is null since the matrix N = Ḃ − 2C
satisﬁes (7.49). The second term is negative deﬁnite. Then, the choice
u = g(q) + K P q,

(8.50)

describing a controller with compensation of gravitational terms and a proportional action, leads to a negative semi-deﬁnite V̇ since
V̇ = 0

q̇ = 0, ∀q.

This result can be obtained also by taking the control law
u = g(q) + K P q − K D q̇,

(8.51)

with K D positive deﬁnite, corresponding to a nonlinear compensation action
of gravitational terms with a linear proportional-derivative (PD) action. In
fact, substituting (8.51) into (8.49) gives
V̇ = −q̇ T (F + K D )q̇,

(8.52)

8.5 Centralized Control

329

Fig. 8.20. Block scheme of joint space PD control with gravity compensation

which reveals that the introduction of the derivative term causes an increase
of the absolute values of V̇ along the system trajectories, and then it gives an
improvement of system time response. Notice that the inclusion of a derivative
action in the controller, as in (8.51), is crucial when direct-drive manipulators
are considered. In that case, in fact, mechanical viscous damping is practically null, and current control does not allow the exploitation of the electrical
viscous damping provided by voltage-controlled actuators.
According to the above, the function candidate V decreases as long as
q̇ = 0 for all system trajectories. It can be shown that the system reaches an
equilibrium posture. To ﬁnd such posture, notice that V̇ ≡ 0 only if q̇ ≡ 0.
System dynamics under control (8.51) is given by
B(q)q̈ + C(q, q̇)q̇ + F q̇ + g(q) = g(q) + K P q − K D q̇.

(8.53)

At the equilibrium (q̇ ≡ 0, q̈ ≡ 0) it is
KP q = 0

(8.54)

and then
q = qd − q ≡ 0
is the sought equilibrium posture. The above derivation rigorously shows that
any manipulator equilibrium posture is globally asymptotically stable under
a controller with a PD linear action and a nonlinear gravity compensating
action. Stability is ensured for any choice of K P and K D , as long as these are
positive deﬁnite matrices. The resulting block scheme is shown in Fig. 8.20.
The control law requires the on-line computation of the term g(q). If compensation is imperfect, the above discussion does not lead to the same result;
this aspect will be revisited later with reference to robustness of controllers
performing nonlinear compensation.

330

8 Motion Control

Fig. 8.21. Exact linearization performed by inverse dynamics control

8.5.2 Inverse Dynamics Control
Consider now the problem of tracking a joint space trajectory. The reference
framework is that of control of nonlinear multivariable systems. The dynamic
model of an n-joint manipulator is expressed by (8.7) which can be rewritten
as
B(q)q̈ + n(q, q̇) = u,
(8.55)
where for simplicity it has been set
n(q, q̇) = C(q, q̇)q̇ + F q̇ + g(q).

(8.56)

The approach that follows is founded on the idea to ﬁnd a control vector u, as
a function of the system state, which is capable of realizing an input/output
relationship of linear type; in other words, it is desired to perform not an
approximate linearization but an exact linearization of system dynamics obtained by means of a nonlinear state feedback . The possibility of ﬁnding such
a linearizing controller is guaranteed by the particular form of system dynamics. In fact, the equation in (8.55) is linear in the control u and has a full-rank
matrix B(q) which can be inverted for any manipulator conﬁguration.
Taking the control u as a function of the manipulator state in the form
u = B(q)y + n(q, q̇),

(8.57)

leads to the system described by
q̈ = y
where y represents a new input vector whose expression is to be determined
yet; the resulting block scheme is shown in Fig. 8.21. The nonlinear control
law in (8.57) is termed inverse dynamics control since it is based on the computation of manipulator inverse dynamics. The system under control (8.57)
is linear and decoupled with respect to the new input y. In other words, the
component yi inﬂuences, with a double integrator relationship, only the joint
variable qi , independently of the motion of the other joints.

8.5 Centralized Control

331

Fig. 8.22. Block scheme of joint space inverse dynamics control

In view of the choice (8.57), the manipulator control problem is reduced
to that of ﬁnding a stabilizing control law y. To this end, the choice
y = −K P q − K D q̇ + r

(8.58)

leads to the system of second-order equations
q̈ + K D q̇ + K P q = r

(8.59)

which, under the assumption of positive deﬁnite matrices K P and K D , is
asymptotically stable. Choosing K P and K D as diagonal matrices of the
type
2
2
K P = diag{ωn1
, . . . , ωnn
}

K D = diag{2ζ1 ωn1 , . . . , 2ζn ωnn },

gives a decoupled system. The reference component ri inﬂuences only the joint
variable qi with a second-order input/output relationship characterized by a
natural frequency ωni and a damping ratio ζi .
Given any desired trajectory q d (t), tracking of this trajectory for the output q(t) is ensured by choosing
r = q̈ d + K D q̇ d + K P q d .

(8.60)

In fact, substituting (8.60) into (8.59) gives the homogeneous second-order
diﬀerential equation
¨ + K D q˙ + K P q = 0
q
(8.61)
expressing the dynamics of position error (8.46) while tracking the given tra˙
jectory. Such error occurs only if q(0) and/or q(0)
are diﬀerent from zero
and converges to zero with a speed depending on the matrices K P and K D
chosen.

332

8 Motion Control

The resulting block scheme is illustrated in Fig. 8.22, in which two feedback loops are represented; an inner loop based on the manipulator dynamic
model, and an outer loop operating on the tracking error. The function of
the inner loop is to obtain a linear and decoupled input/output relationship,
whereas the outer loop is required to stabilize the overall system. The controller design for the outer loop is simpliﬁed since it operates on a linear and
time-invariant system. Notice that the implementation of this control scheme
requires computation of the inertia matrix B(q) and of the vector of Coriolis,
centrifugal, gravitational, and damping terms n(q, q̇) in (8.56). Unlike computed torque control, these terms must be computed on-line since control is
now based on nonlinear feedback of the current system state, and thus it is
not possible to precompute the terms oﬀ line as for the previous technique.
The above technique of nonlinear compensation and decoupling is very attractive from a control viewpoint since the nonlinear and coupled manipulator
dynamics is replaced with n linear and decoupled second-order subsystems.
Nonetheless, this technique is based on the assumption of perfect cancellation
of dynamic terms, and then it is quite natural to raise questions about sensitivity and robustness problems due to unavoidably imperfect compensation.
Implementation of inverse dynamics control laws indeed requires that parameters of the system dynamic model are accurately known and the complete
equations of motion are computed in real time. These conditions are diﬃcult
to verify in practice. On one hand, the model is usually known with a certain
degree of uncertainty due to imperfect knowledge of manipulator mechanical parameters, existence of unmodelled dynamics, and model dependence on
end-eﬀector payloads not exactly known and thus not perfectly compensated.
On the other hand, inverse dynamics computation is to be performed at sampling times of the order of a millisecond so as to ensure that the assumption
of operating in the continuous time domain is realistic. This may pose severe
constraints on the hardware/software architecture of the control system. In
such cases, it may be advisable to lighten the computation of inverse dynamics
and compute only the dominant terms.
On the basis of the above remarks, from an implementation viewpoint,
compensation may be imperfect both for model uncertainty and for the approximations made in on-line computation of inverse dynamics. In the following, two control techniques are presented which are aimed at counteracting
the eﬀects of imperfect compensation. The ﬁrst consists of the introduction of
an additional term to an inverse dynamics controller which provides robustness to the control system by counteracting the eﬀects of the approximations
made in on-line computation of inverse dynamics. The second adapts the parameters of the model used for inverse dynamics computation to those of the
true manipulator dynamic model.

8.5 Centralized Control

333

8.5.3 Robust Control
In the case of imperfect compensation, it is reasonable to assume in (8.55) a
control vector expressed by
)
) (q, q̇)
u = B(q)y
+n

(8.62)

) and n
) represent the adopted computational model in terms of eswhere B
timates of the terms in the dynamic model. The error on the estimates, i.e.,
the uncertainty, is represented by
) −B
B=B

) −n
n=n

(8.63)

and is due to imperfect model compensation as well as to intentional simpliﬁ) = B̄ (where
cation in inverse dynamics computation. Notice that by setting B
) = 0, the
B̄ is the diagonal matrix of average inertia at the joint axes) and n
above decentralized control scheme is recovered where the control action y
can be of the general PID type computed on the error.
Using (8.62) as a nonlinear control law gives
) +n
)
Bq̈ + n = By

(8.64)

where functional dependence has been omitted. Since the inertia matrix B is
invertible, it is

where

) − I)y + B −1 n = y − η
q̈ = y + (B −1 B

(8.65)

) − B −1 n.
η = (I − B −1 B)y

(8.66)

Taking as above
y = q̈ d + K D (q̇ d − q̇) + K P (q d − q),
leads to

¨ + K D q˙ + K P q = η.
q

(8.67)

The system described by (8.67) is still nonlinear and coupled, since η is a
˙ error convergence to zero is not ensured by the
nonlinear function of q and q;
term on the left-hand side only.
To ﬁnd control laws ensuring error convergence to zero while tracking a
trajectory even in the face of uncertainties, a linear PD control is no longer
suﬃcient. To this end, the Lyapunov direct method can be utilized again for
the design of an outer feedback loop on the error which should be robust to
the uncertainty η.
Let the desired trajectory q d (t) be assigned in the joint space and let
q = q d − q be the position error. Its ﬁrst time-derivative is q˙ = q̇ d − q̇, while
its second time-derivative in view of (8.65) is
¨ = q̈ − y + η.
q
d

(8.68)

334

8 Motion Control

By taking

 
q
ξ= ˙ ,
q

(8.69)

as the system state, the following ﬁrst-order diﬀerential matrix equation is
obtained:
ξ̇ = Hξ + D(q̈ d − y + η),
(8.70)
where H and D are block matrices of dimensions (2n × 2n) and (2n × n),
respectively:


 
O I
O
H=
D=
.
(8.71)
O O
I
Then, the problem of tracking a given trajectory can be regarded as the problem of ﬁnding a control law y which stabilizes the nonlinear time-varying error
system (8.70).
Control design is based on the assumption that, even though the uncertainty η is unknown, an estimate on its range of variation is available. The
sought control law y should guarantee asymptotic stability of (8.70) for any
η varying in the above range. By recalling that η in (8.66) is a function of q,
q̇, q̈ d , the following assumptions are made:
supt≥0 q̈ d  < QM < ∞

∀q̈ d

(8.72)

)
≤ α ≤ 1 ∀q
I − B −1 (q)B(q)
n ≤ Φ < ∞

(8.73)

∀q, q̇.

(8.74)

Assumption (8.72) is practically satisﬁed since any planned trajectory cannot
require inﬁnite accelerations.
Regarding assumption (8.73), since B is a positive deﬁnite matrix with
upper and lower limited norms, the following inequality holds:
0 < Bm ≤ B −1 (q) ≤ BM < ∞

∀q,

(8.75)

) always exists which satisﬁes (8.73). In fact, by setting
and then a choice for B
) =
B

2
I,
BM + Bm

from (8.73) it is
) − I ≤
B −1 B

BM − Bm
= α < 1.
BM + Bm

(8.76)

) is a more accurate estimate of the inertia matrix, the inequality is satisﬁed
If B
) =B
with values of α that can be made arbitrarily small (in the limit, it is B
and α = 0).

8.5 Centralized Control

335

Finally, concerning assumption (8.74), observe that n is a function of q and
q̇. For revolute joints a periodical dependence on q is obtained, while for prismatic joints a linear dependence is obtained, but the joint ranges are limited
and then the above contribution is also limited. On the other hand, regarding
the dependence on q̇, unbounded velocities for an unstable system may arise
in the limit, but in reality saturations exist on the maximum velocities of the
motors. In summary, assumption (8.74) can be realistically satisﬁed, too.
With reference to (8.65), choose now
y = q̈ d + K D q˙ + K P q + w

(8.77)

where the PD term ensures stabilization of the error dynamic system matrix,
q̈ d provides a feedforward term, and the term w is to be chosen to guarantee
robustness to the eﬀects of uncertainty described by η in (8.66).
Using (8.77) and setting K = [ K P K D ] yields
- + D(η − w),
ξ̇ = Hξ
where
- = (H − DK) =
H



O
−K P

(8.78)
I
−K D



is a matrix whose eigenvalues all have negative real parts — K P and K D
being positive deﬁnite — which allows the desired error system dynamics to
2
2
, . . . , ωnn
} and K D =
be prescribed. In fact, by choosing K P = diag{ωn1
diag{2ζ1 ωn1 , . . . , 2ζn ωnn }, n decoupled equations are obtained as regards the
linear part. If the uncertainty term vanishes, it is obviously w = 0 and the
) =B
above result with an exact inverse dynamics controller is recovered (B
) = n).
and n
To determine w, consider the following positive deﬁnite quadratic form as
Lyapunov function candidate:
V (ξ) = ξ T Qξ > 0

∀ξ = 0,

(8.79)

where Q is a (2n × 2n) positive deﬁnite matrix. The derivative of V along the
trajectories of the error system (8.78) is
T

V̇ = ξ̇ Qξ + ξ T Qξ̇

(8.80)

T

- Q + QH)ξ
- + 2ξ T QD(η − w).
= ξ T (H
- has eigenvalues with all negative real parts, it is well-known that for
Since H
any symmetric positive deﬁnite matrix P , the equation
T

- = −P
- Q + QH
H

(8.81)

gives a unique solution Q which is symmetric positive deﬁnite as well. In view
of this, (8.80) becomes
V̇ = −ξ T P ξ + 2ξ T QD(η − w).

(8.82)

336

8 Motion Control

Fig. 8.23. Block scheme of joint space robust control

The ﬁrst term on the right-hand side of (8.82) is negative deﬁnite and then
the solutions converge if ξ ∈ N (D T Q). If instead ξ ∈ N (D T Q), the control
w must be chosen so as to render the second term in (8.82) less than or equal
to zero. By setting z = D T Qξ, the second term in (8.82) can be rewritten as
z T (η − w). Adopting the control law
ρ
z
ρ>0
(8.83)
w=
z
gives8
ρ T
z z
z
≤ zη − ρz
= z(η − ρ).

z T (η − w) = z T η −

(8.84)

Then, if ρ is chosen so that
ρ ≥ η

∀q, q̇, q̈ d ,

(8.85)

the control (8.83) ensures that V̇ is less than zero along all error system
trajectories.
In order to satisfy (8.85), notice that, in view of the deﬁnition of η in (8.66)
and of assumptions (8.72)–(8.74), and being w = ρ, it is

) q̈ d  + K ξ + w + B −1  n
η ≤ I − B −1 B
8

Notice that it is necessary to divide z by the norm of z so as to obtain a linear
dependence on z of the term containing the control z T w, and thus to eﬀectively
counteract, for z → 0, the term containing the uncertainty z T η which is linear
in z.

8.5 Centralized Control

≤ αQM + αK ξ + αρ + BM Φ.

337

(8.86)

Therefore, setting
ρ≥

1
(αQM + αKξ + BM Φ)
1−α

gives


V̇ = −ξ P ξ + 2z
T

T

ρ
z
η−
z

(8.87)


<0

∀ξ = 0.

(8.88)

The resulting block scheme is illustrated in Fig. 8.23.
To summarize, the presented approach has lead to ﬁnding a control law
which is formed by three diﬀerent contributions:
)
) ensures an approximate compensation of nonlinear eﬀects
• The term By+
n
and joint decoupling.
• The term q̈ d + K D q˙ + K P q introduces a linear feedforward action (q̈ d +
K D q̇ d +K P q d ) and linear feedback action (−K D q̇−K P q) which stabilizes
the error system dynamics.
• The term w = (ρ/z)z represents the robust contribution that counteracts the indeterminacy B and n in computing the nonlinear terms that
depend on the manipulator state; the greater the uncertainty, the greater
the positive scalar ρ. The resulting control law is of the unit vector type,
since it is described by a vector of magnitude ρ aligned with the unit vector
of z = D T Qξ, ∀ξ.
All the resulting trajectories under the above robust control reach the subspace z = D T Qξ = 0 that depends on the matrix Q in the Lyapunov function
V . On this attractive subspace, termed sliding subspace, the control w is ideally commuted at an inﬁnite frequency and all error components tend to zero
with a transient depending on the matrices Q, K P , K D . A characterization
of an error trajectory in the two-dimensional case is given in Fig. 8.24. Notice
that in the case ξ(0) = 0, with ξ(0) ∈ N (D T Q), the trajectory is attracted
on the sliding hyperplane (a line) z = 0 and tends towards the origin of the
error state space with a time evolution governed by ρ.
In reality, the physical limits on the elements employed in the controller
impose a control signal that commutes at a ﬁnite frequency, and the trajectories oscillate around the sliding subspace with a magnitude as low as the
frequency is high.
Elimination of these high-frequency components (chattering) can be achieved by adopting a robust control law which, even if it does not guarantee error
convergence to zero, ensures bounded-norm errors. A control law of this type
is
⎧ ρ
⎪
z
per z ≥
⎨
z
(8.89)
w=
⎪
⎩ ρz
per z < .

338

8 Motion Control

Fig. 8.24. Error trajectory with robust control

In order to provide an intuitive interpretation of this law, notice that (8.89)
gives a null control input when the error is in the null space of matrix D T Q.
On the other hand, (8.83) has an equivalent gain tending to inﬁnity when z
tends to the null vector, thus generating a control input of limited magnitude.
Since these inputs commute at an inﬁnite frequency, they force the error
system dynamics to stay on the sliding subspace. With reference to the above
example, control law (8.89) gives rise to a hyperplane z = 0 which is no
longer attractive, and the error is allowed to vary within a boundary layer
whose thickness depends on (Fig. 8.25).
The introduction of a contribution based on the computation of a suitable
linear combination of the generalized error confers robustness to a control
scheme based on nonlinear compensation. Even if the manipulator is accurately modeled, indeed, an exact nonlinear compensation may be computationally demanding, and thus it may require either a sophisticated hardware
architecture or an increase of the sampling time needed to compute the control law. The solution then becomes weak from an engineering viewpoint, due
either to infeasible costs of the control architecture, or to poor performance
at decreased sampling rates. Therefore, considering a partial knowledge of the
manipulator dynamic model with an accurate, pondered estimate of uncertainty may suggest robust control solutions of the kind presented above. It
is understood that an estimate of the uncertainty should be found so as to
impose control inputs which the mechanical structure can bear.
8.5.4 Adaptive Control
The computational model employed for computing inverse dynamics typically
has the same structure as that of the true manipulator dynamic model, but
parameter estimate uncertainty does exist. In this case, it is possible to devise
solutions that allow an on-line adaptation of the computational model to the

8.5 Centralized Control

339

Fig. 8.25. Error trajectory with robust control and chattering elimination

dynamic model , thus performing a control scheme of the inverse dynamics
type.
The possibility of ﬁnding adaptive control laws is ensured by the property
of linearity in the parameters of the dynamic model of a manipulator. In
fact, it is always possible to express the nonlinear equations of motion in a
linear form with respect to a suitable set of constant dynamic parameters as
in (7.81). The equation in (8.7) can then be written as
B(q)q̈ + C(q, q̇)q̇ + F q̇ + g(q) = Y (q, q̇, q̈)π = u,

(8.90)

where π is a (p × 1) vector of constant parameters and Y is an (n × p)
matrix which is a function of joint positions, velocities and accelerations. This
property of linearity in the dynamic parameters is fundamental for deriving
adaptive control laws, among which the technique illustrated below is one of
the simplest.
At ﬁrst, a control scheme which can be derived through a combined computed torque/inverse dynamics approach is illustrated. The computational
model is assumed to coincide with the dynamic model.
Consider the control law
u = B(q)q̈ r + C(q, q̇)q̇ r + F q̇ r + g(q) + K D σ,

(8.91)

with K D a positive deﬁnite matrix. The choice
q̇ r = q̇ d + Λq

˙
q̈ r = q̈ d + Λq,

(8.92)

with Λ a positive deﬁnite (usually diagonal) matrix, allows the nonlinear compensation and decoupling terms to be expressed as a function of the desired
velocity and acceleration, corrected by the current state (q and q̇) of the manipulator. In fact, notice that the term q̇ r = q̇ d + Λq weighs the contribution

340

8 Motion Control

that depends on velocity, not only on the basis of the desired velocity but also
on the basis of the position tracking error. A similar argument also holds for
the acceleration contribution, where a term depending on the velocity tracking
error is considered besides the desired acceleration.
The term K D σ is equivalent to a PD action on the error if σ is taken as
σ = q̇ r − q̇ = q˙ + Λq.

(8.93)

Substituting (8.91) into (8.90) and accounting for (8.93) yields
B(q)σ̇ + C(q, q̇)σ + F σ + K D σ = 0.

(8.94)

Consider the Lyapunov function candidate
V (σ, q) =

1 T
1
σ B(q)σ + q T M q > 0
2
2

∀σ, q = 0,

(8.95)

where M is an (n × n) symmetric positive deﬁnite matrix; the introduction
of the second term in (8.95) is necessary to obtain a Lyapunov function of the
entire system state which vanishes for q = 0 and q˙ = 0. The time derivative
of V along the trajectories of system (8.94) is
1
V̇ = σ T B(q)σ̇ + σ T Ḃ(q)σ + q T M q˙
2
˙
= −σ T (F + K D )σ + q T M q,

(8.96)

where the skew-symmetry propertyv of the matrix N = Ḃ − 2C has been
exploited. In view of the expression of σ in (8.93), with diagonal Λ and K D ,
it is convenient to choose M = 2ΛK D ; this leads to
T
V̇ = −σ T F σ − q˙ K D q˙ − q T ΛK D Λq.

(8.97)

This expression shows that the time derivative is negative deﬁnite since it
vanishes only if q ≡ 0 and q˙ ≡ 0; thus, it follows that the origin of the state
space [ q T σ T ]T = 0 is globally asymptotically stable. It is worth noticing
that, unlike the robust control case, the error trajectory tends to the subspace
σ = 0 without the need of a high-frequency control.
On the basis of this notable result, the control law can be made adaptive
with respect to the vector of parameters π.
Suppose that the computational model has the same structure as that of
the manipulator dynamic model, but its parameters are not known exactly.
The control law (8.91) is then modiﬁed into
)
)
)
) + KDσ
u = B(q)q̈
r + C(q, q̇)q̇ r + F q̇ r + g
= Y (q, q̇, q̇ r , q̈ r ))
π + K D σ,

(8.98)

8.5 Centralized Control

341

) represents the available estimate on the parameters and, accordingly,
where π
) C,
) F
), g
) denote the estimated terms in the dynamic model. Substituting
B,
control (8.98) into (8.90) gives
B(q)σ̇ + C(q, q̇)σ + F σ + K D σ = −B(q)q̈ r − C(q, q̇)q̇ r − F q̇ r − g(q)
= −Y (q, q̇, q̇ r , q̈ r )π,

(8.99)

where the property of linearity in the error parameter vector
) −π
π=π

(8.100)

has been conveniently exploited. In view of (8.63), the modelling error is
characterized by
) −B
B=B

) −C
C=C

) −F
F =F

) − g.
g=g

(8.101)

It is worth remarking that, in view of position (8.92), the matrix Y does not
depend on the actual joint accelerations but only on their desired values; this
avoids problems due to direct measurement of acceleration.
At this point, modify the Lyapunov function candidate in (8.95) into the
form
V (σ, q, π) =

1
1 T
σ B(q)σ + q T ΛK D q + π T K π π > 0
2
2

∀σ, q, π = 0,

(8.102)
which features an additional term accounting for the parameter error (8.100),
with K π symmetric positive deﬁnite. The time derivative of V along the
trajectories of system (8.99) is

T
V̇ = −σ T F σ − q˙ K D q˙ − q T ΛK D Λq + π T K π π˙ − Y T (q, q̇, q̇ r , q̈ r )σ .
(8.103)
If the estimate of the parameter vector is updated as in the adaptive law
T
)˙ = K −1
π
π Y (q, q̇, q̇ r , q̈ r )σ,

(8.104)

the expression in (8.103) becomes
T
V̇ = −σ T F σ − q˙ K D q˙ − q T ΛK D Λq

)˙ = π˙ — π is constant.
since π
By an argument similar to above, it is not diﬃcult to show that the trajectories of the manipulator described by the model
B(q)q̈ + C(q, q̇)q̇ + F q̇ + g(q) = u,
under the control law
π + K D (q˙ + Λq)
u = Y (q, q̇, q̇ r , q̈ r ))

342

8 Motion Control

Fig. 8.26. Block scheme of joint space adaptive control

and the parameter adaptive law
T
˙
)˙ = K −1
π
π Y (q, q̇, q̇ r , q̈ r )(q + Λq),

globally asymptotically converge to σ = 0 and q = 0, which implies conver˙ and boundedness of π
) . The equation in (8.99) shows
gence to zero of q, q,
that asymptotically it is
Y (q, q̇, q̇ r , q̈ r )()
π − π) = 0.

(8.105)

) tends to π; indeed, convergence of paramThis equation does not imply that π
eters to their true values depends on the structure of the matrix Y (q, q̇, q̇ r , q̈ r )
and then on the desired and actual trajectories. Nonetheless, the followed approach is aimed at solving a direct adaptive control problem, i.e., ﬁnding a
control law that ensures limited tracking errors, and not at determining the
actual parameters of the system (as in an indirect adaptive control problem).
The resulting block scheme is illustrated in Fig. 8.26. To summarize, the above
control law is formed by three diﬀerent contributions:
) describes a control action of inverse dynamics type which
• The term Y π
ensures an approximate compensation of nonlinear eﬀects and joint decoupling.
• The term K D σ introduces a stabilizing linear control action of PD type
on the tracking error .
) is updated by an adaptive law of
• The vector of parameter estimates π
gradient type so as to ensure asymptotic compensation of the terms in the
manipulator dynamic model; the matrix K π determines the convergence
rate of parameters to their asymptotic values.
Notice that, with σ ≈ 0, the control law (8.98) is equivalent to a pure
inverse dynamics compensation of the computed torque type on the basis of

8.6 Operational Space Control

343

desired velocities and accelerations; this is made possible by the fact that
) ≈ Y π.
Yπ
The control law with parameter adaptation requires the availability of a
complete computational model and it does not feature any action aimed at
reducing the eﬀects of external disturbances. Therefore, a performance degradation is expected whenever unmodelled dynamic eﬀects, e.g., when a reduced
computational model is used, or external disturbances occur. In both cases,
the eﬀects induced on the output variables are attributed by the controller to
parameter estimate mismatching; as a consequence, the control law attempts
to counteract those eﬀects by acting on quantities that did not provoke them
originally.
On the other hand, robust control techniques provide a natural rejection
to external disturbances, although they are sensitive to unmodelled dynamics;
this rejection is provided by a high-frequency commuted control action that
constrains the error trajectories to stay on the sliding subspace. The resulting
inputs to the mechanical structure may be unacceptable. This inconvenience
is in general not observed with the adoption of adaptive control techniques
whose action has a naturally smooth time behaviour.

8.6 Operational Space Control
In all the above control schemes, it was always assumed that the desired trajectory is available in terms of the time sequence of the values of joint position,
velocity and acceleration. Accordingly, the error for the control schemes was
expressed in the joint space.
As often pointed out, motion speciﬁcations are usually assigned in the operational space, and then an inverse kinematics algorithm has to be utilized to
transform operational space references into the corresponding joint space references. The process of kinematic inversion has an increasing computational
load when, besides inversion of direct kinematics, inversion of ﬁrst-order and
second-order diﬀerential kinematics is also required to transform the desired
time history of end-eﬀector position, velocity and acceleration into the corresponding quantities at the joint level. It is for this reason that current industrial robot control systems compute the joint positions through kinematics
inversion, and then perform a numerical diﬀerentiation to compute velocities
and accelerations.
A diﬀerent approach consists of considering control schemes developed
directly in the operational space. If the motion is speciﬁed in terms of operational space variables, the measured joint space variables can be transformed
into the corresponding operational space variables through direct kinematics
relations. Comparing the desired input with the reconstructed variables allows
the design of feedback control loops where trajectory inversion is replaced with
a suitable coordinate transformation embedded in the feedback loop.

344

8 Motion Control

Fig. 8.27. Block scheme of Jacobian inverse control

All operational space control schemes present considerable computational
requirements, in view of the necessity to perform a number of computations
in the feedback loop which are somewhat representative of inverse kinematics
functions. With reference to a numerical implementation, the presence of a
computationally demanding load requires sampling times that may lead to
degrading the performance of the overall control system.
In the face of the above limitations, it is worth presenting operational
space control schemes, whose utilization becomes necessary when the problem of controlling interaction between the manipulator and the environment
is of concern. In fact, joint space control schemes suﬃce only for motion control in the free space. When the manipulator’s end-eﬀector is constrained by
the environment, e.g., in the case of end-eﬀector in contact with an elastic
environment, it is necessary to control both positions and contact forces and
it is convenient to refer to operational space control schemes. Hence, below
some solutions are presented; these are worked out for motion control, but
they constitute the premise for the force/position control strategies that will
be illustrated in the next chapter.
8.6.1 General Schemes
As pointed out above, operational space control schemes are based on a direct
comparison of the inputs, specifying operational space trajectories, with the
measurements of the corresponding manipulator outputs. It follows that the
control system should incorporate some actions that allow the transformation
from the operational space, in which the error is speciﬁed, to the joint space,
in which control generalized forces are developed.
A possible control scheme that can be devised is the so-called Jacobian
inverse control (Fig. 8.27). In this scheme, the end-eﬀector pose in the operational space xe is compared with the corresponding desired quantity xd ,
and then an operational space deviation Δx can be computed. Assumed that
this deviation is suﬃciently small for a good control system, Δx can be transformed into a corresponding joint space deviation Δq through the inverse of
the manipulator Jacobian. Then, the control input generalized forces can be
computed on the basis of this deviation through a suitable feedback matrix
gain. The result is a presumable reduction of Δq and in turn of Δx. In other
words, the Jacobian inverse control leads to an overall system that intuitively

8.6 Operational Space Control

345

Fig. 8.28. Block scheme of Jacobian transpose control

behaves like a mechanical system with a generalized n-dimensional spring in
the joint space, whose constant stiﬀness is determined by the feedback matrix
gain. The role of such system is to take the deviation Δq to zero. If the matrix
gain is diagonal, the generalized spring corresponds to n independent elastic
elements, one for each joint.
A conceptually analogous scheme is the so-called Jacobian transpose control (Fig. 8.28). In this case, the operational space error is treated ﬁrst through
a matrix gain. The output of this block can then be considered as the elastic force generated by a generalized spring whose function in the operational
space is that to reduce or to cancel the position deviation Δx. In other words,
the resulting force drives the end-eﬀector along a direction so as to reduce Δx.
This operational space force has then to be transformed into the joint space
generalized forces, through the transpose of the Jacobian, so as to realize the
described behaviour.
Both Jacobian inverse and transpose control schemes have been derived
in an intuitive fashion. Hence, there is no guarantee that such schemes are
eﬀective in terms of stability and trajectory tracking accuracy. These problems
can be faced by presenting two mathematical solutions below, which will be
shown to be substantially equivalent to the above schemes.
8.6.2 PD Control with Gravity Compensation
By analogy with joint space stability analysis, given a constant end-eﬀector
pose xd , it is desired to ﬁnd the control structure so that the operational space
error
(8.106)
x = xd − xe
tends asymptotically to zero. Choose the following positive deﬁnite quadratic
form as a Lyapunov function candidate:
V (q̇, x) =

1
1 T
q̇ B(q)q̇ + xT K P x > 0
2
2

∀q̇, x = 0,

(8.107)

with K P a symmetric positive deﬁnite matrix. Diﬀerentiating (8.107) with
respect to time gives
T
1
V̇ = q̇ T B(q)q̈ + q̇ T Ḃ(q)q̇ + x˙ K P x.
2

346

8 Motion Control

Fig. 8.29. Block scheme of operational space PD control with gravity compensation

Since ẋd = 0, in view of (3.62) it is
x˙ = −J A (q)q̇
and then

1
(8.108)
V̇ = q̇ T B(q)q̈ + q̇ T Ḃ(q)q̇ − q̇ T J TA (q)K P x.
2
By recalling the expression of the joint space manipulator dynamic model
in (8.7) and the property in (7.49), the expression in (8.108) becomes

(8.109)
V̇ = −q̇ T F q̇ + q̇ T u − g(q) − J TA (q)K P x .
This equation suggests the structure of the controller; in fact, by choosing
the control law
u = g(q) + J TA (q)K P x − J TA (q)K D J A (q)q̇

(8.110)

with K D positive deﬁnite, (8.109) becomes
V̇ = −q̇ T F q̇ − q̇ T J TA (q)K D J A (q)q̇.

(8.111)

As can be seen from Fig. 8.29, the resulting block scheme reveals an analogy with the scheme of Fig. 8.28. Control law (8.110) performs a nonlinear compensating action of joint space gravitational forces and an operational
space linear PD control action. The last term has been introduced to enhance
system damping; in particular, if measurement of ẋ is deduced from that of
q̇, one can simply choose the derivative term as −K D q̇.
The expression in (8.111) shows that, for any system trajectory, the Lyapunov function decreases as long as q̇ = 0. The system then reaches an equilibrium posture. By a stability argument similar to that in the joint space
(see (8.52)–(8.54)) this posture is determined by
J TA (q)K P x = 0.

(8.112)

8.6 Operational Space Control

347

Fig. 8.30. Block scheme of operational space inverse dynamics control

From (8.112) it can be recognized that, under the assumption of full-rank
Jacobian, it is
x = xd − xe = 0,
i.e., the sought result.
If measurements of xe and ẋe are made directly in the operational space,
k(q) and J A (q) in the scheme of Fig. 8.45 are just indicative of direct kinematics functions; it is, however, necessary to measure q to update both J TA (q)
and g(q) on-line. If measurements of operational space quantities are indirect,
the controller has to compute the direct kinematics functions, too.
8.6.3 Inverse Dynamics Control
Consider now the problem of tracking an operational space trajectory. Recall
the manipulator dynamic model in the form (8.55)
B(q)q̈ + n(q, q̇) = u,
where n is given by (8.56). As in (8.57), the choice of the inverse dynamics
linearizing control
u = B(q)y + n(q, q̇)
leads to the system of double integrators
q̈ = y.

(8.113)

The new control input y is to be designed so as to yield tracking of a trajectory
speciﬁed by xd (t). To this end, the second-order diﬀerential equation in the
form (3.98)
ẍe = J A (q)q̈ + J̇ A (q, q̇)q̇

348

8 Motion Control

suggests, for a nonredundant manipulator, the choice of the control law —
formally analogous to (3.102) —
˙
y = J −1
A (q) ẍd + K D x + K P x − J̇ A (q, q̇)q̇



(8.114)

with K P and K D positive deﬁnite (diagonal) matrices. In fact, substituting (8.114) into (8.113) gives
¨ + K D x˙ + K P x = 0
x

(8.115)

which describes the operational space error dynamics, with K P and K D
determining the error convergence rate to zero. The resulting inverse dynamics
control scheme is reported in Fig. 8.30, which conﬁrms the anticipated analogy
with the scheme of Fig. 8.27. Again in this case, besides xe and ẋe , q and q̇ are
also to be measured. If measurements of xe and ẋe are indirect, the controller
must compute the direct kinematics functions k(q) and J A (q) on-line.
A critical analysis of the schemes in Figs. 8.29, 8.30 reveals that the design
of an operational space controller always requires computation of manipulator
Jacobian. As a consequence, controlling a manipulator in the operational space
is in general more complex than controlling it in the joint space. In fact, the
presence of singularities and/or redundancy inﬂuences the Jacobian, and the
induced eﬀects are somewhat diﬃcult to handle with an operational space
controller. For instance, if a singularity occurs for the scheme of Fig. 8.29 and
the error enters the null space of the Jacobian, the manipulator gets stuck
at a diﬀerent conﬁguration from the desired one. This problem is even more
critical for the scheme of Fig. 8.30 which would require the computation of a
DLS inverse of the Jacobian. Yet, for a redundant manipulator, a joint space
control scheme is naturally transparent to this situation, since redundancy
has already been solved by inverse kinematics, whereas an operational space
control scheme should incorporate a redundancy handling technique inside
the feedback loop.
As a ﬁnal remark, the above operational space control schemes have been
derived with reference to a minimal description of orientation in terms of
Euler angles. It is understood that, similar to what is presented in Sect. 3.7.3
for inverse kinematics algorithms, it is possible to adopt diﬀerent deﬁnitions
of orientation error, e.g., based on the angle and axis or the unit quaternion.
The advantage is the use of the geometric Jacobian in lieu of the analytical
Jacobian. The price to pay, however, is a more complex analysis of the stability
and convergence characteristics of the closed-loop system. Even the inverse
dynamics control scheme will not lead to a homogeneous error equation, and
a Lyapunov argument should be invoked to ascertain its stability.

8.7 Comparison Among Various Control Schemes

349

8.7 Comparison Among Various Control Schemes
In order to make a comparison between the various control schemes presented,
consider the two-link planar arm with the same data of Example 7.2:
a1 = a2 = 1 m

1 = 2 = 0.5 m

kr1 = kr2 = 100

m

1

=m

mm1 = mm2 = 5 kg

2

= 50 kg

I

1

=I

2

= 10 kg·m2

Im1 = Im2 = 0.01 kg·m2 .

The arm is assumed to be driven by two equal actuators with the following
data:
Fm1 = Fm2 = 0.01 N·m·s/rad
Ra1 = Ra2 = 10 ohm
kt1 = kt2 = 2 N·m/A

kv1 = kv2 = 2 V·s/rad;

it can be veriﬁed that Fmi  kvi kti /Rai for i = 1, 2.
The desired tip trajectories have a typical trapezoidal velocity proﬁle, and
thus it is anticipated that sharp torque variations will be induced. The tip path
is a motion of 1.6 m along the horizontal axis, as in the path of Example 7.2. In
the ﬁrst case (fast trajectory), the acceleration time is 0.6 s and the maximum
velocity is 1 m/s. In the second case (slow trajectory), the acceleration time is
0.6 s and the maximum velocity is 0.25 m/s. The motion of the controlled arm
was simulated on a computer, by adopting a discrete-time implementation of
the controller with a sampling time of 1 ms.
The following control schemes in the joint space and in the operational
space have been utilized; an (analytic) inverse kinematics solution has been implemented to generate the reference inputs to the joint space control schemes:
A. Independent joint control with position and velocity feedback (Fig. 5.11)
with the following data for each joint servo:
KP = 5

KV = 10

kT P = kT V = 1,

corresponding to ωn = 5 rad/s and ζ = 0.5.
B. Independent joint control with position, velocity and acceleration feedback
(Fig. 8.9) with the following data for each joint servo:
KP = 5

KV = 10

KA = 2

kT P = kT V = kT A = 1,

corresponding to ωn = 5 rad/s, ζ = 0.5, XR = 100. To reconstruct acceleration, a ﬁrst-order ﬁlter has been utilized (Fig. 8.11) characterized by
ω3f = 100 rad/s.
C. As in scheme A with the addition of a decentralized feedforward action
(Fig. 8.13).
D. As in scheme B with the addition of a decentralized feedforward action
(Fig. 8.14).
E. Joint space computed torque control (Fig. 8.19) with feedforward compensation of the diagonal terms of the inertia matrix and of gravitational
terms, and decentralized feedback controllers as in scheme A.

350

8 Motion Control

F. Joint space PD control with gravity compensation (Fig. 8.20), modiﬁed
by the addition of a feedforward velocity term K D q̇ d , with the following
data:
K D = 750I 2 .
K P = 3750I 2
G. Joint space inverse dynamics control (Fig. 8.22) with the following data:
K P = 25I 2

K D = 5I 2 .

H. Joint space robust control (Fig. 8.23), under the assumption of constant
) = B̄) and compensation of friction and gravity ()
inertia (B
n = F v q̇ + g),
with the following data:
K P = 25I 2

K D = 5I 2

P = I2

ρ = 70

= 0.004.

I. As in case H with = 0.01.
J. Joint space adaptive control (Fig. 8.26) with a parameterization of the
arm dynamic model (7.82) as in (7.83), (7.84). The initial estimate of the
) is computed on the basis of the nominal parameters. The arm
vector π
is supposed to carry a load which causes the following variations on the
second link parameters:
Δm2 = 10 kg

Δm2 C2 = 11 kg·m

ΔI)2 = 12.12 kg·m2 .

This information is obviously utilized only to update the simulated arm
model. Further, the following data are set:
Λ = 5I 2

K D = 750I 2

K π = 0.01I 8 .

K. Operational space PD control with gravity compensation (Fig. 8.29), modiﬁed by the addition of a feedforward velocity term K D ẋd , with the following data:
K D = 3250I 2 .
K P = 16250I 2
L. Operational space inverse dynamics control (Fig. 8.30) with the following
data:
K D = 5I 2 .
K P = 25I 2
It is worth remarking that the adopted model of the dynamic system of arm
with drives is that described by (8.7). In the decentralized control schemes A–
E, the joints have been voltage-controlled as in the block scheme of Fig. 8.3,
with unit ampliﬁer gains (Gv = I). On the other hand, in the centralized
control schemes F–L, the joints have been current-controlled as in the block
scheme of Fig. 8.4, with unit ampliﬁer gains (Gi = I).
Regarding the parameters of the various controllers, these have been chosen in such a way as to allow a signiﬁcant comparison of the performance of
each scheme in response to congruent control actions. In particular, it can be
observed that:

8.7 Comparison Among Various Control Schemes
joint 1 pos

0

2.5
2
[rad]

[rad]

−1
−1.5
−2

1
0.5
1

2
[s]

3

0
0

4

2
[s]

3

4

3

4

pos errors

0.2
[m]

1000
500
0
−500
0

1

joint torques

1500

[Nm]

1.5

−2.5
−3
0

joint 2 pos

3

−0.5

351

0.1
0

1

2
[s]

3

−0.1
0

4

1

2
[s]

Fig. 8.31. Time history of the joint positions and torques and of the tip position
errors for the fast trajectory with control scheme A
joint torques

1500

1000
[Nm]

[Nm]

1000
500
0
−500
0

joint torques

1500

500
0

1

2
[s]

3

−500
0

4

pos error norm

0.015

1

3

4

3

4

pos error norm

0.015

[m]

0.01

[m]

0.01

2
[s]

0.005

0
0

0.005

1

2
[s]

3

4

0
0

1

2
[s]

Fig. 8.32. Time history of the joint torques and of the norm of tip position error
for the fast trajectory; left: with control scheme C, right: with control scheme D

352

8 Motion Control
joint torques

1500

0.01
[m]

[Nm]

1000
500

0.005

0
−500
0

pos error norm

0.015

1

2
[s]

3

0
0

4

1

2
[s]

3

4

Fig. 8.33. Time history of the joint torques and of the norm of tip position error
for the fast trajectory with control scheme E
joint 1 pos

−0.5

2.5

−1

2

−1.5

1

−2.5

0.5
1

2
[s]

3

0
0

4

joint torques

1500

2
[s]

3

4

3

4

pos error norm

0.08
[m]

500
0
−500
0

1

0.1

1000
[Nm]

1.5

−2

−3
0

joint 2 pos

3

[rad]

[rad]

0

0.06
0.04
0.02

1

2
[s]

3

0
0

4

1

2
[s]

Fig. 8.34. Time history of the joint positions and torques and of the norm of tip
position error for the fast trajectory with control scheme F
joint torques

1500

x 10

3
[m]

[Nm]

1000
500
0
−500
0

pos error norm

−4

4

2
1

1

2
[s]

3

4

0
0

1

2
[s]

3

4

Fig. 8.35. Time history of the joint torques and of the norm of tip position error
for the fast trajectory with control scheme G

8.7 Comparison Among Various Control Schemes
joint torques

1500

[Nm]

[Nm]

1000

500

500

0

0

−500
0

1

2
[s]

3

4

−500
0

pos error norm

−4

4

x 10

2
[s]

3

4

3

4

pos error norm

x 10

[m]

3

2
1
0
0

1
−4

4

3
[m]

joint torques

1500

1000

353

2
1

1

2
[s]

3

4

0
0

1

2
[s]

Fig. 8.36. Time history of the joint torques and of the norm of tip position error
for the fast trajectory; left: with control scheme H, right: with control scheme I

• The dynamic behaviour of the joints is the same for schemes A–E.
• The gains of the PD actions in schemes G, H, I and L have been chosen
so as to obtain the same natural frequency and damping ratios as those of
schemes A–E.
The results obtained with the various control schemes are illustrated in
Figs. 8.31–8.39 for the fast trajectory and in Figs. 8.40–8.48 for the slow
trajectory, respectively. In the case of two quantities represented in the same
plot notice that:
• For the joint trajectories, the dashed line indicates the reference trajectory
obtained from the tip trajectory via inverse kinematics, while the solid line
indicates the actual trajectory followed by the arm.
• For the joint torques, the solid line refers to Joint 1 while the dashed line
refers to Joint 2.
• For the tip position error, the solid line indicates the error component along
the horizontal axis while the dashed line indicates the error component
along the vertical axis.
Finally, the representation scales have been made as uniform as possible
in order to allow a more direct comparison of the results.
Regarding performance of the various control schemes for the fast trajectory, the obtained results lead to the following considerations.

354

8 Motion Control
pos error norm

0.01

16
15

0.006

[m]

[m]

0.008

0.004

14
13

0.002
0
0

parameter error norm

17

12
1

2
[s]

3

11
0

4

1

2
[s]

3

4

Fig. 8.37. Time history of the norm of tip position error and of the norm of parameter error vector for the fast trajectory with control scheme J
joint torques

1500

0.08
[m]

[Nm]

1000
500
0
−500
0

pos error norm

0.1

0.06
0.04
0.02

1

2
[s]

3

0
0

4

1

2
[s]

3

4

Fig. 8.38. Time history of the joint torques and of the norm of tip position error
for the fast trajectory with control scheme K
joint torques

1500

x 10

3
[m]

[Nm]

1000
500
0
−500
0

pos error norm

−4

4

2
1

1

2
[s]

3

4

0
0

1

2
[s]

3

4

Fig. 8.39. Time history of the joint torques and of the norm of tip position error
for the fast trajectory with control scheme L

Deviation of the actual joint trajectories from the desired ones shows that
tracking performance of scheme A is quite poor (Fig. 8.31). It should be
noticed, however, that the largest contribution to the error is caused by a
time lag of the actual trajectory behind the desired one, while the distance

8.7 Comparison Among Various Control Schemes
joint 1 pos

0

2.5
2
[rad]

[rad]

−1
−1.5

1.5

−2

1

−2.5

0.5

−3
0

2

4

[s]

6

8

0
0

10

2

joint torques

1500

4

[s]

6

8

10

8

10

pos errors

1000

0.05
[m]

[Nm]

joint 2 pos

3

−0.5

355

500

0

0
−500
0

2

4

[s]

6

8

10

−0.05
0

2

4

[s]

6

Fig. 8.40. Time history of the joint positions and torques and of the tip position
errors for the slow trajectory with control scheme A
joint torques

1500

1000
[Nm]

[Nm]

1000
500

500

0

0

−500
0

2

4

[s]

6

8

10

−500
0

pos error norm

−3

4

x 10

4

[s]

6

8

10

8

10

pos error norm

x 10

[m]

3

2
1
0
0

2
−3

4

3
[m]

joint torques

1500

2
1

2

4

[s]

6

8

10

0
0

2

4

[s]

6

Fig. 8.41. Time history of the joint torques and of the norm of tip position error
for the slow trajectory; left: with control scheme C, right: with control scheme D

356

8 Motion Control
joint torques

1500

x 10

3
[m]

[Nm]

1000
500
0
−500
0

pos error norm

−3

4

2
1

2

4

[s]

6

8

0
0

10

2

4

[s]

6

8

10

Fig. 8.42. Time history of the joint torques and of the norm of tip position error
for the slow trajectory with control scheme E
joint 1 pos

−0.5

2.5

−1

2

−1.5

1

−2.5

0.5
2

4

[s]

6

8

0
0

10

joint torques

1500

4

[s]

6

8

10

8

10

pos error norm

[m]

0.015

500
0
−500
0

2

0.02

1000
[Nm]

1.5

−2

−3
0

joint 2 pos

3

[rad]

[rad]

0

0.01
0.005

2

4

[s]

6

8

0
0

10

2

4

[s]

6

Fig. 8.43. Time history of the joint positions and torques and of the norm of tip
position error for the slow trajectory with control scheme F
joint torques

1500

[m]

[Nm]

x 10

0.8

1000
500
0
−500
0

pos error norm

−4

1

0.6
0.4
0.2

2

4

[s]

6

8

10

0
0

2

4

[s]

6

8

10

Fig. 8.44. Time history of the joint torques and of the norm of tip position error
for the slow trajectory with control scheme G

8.7 Comparison Among Various Control Schemes

joint torques

1500

[Nm]

[Nm]

1000

500

500

0

0

−500
0

2

[s]

6

8

10

−500
0

x 10

0.8

0.8

0.6

0.6

0.4
0.2
0
0

2

4

[s]

6

8

10

8

10

pos error norm

−4

1

[m]

[m]

4

pos error norm

−4

1

joint torques

1500

1000

357

x 10

0.4
0.2

2

4

[s]

6

8

10

0
0

2

4

[s]

6

Fig. 8.45. Time history of the joint torques and of the norm of tip position error
for the slow trajectory; left: with control scheme H, right: with control scheme I

of the tip from the geometric path is quite contained. Similar results were
obtained with scheme B, and then they have not been reported.
With schemes C and D, an appreciable tracking accuracy improvement is
observed (Fig. 8.32), with better performance for the second scheme, thanks
to the outer acceleration feedback loop that allows a disturbance rejection
factor twice as much as for the ﬁrst scheme. Notice that the feedforward
action yields a set of torques which are closer to the nominal ones required to
execute the desired trajectory; the torque time history has a discontinuity in
correspondence of the acceleration and deceleration fronts.
The tracking error is further decreased with scheme E (Fig. 8.33), by virtue
of the additional nonlinear feedforward compensation.
Scheme F guarantees stable convergence to the ﬁnal arm posture with a
tracking performance which is better than that of schemes A and B, thanks to
the presence of a velocity feedforward action, but worse than that of schemes
C–E, in view of lack of an acceleration feedforward action (Fig. 8.34).
As would be logical to expect, the best results are observed with scheme G
for which the tracking error is practically zero, and it is mainly due to numerical discretization of the controller (Fig. 8.35).
It is then worth comparing the performance of schemes H and I (Fig. 8.36).
In fact, the choice of a small threshold value for (scheme H) induces high-

358

8 Motion Control
pos error norm

−3

x 10

parameter error norm

17

4

16

3

15

[m]

[m]

5

2
1

14
13

0
0

2

4

[s]

6

8

12
0

10

2

4

[s]

6

8

10

Fig. 8.46. Time history of the norm of tip position error and of the norm of parameter error vector for the slow trajectory with control scheme J
joint torques

1500

0.015
[m]

[Nm]

1000
500
0
−500
0

pos error norm

0.02

0.01
0.005

2

4

[s]

6

8

0
0

10

2

4

[s]

6

8

10

Fig. 8.47. Time history of the joint torques and of the norm of tip position error
for the slow trajectory with control scheme K
joint torques

1500

[m]

[Nm]

x 10

0.8

1000
500
0
−500
0

pos error norm

−4

1

0.6
0.4
0.2

2

4

[s]

6

8

10

0
0

2

4

[s]

6

8

10

Fig. 8.48. Time history of the joint torques and of the norm of tip position error
for the slow trajectory with control scheme L

frequency components in Joint 1 torque (see the thick portions of the torque
plot) at the advantage of a very limited tracking error. As the threshold value is
increased (scheme I), the torque assumes a smoother behaviour at the expense
of a doubled norm of tracking error, though.

Bibliography

359

For scheme J, a lower tracking error than that of scheme F is observed,
thanks to the eﬀectiveness of the adaptive action on the parameters of the
dynamic model. Nonetheless, the parameters do not converge to their nominal
values, as conﬁrmed by the time history of the norm of the parameter error
vector that reaches a non-null steady-state value (Fig. 8.37).
Finally, the performance of schemes K and L is substantially comparable
to that of corresponding schemes F and G (Figs. 8.38 and 8.39).
Performance of the various control schemes for the slow trajectory is globally better than that for the fast trajectory. Such improvement is particularly
evident for the decentralized control schemes (Figs. 8.40–8.42), whereas the
tracking error reduction for the centralized control schemes is less dramatic
(Figs. 8.43–8.48), in view of the small order of magnitude of the errors already
obtained for the fast trajectory. In any case, as regards performance of each
single scheme, it is possible to make a number of remarks analogous to those
previously made.

Bibliography
The independent joint control is analyzed in classical texts [180, 120, 200], and
scientiﬁc articles [19, 127, 141, 101, 39]. Stability of PD control with gravity
compensation is proved in [7], on the basis of the notable properties of the
dynamic model in [226].
Computed torque control and inverse dynamics control were developed at
the beginning of the 1970s. One of the ﬁrst experimental works is [149]. Other
articles on the topic are [83, 4, 117, 121, 126, 227, 29].
The main approaches of robust control are inspired to the work [50].
Among them it is worth citing [212, 84, 130, 219, 205, 216]. Robust controllers based on the high gain concept are presented in [192, 222]. A survey
on robust control is [1].
One of the ﬁrst approaches to adaptive control, based on the assumption
of decoupled joint dynamics, is presented in [67]. The ﬁrst works on adaptive
control accounting for the manipultor nonlinear dynamics are [15, 167, 100],
yet they exploit the notable properties of the dynamic model only to some
extent. The adaptive version of inverse dynamics control is analyzed in [52,
157]. The approach based on the energy properties of the dynamic model has
been proposed in [214] and further analyzed in [218]. An interesting tutorial
on adaptive control is [175].
Operational space control has been proposed in [114], on the basis of the
resolved acceleration control concept [143]. Inverse dynamics control schemes
in the operational space are given in [30]. For the extension to redundant
manipulators see [102].

360

8 Motion Control

Problems
8.1. With reference to the block scheme with position feedback in Fig. 5.10,
ﬁnd the transfer functions of the forward path, the return path, and the
closed-loop system.
8.2. With reference to the block scheme with position and velocity feedback
in Fig. 5.11, ﬁnd the transfer functions of the forward path, the return path,
and the closed-loop system.
8.3. With reference to the block scheme with position, velocity and acceleration feedback in Fig. 8.9, ﬁnd the transfer functions of the forward path, the
return path, and the closed-loop system.
8.4. For a single joint drive system with the data: I = 6 kg·m2 , Ra = 0.3 ohm,
kt = 0.5 N·m/A, kv = 0.5 V·s/rad, Fm = 0.001 N·m·s/rad, ﬁnd the parameters
of the controller with position feedback (unit transducer constant) that yield a
closed-loop response with damping ratio ζ ≥ 0.4. Discuss disturbance rejection
properties.
8.5. For the drive system of Problem 8.4, ﬁnd the parameters of the controller
with position and velocity feedback (unit transducer constants) that yield
a closed-loop response with damping ratio ζ ≥ 0.4 and natural frequency
ωn = 20 rad/s. Discuss disturbance rejection properties.
8.6. For the drive system of Problem 8.4, ﬁnd the parameters of the controller
with position, velocity and acceleration feedback (unit transducer constants)
that yield a closed-loop response with damping ratio ζ ≥ 0.4, natural frequency ωn = 20 rad/s and disturbance rejection factor XR = 400. Also, design
a ﬁrst-order ﬁlter that allows acceleration measurement reconstruction.
8.7. Verify that the control schemes in Figs. 8.12, 8.13, 8.14 correspond to
realizing (8.42), (8.43), (8.44), respectively.
8.8. Verify that the standard regulation schemes in Figs. 8.15, 8.16, 8.17 are
equivalent to the schemes in Figs. 8.12, 8.13, 8.14, respectively.
8.9. Prove inequality (8.76).
8.10. For the two-link planar arm with the same data as in Sect. 8.7, design a
joint control of PD type with gravity compensation. By means of a computer
simulation, verify stability for the following postures q = [ π/4 −π/2 ]T and
q = [ −π −3π/4 ]T , respectively. Implement the control in discrete-time with
a sampling time of 1 ms.
8.11. For the two-link planar arm with the same data as in Sect. 8.7, under
the assumption of a concentrated tip payload of mass mL = 10 kg, design
an independent joint control with feedforward computed torque. Perform a

Problems

361

computer simulation of the motion of the controlled arm along the joint space
rectilinear path from q i = [ 0 π/4 ]T to q f = [ π/2 π/2 ]T with a trapezoidal
velocity proﬁle and a trajectory duration tf = 1 s. Implement the control in
discrete-time with a sampling time of 1 ms.
8.12. For the two-link planar arm of Problem 8.11, design an inverse dynamics
joint control. Perform a computer simulation of the motion of the controlled
arm along the trajectory speciﬁed in Problem 8.11. Implement the control in
discrete-time with a sampling time of 1 ms.
8.13. For the two-link planar arm of Problem 8.11, design a robust joint control. Perform a computer simulation of the motion of the controlled arm along
the trajectory speciﬁed in Problem 8.11. Implement the control in discretetime with a sampling time of 1 ms.
8.14. For the two-link planar arm of Problem 8.11, design an adaptive joint
control, on the basis of a suitable parameterization of the arm dynamic model.
Perform a computer simulation of the motion of the controlled arm along the
trajectory speciﬁed in Problem 8.11. Implement the control in discrete-time
with a sampling time of 1 ms.
8.15. For the two-link planar of Problem 8.11, design a PD control in the
operational space with gravity compensation. By means of a computer simulation, verify stability for the following postures p = [ 0.5 0.5 ]T and
p = [ 0.6 −0.2 ]T , respectively. Implement the control in discrete-time with
a sampling time of 1 ms.
8.16. For the two-link planar arm of Problem 8.11, design an inverse dynamics
control in the operational space. Perform a computer simulation of the motion
of the controlled arm along the operational space rectlinear path from p(0) =
[ 0.7 0.2 ]T to p(1) = [ 0.1 −0.6 ]T with a trapezoidal velocity proﬁle and a
trajectory duration tf = 1 s. Implement the control in discrete-time with a
sampling time of 1 ms.

9
Force Control

One of the fundamental requirements for the success of a manipulation task
is the capacity to handle interaction between manipulator and environment.
The quantity that describes the state of interaction more eﬀectively is the
contact force at the manipulator’s end-eﬀector. High values of contact force
are generally undesirable since they may stress both the manipulator and the
manipulated object. In this chapter, performance of operational space motion
control schemes is studied ﬁrst, during the interaction of a manipulator with
the environment. The concepts of mechanical compliance and impedance are
introduced, with special regard to the problem of integrating contact force
measurements into the control strategy. Then, force control schemes are presented which are obtained from motion control schemes suitably modiﬁed by
the closure of an outer force regulation feedback loop. For the planning of control actions to perform an interaction task, natural constraints set by the task
geometry and artiﬁcial constraints set by the control strategy are established;
the constraints are referred to a suitable constraint frame. The formulation is
conveniently exploited to derive hybrid force/motion control schemes.

9.1 Manipulator Interaction with Environment
Control of interaction between a robot manipulator and the environment is
crucial for successful execution of a number of practical tasks where the robot’s
end-eﬀector has to manipulate an object or perform some operation on a surface. Typical examples include polishing, deburring, machining or assembly.
A complete classiﬁcation of possible robot tasks is practically infeasible in
view of the large variety of cases that may occur, nor would such a classiﬁcation be really useful to ﬁnd a general strategy to interaction control with the
environment.
During the interaction, the environment sets constraints on the geometric
paths that can be followed by the end-eﬀector. This situation is generally
referred to as constrained motion. In such a case, the use of a purely motion

364

9 Force Control

control strategy for controlling interaction is a candidate to fail, as explained
below.
Successful execution of an interaction task with the environment by using
motion control could be obtained only if the task were accurately planned.
This would, in turn, require an accurate model of both the robot manipulator
(kinematics and dynamics) and the environment (geometry and mechanical
features). Manipulator modelling can be achieved with enough precision, but
a detailed description of the environment is diﬃcult to obtain.
To understand the importance of task planning accuracy, it is suﬃcient to
observe that to perform a mechanical part mating with a positional approach,
the relative positioning of the parts should be guaranteed with an accuracy
of an order of magnitude greater than part mechanical tolerance. Once the
absolute position of one part is exactly known, the manipulator should guide
the motion of the other with the same accuracy.
In practice, the planning errors may give rise to a contact force causing a
deviation of the end-eﬀector from the desired trajectory. On the other hand,
the control system reacts to reduce such deviation. This ultimately leads to a
build-up of the contact force until saturation of the joint actuators is reached
or breakage of the parts in contact occurs.
The higher the environment stiﬀness and position control accuracy, the
more likely a situation like the one just described can occur. This drawback
can be overcome if compliant behaviour is ensured during the interaction.
From the above discussion it should be clear that the contact force is
the quantity describing the state of interaction in the most complete fashion;
to this end, the availability of force measurements is expected to provide
enhanced performance for controlling interaction.
Interaction control strategies can be grouped in two categories; those performing indirect force control and those performing direct force control . The
main diﬀerence between the two categories is that the former achieve force
control via motion control, without explicit closure of a force feedback loop;
the latter, instead, oﬀer the possibility of controlling the contact force to a
desired value, thanks to the closure of a force feedback loop. To the ﬁrst category belong compliance control and impedance control which are treated next.
Then, force control and hybrid force/motion control schemes will follow.

9.2 Compliance Control
For a detailed analysis of interaction between the manipulator and environment it is worth considering the behaviour of the system under a position
control scheme when contact forces arise. Since these are naturally described
in the operational space, it is convenient to refer to operational space control
schemes.

9.2 Compliance Control

365

Consider the manipulator dynamic model (8.7). In view of (7.42), the
model can be written as
B(q)q̈ + C(q, q̇)q̇ + F q̇ + g(q) = u − J T (q)he

(9.1)

where he is the vector of contact forces exerted by the manipulator’s endeﬀector on the environment.1
It is reasonable to predict that, in the case he = 0, the control scheme
based on (8.110) no longer ensures that the end-eﬀector reaches its desired
pose xd . In fact, by recalling that x = xd − xe , where xe denotes the endeﬀector pose, at the equilibrium it is
J TA (q)K P x = J T (q)he .

(9.2)

On the assumption of a full-rank Jacobian, one has
T
−1
x = K −1
P T A (xe )he = K P hA

(9.3)

where hA is the vector of equivalent forces that can be deﬁned according
to (7.128). The expression in (9.3) shows that at the equilibrium the manipulator, under a pose control action, behaves like a generalized spring in the
operational space with compliance K −1
P in respect of the equivalent force hA .
By recalling the expression of the transformation matrix T A in (3.65) and assuming matrix K P to be diagonal, it can be recognized that linear compliance
(due to force components) is independent of the conﬁguration, whereas torsional compliance (due to moment components) does depend on the current
end-eﬀector orientation through the matrix T .
On the other hand, if he ∈ N (J T ), one has x = 0 with he = 0, namely
contact forces are completely balanced by the manipulator mechanical structure; for instance, the anthropomorphic manipulator at a shoulder singularity
in Fig. 3.13 does not react to any force orthogonal to the plane of the structure.
Equation (9.3) can be rewritten in the form
hA = K P x

(9.4)

where K P represents a stiﬀness matrix as regards the vector of the equivalent
forces hA . It is worth observing that the compliant (or stiﬀ) behaviour of the
manipulator is achieved by virtue of the control. This behaviour is termed
active compliance whereas the term passive compliance denotes mechanical
systems with a prevalent dynamics of elastic type.
For a better understanding of the interaction between manipulator and
environment, it is necessary to analyze further the concept of passive compliance.
1

In this chapter the term force, in general, is referred to a (6 × 1) vector of force
and moment, unless otherwise speciﬁed.

366

9 Force Control

9.2.1 Passive Compliance
Consider two elastically coupled rigid bodies R and S and two reference
frames, each attached to one of the two bodies so that at equilibrium, in
the absence of interaction forces and moments, the two frames coincide. Let
dxr,s denote an elementary displacement from the equilibrium of frame s with
respect to frame r, deﬁned as


dor,s
(9.5)
dxr,s =
= v r,s dt
ω r,s dt
where v r,s = v s − v r is the vector of linear and angular velocity of frame
s with respect to frame r, dor,s = os − or is the vector corresponding to
the translation of the origin os of frame s with respect to the origin or of
frame r and ω r,s dt, with ω r,s = ω s − ω r , represents the vector of small
rotations of frame s about the axes of frame r as in (3.106). This elementary
displacement is assumed to be equivalently referred to frame r or s because,
at the equilibrium, the two frames coincide; therefore, the reference frame was
not explicitly denoted.
To the displacement dxr,s , coinciding with the deformation of the spring
between R and S, it corresponds the elastic force
  


Kf Kc
fs
dor,s
(9.6)
=
= Kdxr,s ,
hs =
μs
ω r,s dt
K Tc K μ
applied by body S on the spring and referred equivalently to one of the two
reference frames. In view of the action-reaction law, the force applied by R
has the expression hr = −hs = Kdxs,r , being dxs,r = −dxr,s .
The (6 × 6) matrix K represents a stiﬀness matrix , which is symmetric
and positive semi-deﬁnite. The (3 × 3) matrices K f and K μ are known as
translational stiﬀness and rotational stiﬀness, respectively. The (3×3) matrix
K c is known as coupling stiﬀness. An analogous decomposition can be made
for the compliance matrix C in the mapping
dxr,s = Chs .

(9.7)

In the real elastic systems, matrix K c is, in general, non-symmetric. However, there are special devices, such as the RCC (Remote Centre of Compliance), where K c can be symmetric or null. These are elastically compliant mechanical devices, suitably designed to achieve maximum decoupling between
translation and rotation, that are interposed between the manipulator last
link and the end-eﬀector. The aim is that of introducing a passive compliance
of desired value to facilitate the execution of assembly tasks. For instance, in a
peg-in-hole insertion task, the gripper is provided with a device ensuring high
stiﬀness along the insertion direction and high compliance along the other
directions. Therefore, in the presence of unavoidable position displacements

9.2 Compliance Control

367

from the planned insertion trajectory, contact forces and moments arise which
modify the peg position so as to facilitate insertion.
The inconvenience of such devices is their low versatility to diﬀerent operating conditions and generic interaction tasks, namely, whenever a modiﬁcation of the compliant mechanical hardware is required.
9.2.2 Active Compliance
The aim of compliance control is that of achieving a suitable active compliance
that can be easily modiﬁed acting on the control software so as to satisfy the
requirements of diﬀerent interaction tasks.
Notice that the equilibrium equations in (9.3) and (9.4) show that the
compliant behaviour with respect to he depends on the actual end-eﬀector
orientation, also for elementary displacements, so that, in practice, the selection of stiﬀness parameters is quite diﬃcult. To obtain an equilibrium equation
of the form (9.6), a diﬀerent deﬁnition of error in the operational space must
be considered.
Let Oe –xe ye ze and Od –xd yd zd denote the end-eﬀector frame and the desired frame respectively. The corresponding homogeneous transformation matrices are




Re oe
Rd od
Td =
,
Te =
0T 1
0T
1
with obvious meaning of notation. The position and orientation displacement
of the end-eﬀector frame with respect to the desired frame can be expressed
in terms of the homogeneous transformation matrix
 d

Re odd,e
d
−1
T e = (T d ) T e =
,
(9.8)
0T
1
where Rde = RTd Re and odd,e = RTd (oe − od ). The new error vector in the
operational space can be deﬁned as
 d 
o ,
(9.9)
x = − d,e
φd,e
where φd,e is the vector of Euler angles extracted from the rotation matrix
Rde . The minus sign in (9.9) depends on the fact that, for control purposes, the
error is usually deﬁned as the diﬀerence between the desired and the measured
quantities.
Computing the time derivative of odd,e and taking into account (3.10),
(3.11) gives
(9.10)
ȯdd,e = RTd (ȯe − ȯd ) − S(ω dd )RTd (oe − od ).
On the other hand, computing the time derivative of φd,e and taking into
account (3.64), yields (see Problem 9.1)
φ̇d,e = T −1 (φd,e )ω dd,e = T −1 (φd,e )RTd (ω e − ω d ).

(9.11)

368

9 Force Control

Considering that the desired quantities od and Rd are constant, vector x˙ can
be expressed in the form
 T

O
˙x = −T −1 (φ ) Rd
(9.12)
ve
d,e
A
O RTd
T

being v e = [ ȯTe ω Te ] = J (q)q̇ the vector of linear and angular velocity of
the end-eﬀector. Therefore
x˙ = −J Ad (q, x)q̇,
where the matrix
J Ad (q, x) =

T −1
A (φd,e )



RTd
O

(9.13)

O
RTd


J (q)

(9.14)

represents the analytic Jacobian corresponding to the deﬁnition (9.9) of error
in the operational space.
The PD control with gravity compensation analogous to (8.110), with the
deﬁnition (9.9) of error in the operational space, has the expression
u = g(q) + J TAd (q, x)(K P x − K D J Ad (q, x)q̇).

(9.15)

Notice that, in the case where the operational space is deﬁned only by the
position components, the control laws (8.110) and (9.15) diﬀer only because
the position error (and the corresponding derivative term) is referred to the
base frame in (8.110), while it is referred to the desired frame in (9.15).
In the absence of interaction, the asymptotic stability of the equilibrium
pose corresponding to x = 0, assuming that K P and K D are symmetric and
positive deﬁnite matrices, can be proven using the Lyapunov function
V (q̇, x) =

1 T
1
q̇ B(q)q̇ + xT K P x > 0
2
2

∀q̇, x = 0,

as for the case of the control law (8.110).
In the presence of interaction with the environment, at the equilibrium it
is
J TAd (q)K P x = J T (q)he ;
(9.16)
hence, assuming a full-rank Jacobian, the following equality holds:
hde = T −T
A (φd,e )K P x.

(9.17)

The above equation, to be compared to the elastic model (9.6), must be
rewritten in terms of elementary displacements. To this end, taking into account (9.12) and (9.5), it is
#
#
−1
d
d
dx = x˙ #
dt = T −1
(9.18)
A (0)(v d − v e )dt = T A (0)dxe,d
x =0

9.2 Compliance Control

369

where dxe,d is the elementary displacement of the desired frame with respect
to the end-eﬀector frame about the equilibrium, referred to any of the two
frames. The value of T A (0) depends on the particular choice of Euler angles;
in the following, angles XYZ are adopted, for which T A (0) = I (see Problem 3.13). Therefore, rewriting (9.17) in terms of elementary displacements
gives
(9.19)
he = K P dxe,d ,
which is formally identical to (9.6), where vectors are assumed to be referred
to the desired frame or to the end-eﬀector frame, equivalently. It follows that
matrix K P has the meaning of an active stiﬀness corresponding to a generalized spring acting between the end-eﬀector frame and the desired frame.
Equation (9.19) can be rewritten in the equivalent form
dxe,d = K −1
P he ,

(9.20)

showing that K −1
P corresponds to an active compliance.
The selection of the elements of matrix K P must be made taking into account geometry and mechanical features of the environment. To this end, assume that the interaction force between the end-eﬀector and the environment
derives from a generalized spring acting between the end-eﬀector frame and
a reference frame Or –xr yr zr attached to the environment rest position. Considering an elementary displacement dxr,e between the two reference frames,
the corresponding elastic force applied by the end-eﬀector is
he = Kdxr,e

(9.21)

with a stiﬀness matrix K, where vectors can be referred, equivalently, to the
frame attached to the rest position of the environment or to the end-eﬀector
frame. Typically, the stiﬀness matrix is positive semi-deﬁnite because, in general, the interaction forces and moments belong to some particular directions,
spanning R(K).
In view of the model (9.21), of (9.19) and of the equality
dxr,e = dxr,d − dxe,d ,
the following expression of the contact force at equilibrium can be found:
−1
Kdxr,d .
(9.22)
he = I 6 + KK −1
P
Substituting this expression into (9.20) yields
−1
dxe,d = K −1
P I 6 + KK P

−1

Kdxr,d ,

(9.23)

representing the pose error of the end-eﬀector at the equilibrium.
Notice that vectors in (9.22) and (9.23) can be referred, equivalently, to the
end-eﬀector frame, to the desired frame or to the frame attached to the environment rest position; these frames coincide at equilibrium (see Problem 9.2).

370

9 Force Control

Fig. 9.1. Two-link planar arm in contact with an elastically compliant plane

The analysis of (9.23) shows that the end-eﬀector pose error at the equilibrium depends on the environment rest position, as well as on the desired
pose imposed by the control system of the manipulator. The interaction of
the two systems (environment and manipulator) is inﬂuenced by the mutual
weight of the respective compliance features.
In fact, it is possible to modify the active compliance K −1
P so that the
manipulator dominates the environment and vice versa. Such a dominance
can be speciﬁed with reference to the single directions of the operational
space.
For a given environment stiﬀness K, according to the prescribed interaction task, one may choose large values of the elements of K P for those
directions along which the environment has to comply and small values of
the elements of K P for those directions along which the manipulator has to
comply. As a consequence, the manipulator pose error dxe,d tends to zero
along the directions where the environment complies; vice versa, along the
directions where the manipulator complies, the end-eﬀector pose tends to the
rest pose of the environment, namely dxe,d  dxr,d .
Equation (9.22) gives the value of the contact force at the equilibrium.
This expression reveals that, along the directions where the manipulator stiﬀness is much higher than the environment stiﬀness, the intensity of the elastic
force mainly depends on the stiﬀness of the environment and on the displacement dxr,e between the equilibrium pose of the end-eﬀector (which practically
coincides with the desired pose) and the rest pose of the environment. In the
dual case that the environment stiﬀness is much higher than the manipulator
stiﬀness, the intensity of the elastic force mainly depends on the manipulator stiﬀness and on the displacement dxe,d between the desired pose and the
equilibrium pose of the end-eﬀector (which practically coincides with the rest
pose of the environment).

9.2 Compliance Control

371

Example 9.1
Consider the two-link planar arm whose tip is in contact with a purely frictionless
elastic plane; due to the simple geometry of the problem, involving only position
variables, all the quantities can be conveniently referred to the base frame. Thus,
control law (8.110) will be adopted. Let or = [ xr 0 ]T denote the equilibrium
position of the plane, which is assumed to be orthogonal to axis x0 (Fig. 9.1). The
environment stiﬀness matrix is
K = K f = diag{kx , 0},
corresponding to the absence of interaction forces along the vertical direction (f e =
[ fx 0 ]T ). Let oe = [ xe ye ]T be the end-eﬀector position and od = [ xd yd ]T be
the desired position, which is located beyond the contact plane. The proportional
control action on the arm is characterized by
K P = diag{kP x , kP y }.
The equilibrium equations for the force and position (9.22), (9.23), rewritten with
dxr,d = od − or and dxe,d = od − oe , give

 k k
Px x

fe =

(xd − xr )



kP x + kx
0



oe =

kP x xd + kx xr
kP x + kx
yd



.

With reference to positioning accuracy, the arm tip reaches the vertical coordinate
yd , since the vertical motion direction is not constrained. As for the horizontal
direction, the presence of the elastic plane imposes that the arm can move as far as
it reaches the coordinate xe < xd . The value of the horizontal contact force at the
equilibrium is related to the diﬀerence between xd and xr by an equivalent stiﬀness
coeﬃcient which is given by the parallel composition of the stiﬀness coeﬃcients
of the two interacting systems. Hence, the arm stiﬀness and environment stiﬀness
inﬂuence the resulting equilibrium conﬁguration. In the case when
kP x /kx  1,
it is
xe ≈ xd

fx ≈ kx (xd − xr )

and thus the arm prevails over the environment, in that the plane complies almost
up to xd and the elastic force is mainly generated by the environment (passive
compliance). In the opposite case
kP x /kx

1,

it is
xe ≈ xr

fx ≈ kP x (xd − xr )

and thus the environment prevails over the arm which complies up to the equilibrium
xr , and the elastic force is mainly generated by the arm (active compliance).

372

9 Force Control

To complete the analysis of manipulator compliance in contact with environment, it is worth considering the eﬀects of a joint space position control
law. With reference to (8.51), in the presence of end-eﬀector contact forces,
the equilibrium posture is determined by

and then

K P q = J T (q)he

(9.24)

T
q = K −1
P J (q)he .

(9.25)

On the assumption of small displacements from the equilibrium, it is reasonable to compute the resulting operational space displacement as dx ≈ J (q)dq,
referred to the base frame. Therefore, in view of (9.25) it is
T
dx = J (q)K −1
P J (q)he ,

(9.26)

corresponding to an active compliance referred to the base frame. Notice that
T
the compliance matrix J (q)K −1
P J (q) depends on the manipulator posture,
both for the force and moment components. Also in this case, the occurrence
of manipulator Jacobian singularities is to be analyzed apart.

9.3 Impedance Control
It is now desired to analyze the interaction of a manipulator with the environment under the action of an inverse dynamics control in the operational
space. With reference to model (9.1), consider the control law (8.57)
u = B(q)y + n(q, q̇),
with n as in (8.56). In the presence of end-eﬀector forces, the controlled manipulator is described by
q̈ = y − B −1 (q)J T (q)he

(9.27)

that reveals the existence of a nonlinear coupling term due to contact forces.
Choose y in a way conceptually analogous to (8.114), as
−1
M d ẍd + K D x˙ + K P x − M d J̇ A (q, q̇)q̇
y = J −1
A (q)M d



(9.28)

where M d is a positive deﬁnite diagonal matrix. Substituting (9.28) into (9.27)
and accounting for second-order diﬀerential kinematics in the form (3.98),
yields
¨ + K D x˙ + K P x = M d B −1 (q)hA ,
(9.29)
M dx
A
where

−1
B A (q) = J −T
A (q)B(q)J A (q)

9.3 Impedance Control

373

is the inertia matrix of the manipulator in the operational space as in (7.134);
this matrix is conﬁguration-dependent and is positive deﬁnite if J A has full
rank.
The expression in (9.29) establishes a relationship through a generalized
mechanical impedance between the vector of forces M d B −1
A hA and the vector
of displacements x in the operational space. This impedance can be attributed
to a mechanical system characterized by a mass matrix M d , a damping matrix
K D , and a stiﬀness matrix K P , which can be used to specify the dynamic
behaviour along the operational space directions.
The presence of B −1
A makes the system coupled. If it is wished to keep
linearity and decoupling during interaction with the environment, it is then
necessary to measure the contact force; this can be achieved by means of
appropriate force sensors which are usually mounted to the manipulator wrist,
as discussed in Sect. 5.4.1. Choosing
u = B(q)y + n(q, q̇) + J T (q)he

(9.30)

with

−1
y = J −1
M d ẍd + K D x˙ + K P x − M d J̇ A (q, q̇)q̇ − hA ,
A (q)M d

(9.31)

under the assumption of error-free force measurements, yields
¨ + K D x˙ + K P x = hA .
M dx

(9.32)

It is worth noticing that the addition of the term J T he in (9.30) exactly
compensates the contact forces and then it renders the manipulator inﬁnitely
stiﬀ as regards the external stress. In order to confer a compliant behaviour
−1
to the manipulator, the term −J −1
A M d hA has been introduced in (9.31) so
that the manipulator can be characterized as a linear impedance with regard
to the equivalent forces hA , as shown in (9.32).
The behaviour of the system in (9.32) at the equilibrium is analogous to
that described by (9.4); nonetheless, compared to a control with active compliance speciﬁed by K −1
P , the equation in (9.32) allows a complete characterization of system dynamics through an active impedance speciﬁed by matrices
M d , K D , K P . Also in this case, it is not diﬃcult to recognize that, as regards he , impedance depends on the current end-eﬀector orientation through
the matrix T . Therefore, the selection of the impedance parameters becomes
diﬃcult; moreover, an inadequate behaviour may occur in the neighbourhood
of representation singularities.
To avoid this problem it is suﬃcient to redesign the control input y as a
function of the operational space error (9.9).
Under the assumption that the desired frame Od –xd yd zd is time-varying,
in view of (9.10), (9.11), the time derivative of (9.9) has the expression
x˙ = −J Ad (q, x)q̇ + b(x, Rd , ȯd , ω d ),

(9.33)

374

9 Force Control

Fig. 9.2. Block scheme of impedance control

where J Ad is the analytic Jacobian (9.14) and vector b is
 T

Rd ȯd + S(ωdd )odd,e
b(x, Rd , ȯd , ω d ) =
.
T −1 (φd,e )ω dd
Computing the time derivative of (9.33) yields
¨ = −J A q̈ − J̇ A q̇ + ḃ,
x
d
d

(9.34)

where, for simplicity, the dependence of the functions on their arguments was
omitted. As a consequence, using (9.30) with

−1
K D x˙ + K P x − M d J̇ Ad q̇ + M d ḃ − hde ,
y = J −1
Ad M d
yields the equation

¨ + K D x˙ + K P x = hd ,
M dx
e

(9.35)
(9.36)

where all the vectors are referred to the desired frame. This equation represents a linear impedance as regards the force vector hde , independent from the
manipulator conﬁguration.
The block scheme representing impedance control is reported in Fig. 9.2.
Similar to active and passive compliance, the concept of passive impedance can be introduced if the interaction force he is generated at the contact
with an environment of proper mass, damping and stiﬀness. In this case,
the system of manipulator with environment can be regarded as a mechanical
system constituted by the parallel of the two impedances, and then its dynamic
behaviour is conditioned by the relative weight between them.

Example 9.2
Consider the planar arm in contact with an elastically compliant plane of the previous example. Due to the simple geometry of the problem, involving only position

9.3 Impedance Control

375

Fig. 9.3. Equivalent block scheme of a manipulator in contact with an elastic
environment under impedance control

variables, all the quantities can be conveniently referred to the base frame. Thus,
the impedance control law with force measurement (9.30), (9.31) will be adopted.
Moreover, xd = od , x = od − oe , hA = f e and
M d = diag{mdx , mdy }

K D = diag{kDx , kDy }

K P = diag{kP x , kP y }.

The block scheme of the manipulator in contact with an elastic environment
under impedance control is represented in Fig. 9.3, where xe = oe and xr = or .
If xd is constant, the dynamics of the manipulator and environment system
along the two directions of the operational space is described by
mdx ẍe + kDx ẋe + (kP x + kx )xe = kx xr + kP x xd
mdy ÿe + kDy ẏe + kP y ye = kP y yd .
Along the vertical direction, one has an unconstrained motion whose time behaviour is determined by the following natural frequency and damping factor:

.
ωny =

kP y
mdy

ζy =

kDy
2

mdy kP y

,

while, along the horizontal direction, the behaviour of the contact force fx = kx (xe −
xr ) is determined by

.
ωnx =

kP x + kx
mdx

ζx =

kDx
2

.

mdx (kP x + kx )

Below, the dynamic behaviour of the system is analyzed for two diﬀerent values
of environment compliance: kx = 103 N/m and kx = 104 N/m. The rest position of
the environment is xr = 1. The actual arm is the same as in Example 7.2. Apply
an impedance control with force measurement of the kind (9.30), (9.31), and PD
control actions equivalent to those chosen in the simulations of Sect. 8.7, namely
mdx = mdy = 100

kDx = kDy = 500

kP x = kP y = 2500.

376

9 Force Control
tip y−pos

tip x−force

300
250
200
[N]

[m]

0.1

0.05

150
100
50

0
0

1

[s]

2

0
0

3

1

[s]

2

3

Fig. 9.4. Time history of the tip position along vertical direction and of the contact
force along horizontal direction with impedance control scheme for environments of
diﬀerent compliance

For these values it is
ωny = 5 rad/s

ζy = 0.5.

Then, for the more compliant environment it is
ωnx ≈ 5.9 rad/s

ζx ≈ 0.42

whereas for the less compliant environment it is
ωnx ≈ 11.2 rad/s

ζx ≈ 0.22.

Let the arm tip be in contact with the environment at position xe = [ 1 0 ]T ; it is
desired to take it to position xd = [ 1.1 0.1 ]T .
The results in Fig. 9.4 show that motion dynamics along the vertical direction
is the same in the two cases. As regards the contact force along the horizontal direction, for the more compliant environment (dashed line) a well-damped behaviour is
obtained, whereas for the less compliant environment (solid line) the resulting behaviour is less damped. Further, at the equilibrium, in the ﬁrst case a displacement
of 7.14 cm with a contact force of 71.4 N, whereas in the second case a displacement
of 2 cm with a contact force of 200 N are observed.

The selection of good impedance parameters, so as to achieve a satisfactory
behaviour during the interaction, is not an easy task. Example 9.2 showed that
the closed-loop dynamics along the free motion directions is diﬀerent from the
closed-loop dynamics along the constrained directions. In this latter case, the
dynamic behaviour depends on the stiﬀness characteristics of the environment.
The execution of a complex task, involving diﬀerent types of interaction, may
require diﬀerent values of impedance parameters.
Notice that impedance control, in the absence of interaction or along the
directions of free motion, is equivalent to an inverse dynamics position control.
Therefore, for the selection of the impedance parameters, one also has to take
into account the need to ensure high values to the rejection factor of the
disturbances due to model uncertainties and to the approximations into the

9.3 Impedance Control

377

Fig. 9.5. Block scheme of admittance control

inverse dynamics computation. Such a factor increases proportionally to the
gain matrix K P . Hence the closed-loop behaviour is the more degraded by
disturbances, the more compliant the impedance control is made (by choosing
low values for the elements of K P ) to keep interaction forces limited.
A possible solution may be that of separating the motion control problem
from the impedance control problem according to the control scheme represented in Fig. 9.5. The scheme is based on the concept of compliant frame,
which is a suitable reference frame describing the ideal behaviour of the endeﬀector under impedance control. This frame is speciﬁed by the position of
the origin ot , the rotation matrix Rt , as well as by the liner and angular
velocities and accelerations. These quantities can be computed by integrating
the impedance equations in the form
¨ + K Dt z˙ + K P t z = hd ,
M tz
e

(9.37)

starting from the measurements of the force vector he , where M t , K Dt , K P t
are the parameters of a mechanical impedance. In the above equation, vector
z represents the operational space error between the desired frame and the
compliant frame, as deﬁned in (9.9), using subscript t in place of subscript e.
The kinematic variables of the compliant frame are then input to the
motion control of inverse dynamics type, computed according to Eqs. (9.28),
(9.30). In this way, the gains of the motion control law (9.28) can be designed
so as to guarantee a high value of the disturbance rejection factor. On the
other hand, the gains of the impedance control law (9.37) can be set so as to
guarantee satisfactory behaviour during the interaction with the environment.
Stability of the overall system can be ensured provided that the equivalent
bandwidth of the motion control loop is larger than the equivalent bandwidth
of the impedance control loop.
The above control scheme is also known as admittance control because
Equation (9.37) corresponds to a mechanical admittance being used by the
controller to generate the motion variables (outputs) from the force measurements (inputs). On the other hand, the control deﬁned by Eqs. (9.31) or (9.35)

378

9 Force Control

and (9.30) can be interpreted as a system producing equivalent end-eﬀector
forces (outputs) form the measurements of the motion variables (inputs), thus
corresponding to a mechanical impedance.

9.4 Force Control
The above schemes implement an indirect force control , because the interaction force can be indirectly controlled by acting on the desired pose of the
end-eﬀector assigned to the motion control system. Interaction between manipulator and environment is anyhow directly inﬂuenced by compliance of the
environment and by either compliance or impedance of the manipulator.
If it is desired to control accurately the contact force, it is necessary to
devise control schemes that allow the desired interaction force to be directly
speciﬁed. The development of a direct force control system, in analogy to a
motion control system, would require the adoption of a stabilizing PD control
action on the force error besides the usual nonlinear compensation actions.
Force measurements may be corrupted by noise, and then a derivative action
may not be implemented in practice. The stabilizing action is to be provided
by suitable damping of velocity terms. As a consequence, a force control system typically features a control law based not only on force measurements but
also on velocity measurements, and eventually position measurements too.
The realization of a force control scheme can be entrusted to the closure
of an outer force regulation feedback loop generating the control input for the
motion control scheme the manipulator is usually endowed with. Therefore,
force control schemes are presented below, which are based on the use of an
inverse dynamics position control. The eﬀectiveness of a such control scheme
depends on the particular interaction cases and, in general, on the contact
geometry. To this end, notice that a force control strategy is meaningful only
for those directions of the operational space along which interaction forces
between manipulator and environment may arise.
Below, force control schemes based on the adoption of motion control laws
of inverse dynamics type are presented, assuming that the operational space
is deﬁned only by position variables. Therefore, the end-eﬀector pose can be
speciﬁed by the operational space vector xe = oe . Moreover, the elastic model
f e = K(xe − xr )

(9.38)

is assumed for the environment, obtained from (9.21) with the assumption that
only forces arise at the contact. In (9.38), consider xr = or and assume that
the axes of the frame attached to the environment rest position are parallel
to the axes of the base frame. The above assumptions allow some important
features of force control to be evidenced.

9.4 Force Control

379

Fig. 9.6. Block scheme of force control with inner position loop

9.4.1 Force Control with Inner Position Loop
With reference to the inverse dynamics law with force measurement (9.30),
choose in place of (9.31), the control

−K D ẋe + K P (xF − xe ) − M d J̇ (q, q̇)q̇
y = J −1 (q)M −1
(9.39)
d
where xF is a suitable reference to be related to a force error. Notice that
the control law (9.39) does not foresee the adoption of compensating actions
relative to ẋF and ẍF . Moreover, since the operational space is deﬁned only by
position variables, the analytic Jacobian coincides with the geometric Jacobian
and thus J A (q) = J (q).
Substituting (9.30), (9.39) into (9.1), leads, after similar algebraic manipulation as above, to the system described by
M d ẍe + K D ẋe + K P xe = K P xF ,

(9.40)

which shows how (9.30) and (9.39) perform a position control taking xe to
xF with a dynamics speciﬁed by the choice of matrices M d , K D , K P .
Let f d denote the desired constant force reference; the relation between
xF and the force error can be expressed as
xF = C F (f d − f e ),

(9.41)

where C F is a diagonal matrix, with the meaning of compliance, whose elements give the control actions to perform along the operational space directions of interest. The equations in (9.40), (9.41) reveal that force control is
developed on the basis of a preexisting position control loop.
On the assumption of the elastically compliant environment described
by (9.38), the equation in (9.40) with (9.41) becomes
M d ẍe + K D ẋe + K P (I 3 + C F K)xe = K P C F (Kxr + f d ).

(9.42)

To decide about the kind of control action to specify with C F , it is worth
representing (9.21), (9.40), (9.41) in terms of the block scheme in Fig. 9.6,

380

9 Force Control

Fig. 9.7. Block scheme of force control with inner velocity loop

which is logically derived from the scheme in Fig. 9.3. This scheme suggests
that if C F has a purely proportional control action, then f e cannot reach f d
and xr inﬂuences the interaction force also at steady state.
If C F also has an integral control action on the force components, then it is
possible to achieve f e = f d at steady state and, at the same time, to reject the
eﬀect of xr on f e . Hence, a convenient choice for C F is a proportional-integral
(PI) action
" t
(·) dς.
(9.43)
CF = KF + KI
The dynamic system resulting from (9.42), (9.43) is of third order, and then it
is necessary to choose adequately the matrices K D , K P , K F , K I in respect
of the characteristics of the environment. Since the values of environment
stiﬀness are typically high, the weight of the proportional and integral actions
should be contained; the choice of K F and K I inﬂuences the stability margins
and the bandwidth of the system under force control. On the assumption that
a stable equilibrium is reached, it is f e = f d and then
Kxe = Kxr + f d .

(9.44)

9.4.2 Force Control with Inner Velocity Loop
From the block scheme of Fig. 9.6 it can be observed that, if the position
feedback loop is opened, xF represents a velocity reference, and then an integration relationship exists between xF and xe . This leads to recognizing
that, in this case, the interaction force with the environment coincides with
the desired value at steady state, even with a proportional force controller
C F . In fact, choosing

y = J −1 (q)M −1
−K D ẋe + K P xF − M d J̇ (q, q̇)q̇ ,
(9.45)
d
with a purely proportional control structure (C F = K F ) on the force error
yields
(9.46)
xF = K F (f d − f e )

9.4 Force Control

381

Fig. 9.8. Block scheme of parallel force/position control

and then system dynamics is described by
M d ẍe + K D ẋe + K P K F Kxe = K P K F (Kxr + f d ).

(9.47)

The relationship between position and contact force at the equilibrium is given
by (9.44). The corresponding block scheme is reported in Fig. 9.7. It is worth
emphasizing that control design is simpliﬁed, since the resulting system now is
of second order;2 it should be noticed, however, that the absence of an integral
action in the force controller does not ensure reduction of the eﬀects due to
unmodelled dynamics.
9.4.3 Parallel Force/Position Control
The force control schemes presented require the force reference to be consistent
with the geometric features of the environment. In fact, if f d has components
outside R(K), both (9.42) (in case of an integral action in C F ) and (9.47)
show that, along the corresponding operational space directions, the components of f d are interpreted as velocity references which cause a drift of the
end-eﬀector position. If f d is correctly planned along the directions outside
R(K), the resulting motion governed by the position control action tends to
take the end-eﬀector position to zero in the case of (9.42), and the end-eﬀector
velocity to zero in the case of (9.47). Hence, the above control schemes do not
allow position control even along the admissible task space directions.
If it is desired to specify a desired end-eﬀector pose xd as in pure position
control schemes, the scheme of Fig. 9.6 can be modiﬁed by adding the reference
xd to the input where positions are summed. This corresponds to choosing

−K D ẋe + K P (x + xF ) − M d J̇ A (q, q̇)q̇
y = J −1 (q)M −1
(9.48)
d
where x = xd − xe . The resulting scheme (Fig. 9.8) is termed parallel
force/position control , in view of the presence of a position control action
2

The matrices K P and K F are not independent and one may refer to a single
matrix K F = K P K F .

382

9 Force Control
tip x−pos

1.02

40
[N]

[m]

1.015
1.01
1.005
1
0

tip x−force

50

30
20
10

1

[s]

2

3

0
0

1

[s]

2

3

Fig. 9.9. Time history of the tip position and of the contact force along horizontal
direction with force control scheme with inner position loop for two environments
of diﬀerent compliance

K P x in parallel to a force control action K P C F (f d − f e ). It is easy to
verify that, in this case, the equilibrium position satisﬁes the equation (see
Problem 9.4)

(9.49)
xe = xd + C F K(xr − xe ) + f d .
Therefore, along those directions outside R(K) where motion is unconstrained, the position reference xd is reached by xe . Vice versa, along those
directions in R(K) where motion is constrained, xd is treated as an additional disturbance; the adoption of an integral action in C F as for the scheme
of Fig. 9.6 ensures that the force reference f d is reached at steady state, at
the expense of a position error on xe depending on environment compliance.

Example 9.3
Consider again the planar arm in contact with the elastically compliant plane of the
above examples; let the initial contact position be the same as that of Example 9.2.
Performance of the various force control schemes is analyzed; as in Example 9.2,
a more compliant (kx = 103 N/m) and a less compliant (kx = 104 N/m) environment are considered. The position control actions M d , K D , K P are chosen as in
Example 9.2; a force control action is added along the horizontal direction, namely
C F = diag{cF x , 0}.
The reference for the contact force is chosen as f d = [ 10 0 ]T ; the position reference
— meaningful only for the parallel control — is taken as xd = [ 1.015 0.1 ]T .
With regard to the scheme with inner position loop of Fig. 9.6, a PI control
action cF x is chosen with parameters:
kF x = 0.00064

kIx = 0.0016.

This confers two complex poles (−1.96, ±j5.74), a real pole (−1.09), and a real zero
(−2.5) to the overall system, for the more compliant environment.

9.4 Force Control
tip x−pos

1.02

40
[N]

[m]

tip x−force

50

1.015
1.01
1.005
1
0

383

30
20
10

1

[s]

2

0
0

3

1

[s]

2

3

Fig. 9.10. Time history of the tip position and of the contact force along horizontal
direction with force control scheme with inner velocity loop for two environments of
diﬀerent compliance
tip x−pos

1.02

40
[N]

[m]

1.015
1.01
1.005
1
0

tip x−force

50

30
20
10

1

[s]

2

3

0
0

1

[s]

2

3

Fig. 9.11. Time history of tip position and of the contact force along horizontal direction with parallel force/position control scheme for two environments of diﬀerent
compliance

With regard to the scheme with inner velocity loop of Fig. 9.7, the proportional
control action in cF x is
kF x = 0.0024
so that the overall system, for the more compliant environment, has two complex
poles (−2.5, ±j7.34).
With regard to the parallel control scheme of Fig. 9.8, the PI control action cF x
is chosen with the same parameters as for the ﬁrst control scheme.
Figures 9.9, 9.10, 9.11 report the time history of the tip position and contact
force along axis x0 for the three considered schemes. A comparison between the
various cases shows what follows:
• All control laws guarantee a steady-state value of contact forces equal to the
desired one for both the more compliant (dashed line) and the less compliant
(continuous line) environment.
• For given motion control actions (M d , K D , K P ), the force control with inner
velocity loop presents a faster dynamics than that of the force control with inner
position loop.
• The dynamic response with the parallel control shows how the addition of a position reference along the horizontal direction degrades the transient behaviour,

384

9 Force Control

but it does not inﬂuence the steady-state contact force. This eﬀect can be justiﬁed by noting that a step position input is equivalent to a properly ﬁltered
impulse force input.
The reference position along axis y0 is obviously reached by the arm tip according to dynamics of position control; the relative time history is not reported.

9.5 Constrained Motion
Force control schemes can be employed for the execution of a constrained
motion as long as they suitably take into account the geometric features of the
environment and the force and position references are chosen to be compatible
with those features.
A real manipulation task is characterized by complex contact situations
where some directions are subject to end-eﬀector pose constraints and others
are subject to interaction force constraints. During task execution, the nature
of constraints may vary substantially.
The need to handle complex contact situations requires the capacity to
specify and perform control of both end-eﬀector pose and contact force. However, a fundamental aspect to be considered is that it is not possible to impose
simultaneously arbitrary values of pose and force along each direction. Moreover, one should ensure that the reference trajectories for the control system
be compatible with the constraints imposed by the environment during task
execution.
For the above reasons, it is useful to have an analytic description of the
interaction forces, which is very demanding from a modelling point of view.
A real contact situation is a naturally distributed phenomenon in which
the local characteristics of the contact surfaces as well as the global dynamics
of the manipulator and environment are involved. In detail:
• The environment imposes kinematic constraints on the end-eﬀector motion, due to one or more contacts of diﬀerent type; reaction forces and
moments arise when the end-eﬀector tends to violate the constraints (e.g.,
the case of a robot sliding a rigid tool on a frictionless rigid surface).
• The end-eﬀector, while being subject to kinematic constraints, may also
exert dynamic forces and moments on the environment, in the presence
of environment dynamics (e.g., the case of a robot turning a crank, when
the crank dynamics is relevant, or a robot pushing against a compliant
surface).
• The contact force and moment may depend on the structural compliance
of the robot, due to the ﬁnite stiﬀness of the joints and links of the manipulator, as well as of the wrist force/torque sensor or of the tool (e.g. an
end-eﬀector mounted on an RCC device).

9.5 Constrained Motion

385

• Local deformations of the contact surfaces may occur during the interaction, producing distributed contact areas; moreover, static and dynamic
friction may be present in the case of non-ideally smooth contact surfaces.
The design of the interaction control is usually carried out under simplifying assumptions. The following two cases are considered:
• The robot and the environment are perfectly rigid and purely kinematics
constraints are imposed by the environment.
• The robot is perfectly rigid, all the compliance of the system is localized
in the environment and the contact force and moment is approximated by
a linear elastic model.
In both cases, frictionless contact is assumed. It is obvious that these situations are only ideal. However, the robustness of the control should be able to
cope with situations where some of the ideal assumptions are relaxed. In that
case the control laws may be adapted to deal with non-ideal characteristics.
9.5.1 Rigid Environment
The kinematic constraints imposed by the environment can be represented
by a set of algebraic equations that the variables describing the end-eﬀector
position and orientation must satisfy; since these variables depend on the joint
variables through the direct kinematic equations, the constraint equations can
also be expressed in the joint space as
ϕ(q) = 0.

(9.50)

Vector ϕ is an (m×1) function, with m < n, where n is the number of joints of
the manipulator, assumed to be nonredundant; without loss of generality, the
case n = 6 is considered. The constraints of the form (9.50), involving only the
generalized coordinates of the system, are known as holonomic constraints.
Computing the time derivative of (9.50) yields
J ϕ (q)q̇ = 0,

(9.51)

where J ϕ (q) = ∂ϕ/∂q is the (m × 6) Jacobian of ϕ(q), known as constraint
Jacobian. It is assumed that J ϕ is of rank m at least locally in a neighborhood
of the operating point; equivalently, the m constraint equations (9.50) are
assumed to be locally independent.
In the absence of friction, the interaction forces are reaction forces arising when the end-eﬀector tends to violate the constraints. These end-eﬀector
forces produce reaction torques at the joints that can be computed using the
principle of virtual work, taking into account that the work of the reaction
forces, by deﬁnition, should be null for all virtual displacements which satisfy
the constraints. Considering the expression (3.108) of the virtual work of the

386

9 Force Control

joint torques τ and that, in view of (9.51), the virtual displacement δq satisfy
the equation
J ϕ (q)δq = 0,
yields
τ = J Tϕ (q)λ,
where λ is a suitable (m × 1) vector. The corresponding forces applied to the
end-eﬀector are
he = J −T (q)τ = S f (q)λ,
(9.52)
assuming a nonsingular J , with
S f = J −T (q)J Tϕ (q).

(9.53)

Notice that Eq. (9.50) corresponds to a set of bilateral constraints. This
means that the reaction forces (9.52) act so that, during the motion, the
end-eﬀector always keeps contact with the environment, as for the case of a
gripper turning a crank. However, in many applications, the interaction with
the environment corresponds to unilateral constraints. For example, in the
case of a tool sliding on a surface, the reaction forces arise only when the
tool pushes against the surface and not when it tends to detach. However,
Eq. (9.52) can be still applied under the assumption that the end-eﬀector,
during the motion, never loses contact with the environment.
From (9.52) it follows that he belongs to the m-dimensional subspace
R(S f ). The inverse of the linear transformation (9.52) can be computed as
λ = S †f (q)he ,

(9.54)

where S †f denotes a weighted pseudo-inverse of matrix S f , namely
S †f = (S Tf W S f )−1 S Tf W ,

(9.55)

where W is a symmetric and positive deﬁnite weighting matrix.
Notice that, while subspace R(S f ) is uniquely deﬁned by the geometry
of the contact, matrix S f in (9.53) is not unique, because constraint equations (9.50) are not uniquely deﬁned. Moreover, in general, the physical dimensions of the elements of vector λ are not homogeneous and the columns of
matrix S f , as well as of matrix S †f , do not necessarily represent homogeneous
entities. This may produce invariance problems in the transformation (9.54) if
he represents a quantity that is subject to disturbances and, as a result, may
have components outside R(S f ). In particular, if a physical unit or a reference
frame is changed, matrix S f undergoes a transformation; however, the result
of (9.54) with the transformed pseudo-inverse, in general, depends on the
/ R(S f ),
adopted physical units or reference frame! The reason is that, if he ∈
the problem of computing λ from (9.52) does not have a solution. In this case,
Eq. (9.54) represents only an approximate solution which minimizes the norm

9.5 Constrained Motion

387

of vector he − S f (q)λ weighted by matrix W .3 It is evident that the invariance of the solution can be ensured only if, in the case that a physical unit or
a reference frame is changed, the weighting matrix is transformed accordingly.
In the ideal case he ∈ R(S f ), the computation of the inverse of (9.52) has a
unique solution, deﬁned by (9.54), regardless the weighting matrix; hence the
invariance problem does not occur.
In order to guarantee invariance, it is convenient choosing matrix S f so
that its columns represent linearly independent forces. This implies that (9.52)
gives he as a linear combination of forces and λ is a dimensionless vector.
Moreover, a physically consistent norm in the space of forces can be deﬁned
based on the quadratic form hTe Che , which has the meaning of an elastic
energy if C is a positive deﬁnite compliance matrix. Hence, the choice W = C
can be made for the weighting matrix and, if a physical unit or a reference
frame is changed, the transformations to be applied to matrices S f and W
can be easily found on the basis of their physical meaning.
Notice that, for a given S f , the constraint Jacobian can be computed
from (9.53) as J ϕ (q) = S Tf J (q); moreover, if necessary, the constraint equations can be derived by integrating (9.51).
By using (3.4), (9.53), equality (9.51) can be rewritten in the form
J ϕ (q)J −1 (q)J (q)q̇ = S Tf v e = 0,

(9.56)

which, by virtue of (9.52), is equivalent to
hTe v e = 0.

(9.57)

Equation (9.57) represents the kinetostatic relationship, known as reciprocity,
between the interaction force and moment he — belonging to the so-called
force controlled subspace — which coincides with R(S f ) and the end-eﬀector
linear and angular velocity v e — belonging to the so-called velocity controlled
subspace. The concept of reciprocity expresses the physical fact that, under
the assumption of rigid and frictionless contact, the forces do not produce
any work for all the end-eﬀector displacements which satisfy the constraints.
This concept is often confused with the concept of orthogonality, which is
meaningless in this case because velocities and forces are non-homogeneous
quantities belonging to diﬀerent vector spaces.
Equations (9.56), (9.57) imply that the dimension of the velocity controlled
subspace is 6−m whereas the dimension of the force controlled subspace is m;
moreover, a (6 × (6 − m)) matrix S v can be deﬁned, which satisﬁes equation
S Tf (q)S v (q) = O

(9.58)

and such that R(S v ) represents the velocity controlled subspace. Therefore:
v e = S v (q)ν,
3

(9.59)

See Sect. A.7 for the computation of an approximate solution based on the left
pseudo-inverse and Problem 9.5.

388

9 Force Control

where ν denotes a suitable ((6 − m) × 1) vector.
The inverse of the linear transformation (9.59) can be computed as
ν = S †v (q)v e ,

(9.60)

where S †v denotes a suitable weighted pseudo-inverse of matrix S v , computed
as in (9.55). Notice that, as for the case of S f , although the subspace R(S v )
is uniquely deﬁned, the choice of matrix S v itself is not unique. Moreover,
about Eq. (9.60), invariance problems analogous to that considered for the
case of (9.54) can be observed. In this case, it is convenient to select the matrix
S v so that its columns represent a set of independent velocities; moreover, for
the computation of the pseudo-inverse, a norm in the space of velocities can
be deﬁned based on the kinetic energy of a rigid body or on the elastic energy
expressed in terms of the stiﬀness matrix K = C −1 .
Matrix S v may also have an interpretation in terms of Jacobian. In fact,
due to the presence of m independent holonomic constraints (9.50), the conﬁguration of a manipulator in contact with the environment can be locally
described in terms of a ((6 − m) × 1) vector r of independent coordinates.
From the implicit function theorem, this vector can be deﬁned as
r = ψ(q),

(9.61)

where ψ(q) is any ((6 − m) × 1) vector function such that the m components
of φ(q) and the 6 − m components of ψ(q) are linearly independent at least
locally in a neighborhood of the operating point. This means that the mapping (9.61), together with the constraint equations (9.50), is locally invertible,
with inverse deﬁned as
q = ρ(r).
(9.62)
Equation (9.62) explicitly provides all the joint vectors q which satisfy the
constraint equations (9.50), for any r arbitrary selected in a neighborhood
of the operating point. Moreover, the vector q̇ that satisﬁes (9.51) can be
computed as
q̇ = J ρ (r)ṙ,
where J ρ (r) = ∂ρ/∂r is a (6 × (6 − m)) full rank Jacobian matrix. Also, the
following equality holds:
J ϕ (q)J ρ (r) = O,
which can be interpreted as a reciprocity condition between the subspace
R(J Tϕ ) of the joint torques τ corresponding to the reaction forces acting on
the end-eﬀector and the subspace R(J ρ ) of the joint velocities q̇ which satisfy
the constraints.
The above equation can be rewritten as
J ϕ (q)J −1 (q)J (q)J ρ (r) = O.

9.5 Constrained Motion

389

Hence, assuming that J is nonsingular and taking into account (9.53), (9.58),
matrix S v can be computed as
S v = J (q)J ρ (r).

(9.63)

The matrices S f , S v and the corresponding pseudo-inverse matrices S †f ,
S †v are known as selection matrices. These matrices play a fundamental role
for task speciﬁcation, since they can be used to assign the desired end-eﬀector
motion and the interaction forces and moments consistently with the constraints. Also, they are essential for control synthesis.
To this end, notice that the (6 × 6) matrix P f = S f S †f projects a generic
force vector he on the force controlled subspace R(S f ). Matrix P f is idempotent, namely P 2f = P f P f = P f , and therefore is a projection matrix .
Moreover, matrix (I 6 − P f ) projects force vector he on the orthogonal complement of the force controlled subspace; also, this matrix, being idempotent,
it is a projection matrix.
Similarly, it can be veriﬁed that the (6×6) matrices P v = S v S †v and (I 6 −
P v ) are projection matrices, projecting a generic linear and angular velocity
vector v e on the controlled velocity subspace R(S v ) and on its orthogonal
complement.
9.5.2 Compliant Environment
In many applications, the interaction forces between the end-eﬀector and a
compliant environment can be approximated by the ideal elastic model of the
form (9.21). If the stiﬀness matrix K is positive deﬁnite, this model corresponds to a fully constrained case and the environment deformation coincides
with the elementary end-eﬀector displacement. In general, however, the endeﬀector motion is only partially constrained by the environment and this situation can be modelled by introducing a suitable positive semi-deﬁnite stiﬀness
matrix.
This kind of situation, even for a simple case, has been already considered
in previous examples concerning the interaction with an elastically compliant
plane. In a general case, the stiﬀness matrix describing the partially constrained interaction can be computed by modelling the environment as a pair
of rigid bodies, S and R, connected through an ideal six-DOF spring, and
assuming that the end-eﬀector may slide on the external surface of body S.
Moreover, two reference frames are introduced, one attached to S and one
attached to R. At equilibrium, corresponding to the undeformed spring, the
end-eﬀector frame is assumed to be coincident with the frames attached to S
and R. The selection matrices S f and S v and the corresponding controlled
force and velocity subspaces can be identiﬁed on the basis of the geometry of
the contact between the end-eﬀector and the environment.

390

9 Force Control

Assumed frictionless contact, the interaction force applied by the endeﬀector on body S belongs to the force controlled subspace R(S f ) and thus
he = S f λ,

(9.64)

where λ is a (m × 1) vector. Due to the presence of the generalized spring, the
above force causes a deformation of the environment that can be computed
as
dxr,s = Che ,
(9.65)
where C is the compliance matrix of the spring between S and R, assumed
to be nonsingular. On the other hand, the elementary displacement of the
end-eﬀector with respect to the equilibrium pose can be decomposed as
dxr,e = dxv + dxf ,

(9.66)

dxv = P v dxr,e

(9.67)

where
is the component belonging to the velocity controlled subspace R(S v ), where
the end-eﬀector may slide on the environment, whereas
dxf = (I 6 − P v )dxr,e = (I 6 − P v )dxr,s

(9.68)

is the component corresponding to the deformation of the environment. Notice
that, in general, P v dxr,e = P v dxr,s .
Premultiplying both sides of (9.66) by matrix S Tf and using (9.67), (9.68),
(9.65), (9.64) yields
S Tf dxr,e = S Tf dxr,s = S Tf CS f λ,
where the equality S Tf P v = O has been taken into account. The above equation can be used to compute vector λ which, replaced into (9.64), yields

where

he = K  dxr,e ,

(9.69)

K  = S f (S Tf CS f )−1 S Tf

(9.70)

is the positive semi-deﬁnite stiﬀness matrix corresponding to the partially
constrained elastic interaction.
Expression (9.70) is not invertible. However, using Eqs. (9.68), (9.65), the
following equality can be derived:

where the matrix

dxf = C  he ,

(9.71)

C  = (I 6 − P v )C,

(9.72)

9.6 Natural and Artiﬁcial Constraints

391

of rank 6 − m, has the meaning of compliance matrix.
Notice that contact between the manipulator and the environment may be
compliant along some directions and rigid along other directions. Therefore,
the force control subspace can be decomposed into two distinct subspaces, one
corresponding to elastic forces and the other corresponding to reaction forces.
Matrices K  and C  should be modiﬁed accordingly.

9.6 Natural and Artiﬁcial Constraints
An interaction task can be assigned in terms of a desired end-eﬀector force hd
and velocity v d . In order to be consistent with the constraints, these vectors
must lie in the force and velocity controlled subspaces respectively. This can
be guaranteed by specifying vectors λd and ν d and computing hd and v d as
hd = S f λd ,

vd = S v ν d ,

where S f and S v have to be suitably deﬁned on the basis of the geometry
of the task. Therefore vectors λd and ν d will be termed ‘desired force’ and
‘desired velocity’ respectively.
For many robotic tasks it is possible to deﬁne an orthogonal reference
frame, eventually time-varying, where the constraints imposed by the environment can be easily identiﬁed, making task speciﬁcation direct and intuitive.
This reference frame Oc –xc yc zc is known as constraint frame.
Two DOFs correspond to each axis of the constraint frame: one associated with the linear velocity or to the force along the axis direction and the
other associated with the angular velocity and to the moment along the axis
direction.
For a given constraint frame, in the case of rigid environment and absence
of friction, it can be observed that:
• Along each DOF, the environment imposes to the manipulator’s endeﬀector either a velocity constraint — in the sense that it does not allow
translation along a direction or rotation about an axis — or a force constraint — in the sense that it does not allow the application of any force
along a direction or any torque about an axis; such constraints are termed
natural constraints since they are determined directly by task geometry.
• The manipulator can control only the variables which are not subject to
natural constraints; the reference values for those variables are termed
artiﬁcial constraints since they are imposed with regard to the strategy
for executing the given task.
Notice that the two sets of constraints are complementary, in that they
regard diﬀerent variables for each DOF. Also, they allow a complete speciﬁcation of the task, since they involve all variables.

392

9 Force Control

Fig. 9.12. Sliding of a prismatic object on a planar surface

In the case of compliant environment, for each DOF where interaction
occurs, one can choose the variable to control, namely force or velocity, as
long as the complementarity of the constraints is preserved. In case of high
stiﬀness, it is advisable to choose the force as an artiﬁcial constraint and the
velocity as a natural constraint, as for the case of rigid environment. Vice
versa, in the case of low stiﬀness, it is convenient to make the opposite choice.
Notice also that, in the presence of friction, forces and moments also arise
along the DOFs corresponding to force natural constraints.
9.6.1 Analysis of Tasks
To illustrate description of an interaction task in terms of natural and artiﬁcial
constraints as well as to emphasize the opportunity to use a constraint frame
for task speciﬁcation, in the following a number of typical case studies are
analyzed.
Sliding on a planar surface
The end-eﬀector manipulation task is the sliding of a prismatic object on
a planar surface. Task geometry suggests choosing the constraint frame as
attached to the contact plane with an axis orthogonal to the plane (Fig. 9.12).
Alternatively, the task frame can be chosen with the same orientation but
attached to the object.
Natural constraints can be determined ﬁrst, assuming rigid and frictionless contact. Velocity constraints describe the impossibility to generate a linear
velocity along axis zc and angular velocities along axes xc and yc . Force constraints describe the impossibility to exert forces along axes xc and yc and a
moment along axis zc .
The artiﬁcial constraints regard the variables not subject to natural constraints. Hence, with reference to the natural constraints of force along axes
xc , yc and moment along zc , it is possible to specify artiﬁcial constraints for
linear velocity along xc , yc and angular velocity along zc . Similarly, with reference to natural constraints of linear velocity along axis zc and angular velocity

9.6 Natural and Artiﬁcial Constraints

393

about axes xc and yc , it is possible to specify artiﬁcial constraints for force
along zc and moments about xc and yc . The set of constraints is summarized
in Table 9.1.
Table 9.1. Natural and artiﬁcial constraints for the task of Fig. 9.12
Natural
constraints
ȯcz
ωxc
ωyc
fxc
fyc
μcz

Artiﬁcial
constraints
fzc
μcx
μcy
ȯcx
ȯcy
ωzc

For this task, the dimension of the force controlled subspace is m = 3,
while the dimension of the velocity controlled subspace is 6−m = 3. Moreover,
matrices S f and S v can be chosen as
⎡

0
⎢0
⎢
⎢1
Sf = ⎢
⎢0
⎣
0
0

0
0
0
1
0
0

⎤
0
0⎥
⎥
0⎥
⎥
0⎥
⎦
1
0

⎡

1
⎢0
⎢
⎢0
Sv = ⎢
⎢0
⎣
0
0

0
1
0
0
0
0

⎤
0
0⎥
⎥
0⎥
⎥.
0⎥
⎦
0
1

Notice that, if the constraint frame is chosen attached to the contact plane,
matrices S f and S v remain constant if referred to the base frame but are
time-varying if referred to the end-eﬀector frame. Vice versa, if the constraint
frame is chosen attached to the object, such matrices are constant if referred
to the end-eﬀector frame and time-varying if referred to the base frame.
In the presence of friction, non-null force and moment may also arise along
the velocity controlled DOFs.
In the case of compliant plane, elastic forces and torques may be applied
along the axis zc and about the axes xc and yc respectively, corresponding to
end-eﬀector displacements along the same DOFs. On the basis of the expressions derived for S f and S v , the elements of the stiﬀness matrix K  corresponding to the partially constrained interaction are null with the exception
of those of the (3 × 3) block K m obtained selecting the rows 3, 4 and 5 of K  .
This block matrix can be computed as
⎤−1
⎡
c3,3 c3,4 c3,5
K m = ⎣ c4,3 c4,4 c4,5 ⎦ ,
c5,3 c5,4 c5,5
where ci,j = cj,i are the elements of the (6 × 6) compliant matrix C.

394

9 Force Control

Fig. 9.13. Insertion of a cylindrical peg in a hole

Peg-in-hole
The end-eﬀector manipulation task is the insertion of a cylindrical object (peg)
in a hole. Task geometry suggests choosing the constraint frame with the origin
in the centre of the hole and an axis parallel to the hole axis (Fig. 9.13). This
frame can be chosen attached either to the peg or to the hole.
The natural constraints are determined by observing that it is not possible
to generate arbitrary linear and angular velocities along axes xc , yc , nor is it
possible to exert arbitrary force and moment along zc . As a consequence, the
artiﬁcial constraints can be used to specify forces and moments along xc and
yc , as well as linear and angular velocity along zc . Table 9.2 summarizes the
constraints.
Table 9.2. Natural and artiﬁcial constraints for the task of Fig. 9.13
Natural
constraints
ȯcx
ȯcy
ωxc
ωyc
fzc
μcz

Artiﬁcial
constraints
fxc
fyc
μcx
μcy
ȯcz
ωzc

Among the variables subject to artiﬁcial constraints, ȯcz = 0 describes
insertion while the others are typically null to eﬀectively perform the task.

9.6 Natural and Artiﬁcial Constraints

395

Fig. 9.14. Turning a crank

For this task, the dimension of the force controlled subspace is m = 4,
while the dimension of the velocity controlled subspace is 6−m = 2. Moreover,
matrices S f and S v can be expressed as
⎡

1
⎢0
⎢
⎢0
Sf = ⎢
⎢0
⎣
0
0

0
1
0
0
0
0

0
0
0
1
0
0

⎤
0
0⎥
⎥
0⎥
⎥
0⎥
⎦
1
0

⎡

0
⎢0
⎢
⎢1
Sv = ⎢
⎢0
⎣
0
0

⎤
0
0⎥
⎥
0⎥
⎥.
0⎥
⎦
0
1

Notice that, if the constraint frame is chosen attached to the hole, matrices S f
and S v remain constant if referred to the base frame but are time-varying if
referred to the end-eﬀector frame. Vice versa, if the constraint frame is chosen
attached to the peg, such matrices are constant if referred to the end-eﬀector
frame and time-varying if referred to the base frame.
Turning a crank
The end-eﬀector manipulation task is the turning of a crank. Task geometry
suggests choosing the constraint frame with an axis aligned with the axis of
the idle handle and another axis aligned with the crank lever (Fig. 9.14).
Notice that in this case the constraint frame is time-varying.
The natural constraints do not allow the generation of arbitrary linear
velocities along xc , zc and angular velocities along xc , yc , nor arbitrary force
along yc and moment along zc . As a consequence, the artiﬁcial constraints
allow the speciﬁcation of forces along xc , zc and moments along xc , yc , as well
as a linear velocity along yc and an angular velocity along zc . The situation
is summarized in Table 9.3.
Among the variables subject to artiﬁcial constraints, forces and moments
are typically null for task execution.

396

9 Force Control
Table 9.3. Natural and artiﬁcial constraints for task in Fig. 9.14

Natural
constraints
ȯcx
ȯcz
ωxc
ωyc
fyc
μcz

Artiﬁcial
constraints
fxc
fzc
μcx
μcy
ȯcy
ωzc

For this task, the dimension of the force controlled subspace is m = 4,
while the dimension of the velocity controlled subspace is 6−m = 2. Moreover,
matrices S f and S v can be expressed as
⎡

1
⎢0
⎢
⎢0
Sf = ⎢
⎢0
⎣
0
0

0
0
1
0
0
0

0
0
0
1
0
0

⎤
0
0⎥
⎥
0⎥
⎥
0⎥
⎦
1
0

⎡

0
⎢1
⎢
⎢0
Sv = ⎢
⎢0
⎣
0
0

⎤
0
0⎥
⎥
0⎥
⎥,
0⎥
⎦
0
1

These matrices are constant in the constraint frame but are time-varying if
referred to the base frame or to the end-eﬀector frame, because the constraint
frame moves with respect to both these frames during task execution.

9.7 Hybrid Force/Motion Control
Description of an interaction task between manipulator and environment in
terms of natural constraints and artiﬁcial constraints, expressed with reference to the constraint frame, suggests a control structure that utilizes the
artiﬁcial constraints to specify the objectives of the control system so that
desired values can be imposed only onto those variables not subject to natural constraints. In fact, the control action should not aﬀect those variables
constrained by the environment so as to avoid conﬂicts between control and
interaction with environment that may lead to an improper system behaviour.
Such a control structure is termed hybrid force/motion control , since deﬁnition
of artiﬁcial constraints involves both force and position or velocity variables.
For the design of hybrid control, it is useful rewriting the dynamic model
of the manipulator with respect to the end-eﬀector acceleration
v̇ e = J (q)q̈ + J̇ (q)q̇.
In particular, replacing (7.127) in the above expression yields
B e (q)v̇ e + ne (q, q̇) = γ e − he ,

(9.73)

9.7 Hybrid Force/Motion Control

397

where
B e = J −T BJ −1
ne = J −T (C q̇ + g) − B e J̇ q̇.
In the following, hybrid force/motion control is presented ﬁrst for the case
of compliant environment and then for the case of rigid environment.
9.7.1 Compliant Environment
In the case of compliant environment, on the basis of the decomposition (9.66)
and of Eqs. (9.67), (9.71), (9.64), the following expression can be found
dxr,e = P v dxr,e + C  S f λ.
Computing the elementary displacements in terms of velocity, in view of (9.59)
and taking into account that frame r is motionless, the end-eﬀector velocity
can be decomposed as
(9.74)
v e = S v ν + C  S f λ̇,
where the ﬁrst term belongs to the velocity control subspace and the second
term belongs to its orthogonal complement. All the quantities are assumed
to be referred to a common reference frame which, for simplicity, was not
speciﬁed.
In the following, the base frame is chosen as the common reference frame;
moreover, the contact geometry and the compliance matrix are assumed to be

constant, namely Ṡ v = O, Ṡ f = O and Ċ = O. Therefore, computing the
time derivative of (9.74) yields the following decomposition for the end-eﬀector
acceleration:
v̇ e = S v ν̇ + C  S f λ̈.
(9.75)
By adopting the inverse dynamics control law
γ e = B e (q)α + ne (q, q̇) + he ,
where α is a new control input, in view of (9.73), the closed-loop equation is
v̇ e = α.

(9.76)

On the basis of the decomposition (9.75), with the choice
α = S v αν + C  S f f λ ,

(9.77)

a complete decoupling between force control and velocity control can be
achieved. In fact, replacing (9.75) and (9.77) into (9.76) and premultiplying
both sides of the resulting equation once by S †v and once by S Tf , the following
equalities are obtained:
ν̇ = αν

(9.78)

λ̈ = f λ .

(9.79)

398

9 Force Control

Fig. 9.15. Block scheme of a hybrid force/motion control for a compliant environment

Therefore, the task can be assigned specifying a desired force, in terms
of vector λd (t), and a desired velocity, in terms of vector ν d (t). This control
scheme is referred to as hybrid force/velocity control .
The desired velocity ν d can be achieved using the control law
" t
αν = ν̇ d + K P ν (ν d − ν) + K Iν (ν d (ς) − ν(ς))dς,
(9.80)
0

where K P ν and K Iν are positive deﬁnite matrices. Vector ν can be computed
using (9.60), where the linear and angular velocity of the end-eﬀector v e is
computed from joint position and velocity measurements.
The desired force λd can be achieved using the control law
f λ = λ̈d + K Dλ (λ̇d − λ̇) + K P λ (λd − λ),

(9.81)

where K Dλ and K P λ are positive deﬁnite matrices. The implementation of
the above control law requires the computation of vector λ via (9.54), using
the measurements of end-eﬀector forces and moments he . Also, λ̇ can be
computed as
λ̇ = S †f ḣe
in the ideal case that ḣe is available.
The block scheme of a hybrid force/motion control law is shown in
Fig. 9.15. The output variables are assumed to be the vector of end-eﬀector
forces and moments he and the vector of end-eﬀector linear and angular velocities v e .

9.7 Hybrid Force/Motion Control

399

Since force measurements are often noisy, the use of ḣe is not feasible.
Hence, the feedback of λ̇ is often replaced by
λ̇ = S †f K  J (q)q̇,

(9.82)

where K  is the positive semi-deﬁnite stiﬀness matrix (9.70).
If the contact geometry is known, but only an estimate of the stiﬀness/compliance of the environment is available, control law (9.77) can be
rewritten in the form
) Sf f λ,
α = S v αν + C
) and C
) is an estimate of C.
)  = (I 6 − P v )C
where C
In this case, Eq. (9.78) still holds while, in place of (9.79), the following
equality can be derived:
λ̈ = Lf f λ
) f is a nonsingular matrix. This implies that
where Lf = (S Tf CS f )−1 S Tf CS
the force and velocity control subspaces remain decoupled and thus velocity
control law (9.80) does not need to be modiﬁed.
Since matrix Lf is unknown, it is not possible to achieve the same performance of the force control as in the previous case. Also, if vector λ̇ is computed
starting from velocity measurements using (9.82) with an estimate of K  , only
)˙ is available that, in view of (9.82), (9.70), can be expressed in
an estimate λ
the form
)˙ = (S T CS
) f )−1 S T J (q)q̇.
λ
f
f
Replacing (9.74) in the above equation and using (9.72) yields
)˙ = L−1 λ̇.
λ
f

(9.83)

)˙ + K P λ (λd − λ),
f λ = −kDλ λ

(9.84)

Considering the control law

with a constant λd , the dynamics of the closed-loop system is
λ̈ + kDλ λ̇ + Lf K P λ λ = Lf K P λ λd ,
where expression (9.83) has been used. This equation shows that the equilibrium solution λ = λd is also asymptotically stable in the presence of an
uncertain matrix Lf , with a suitable choice of gain kDλ and of matrix K P λ .

Example 9.4
Consider a two-link planar arm in contact with a purely frictionless elastic plane;
unlike the above examples, the plane is at an angle of π/4 with axis x0 (Fig. 9.16).

400

9 Force Control

Fig. 9.16. Characterization of constraint frame for a two-link planar arm in contact
with an elastically compliant plane

The natural choice of the constraint frame is that with axis xc along the plane and
axis yc orthogonal to the plane; the task is obviously characterized by two DOFs.
For the computation of the analytic model of the contact force, reference frames s
and r are chosen so that, in the case of null force, they coincide with the constraint
frame. In the presence of interaction, frame r remains attached to the rest position
of the plane while frame s remains attached to the contact plane in the deformed
position; the constraint frame is assumed to be attached to frame s. Matrices S cf
and S cv , referred to the constraint frame, have the form

 

S cf =

0
1

 

and the corresponding projection matrices are



P cf =

0
0

0
1

1
,
0

S cv =





P cv =

1
0



0
.
0

In view of (9.70), (9.72), the stiﬀness and the compliance matrices, referred to
the constraint frame, have the expression



K

c

=

0
0

0

c−1
2,2





C

c

=

0
0

0
c2,2



,

where c2,2 characterizes the compliance of frame s with respect to frame r along the
direction orthogonal to the plane, aligned to axis yc of the constraint frame.
It is evident that, under the assumption that the plane is compliant only along
the orthogonal direction and that this direction remains ﬁxed, then the constraint
frame orientation remains constant with respect to the base frame. The corresponding rotation matrix is given by
 √
√ 
1/√2 −1/√ 2
.
(9.85)
Rc =
1/ 2 1/ 2
Moreover, if the task is to slide the manipulator tip along the plane, the end-eﬀector
velocity can be decomposed according to (9.74) in the form
v ce = S cv ν + C c S cf λ̇,

(9.86)

9.7 Hybrid Force/Motion Control

401

where all the quantities are referred to the constraint frame. It can be easily shown
that, if f ce = [ fxc fyc ]T and v ce = [ ȯcx ȯcy ]T , it is ν = ȯcx and λ = fyc . This equation
can also be referred to the base frame, where matrices

 √ 
√ 
−1/√ 2
1/√2
c
c
S v = Rc S v =
,
S f = Rc S f =
1/ 2
1/ 2
are constant and the compliance matrix
C  = Rc C c RTc = c2,2



1/2
−1/2

−1/2
1/2



is constant during the end-eﬀector motion on the plane, for constant c2,2 . The adoption of an inverse dynamics control law, with the choice (9.77), leads to
ν̇ = öcx = αν
λ̈ = f¨yc = fλ ,
showing that hybrid control achieves motion control along axis xc and force control along axis yc , provided that αν and fλ are set according to (9.80) and (9.81)
respectively.
Finally, notice that the formulation of the control law can be further simpliﬁed
if the base frame is chosen parallel to the constraint frame.

9.7.2 Rigid Environment
In the case of rigid environment, the interaction force and moment can be
written in the form he = S f λ. Vector λ can be eliminated from (9.73) by
solving (9.73) for v̇ e and substituting it into the time derivative of the equality (9.56). This yields


T
,
(9.87)
(q)(γ
−
n
(q,
q̇))
+
Ṡ
v
λ = B f (q) S Tf B −1
e
e
e
f
e
−1
where B f = (S Tf B −1
.
e Sf )
Hence, the dynamic model (9.73) for the manipulator constrained by the
rigid environment can be rewritten in the form
T

B e (q)v̇ e + S f B f (q)Ṡ f v e = P (q)(γ e − ne (q, q̇)),

(9.88)

with P = I 6 − S f B f S Tf B −1
e . Notice that P S f = O; moreover, this matrix
is idempotent. Therefore, matrix P is a (6 × 6) projection matrix that ﬁlters
out all the components of the end-eﬀector forces lying in the subspace R(S f ).
Equation (9.87) reveals that vector λ instantaneously depends on the control force γ e . Hence, by suitably choosing γ e , it is possible to control directly
the m independent components of the end-eﬀector forces that tend to violate
the constraints; these components can be computed from λ, using (9.52).

402

9 Force Control

On the other hand, (9.88) represents a set of six second order diﬀerential equations whose solution, if initialized on the constraints, automatically
satisﬁes Eq. (9.50) at all times.
The reduced-order dynamic model of the constrained system is described
by 6 − m independent equations that are obtained premultiplying both sides
of (9.88) by matrix S Tv and substituting the acceleration v̇ e with
v̇ e = S v ν̇ + Ṡ v ν.
Using the identities (9.58) and S Tv P = S Tv yields


B v (q)ν̇ = S Tv γ e − ne (q, q̇) − B e (q)Ṡ v ν ,

(9.89)

where B v = S Tv B e S v . Moreover, expression (9.87) can be rewritten as


λ = B f (q)S Tf B −1
(q)
γ
−
n
(q,
q̇)
−
B
(q)
Ṡ
ν
,
(9.90)
e
e
v
e
e
T

where the identity Ṡ f S v = −S Tf Ṡ v has been exploited.
With reference to (9.89), consider the choice
γ e = B e (q)S v αv + S f f λ + ne (q, q̇) + B e (q)Ṡ v ν,

(9.91)

where αv and f λ are new control inputs. By replacing (9.91) in (9.89), (9.90),
the following two equations can be found:
ν̇ = αν
λ = f λ,
showing that the inverse dynamics control law (9.91) allows a complete decoupling between force and velocity controlled subspaces.
It is worth noticing that, for the implementation of control law (9.91),
constraint equations (9.50) as well as Eq. (9.61) deﬁning the vector of the
conﬁguration variables for the constrained system are not required, provided
that matrices S f and S v are known. These matrices can be computed on the
basis of the geometry of the environment or estimated on-line, using force and
velocity measurements.
The task can easily be assigned by specifying a desired force, in terms
of vector λd (t), and a desired velocity, in terms of vector ν d (t); the resulting
scheme of hybrid force/velocity control is conceptually analogous to that shown
in Fig. 9.15.
The desired velocity ν d can be achieved by setting αν according to (9.80),
as for the case of compliant environment.
The desired force λd can be achieved by setting
f λ = λd ,

(9.92)

Bibliography

403

but this choice is very sensitive to disturbance forces, since it contains no force
feedback. Alternative choices are

or

f λ = λd + K P λ (λd − λ),

(9.93)

" t
f λ = λd + K Iλ (λd (ς) − λ(ς))dς,

(9.94)

0

where K P λ and K Iλ are suitable positive deﬁnite matrices. The proportional
feedback is able to reduce the force error due to disturbance forces, while the
integral action is able to compensate for constant bias disturbances.
The implementation of force feedback requires the computation of vector
λ from the measurement of the end-eﬀector force and moment he , that can
be achieved using (9.54).
When Eqs. (9.50) and (9.61) are available, matrices S f and S v can be
computed according to (9.53) and (9.63), respectively. Moreover, a hybrid
force/position control can be designed specifying a desired force λd (t), and a
desired position r d (t).
The force control law can be designed as above, while the desired position
r d can be achieved with the choice (see Problem 9.11)
αν = r̈ d + K Dr (ṙ d − ν) + K P r (r d − r),

(9.95)

where K Dr and K P r are suitable positive deﬁnite matrices. Vector r can be
computed from the joint position measurements using (9.61).

Bibliography
Scientiﬁc publications on force control are numerous and cover a time period
of about 30 years. Review papers are [243] for the ﬁrst decade, and [63] for
the second decade. Recent monographs on this subject are [90, 209].
Control based on the concept of compliance was originally proposed
by [165] in the joint space and [190] in the Cartesian space. The Remote
Centre of Compliance concept is presented in [55] and its use for assembling
operation is discussed in [242]. A reference paper for modelling of six-DOF
elastic systems is [136] and their properties are analyzed in [177, 74]. The idea
of impedance control was presented in [95] and a similar formulation can be
found in [105]. Various schemes of impedance based on diﬀerent representations of the orientation error are presented in [31, 32] and a rigorous analysis
can be found in [223].
Early works on force control are described in [241]. Approaches not requiring the exact knowledge of the environment model are force control with
position feedforward [65] and parallel force/position control [40, 43].

404

9 Force Control

Fig. 9.17. Driving a screw in a hole

Natural and artiﬁcial constraints were introduced in [150] and further developed in [64, 27]. The concept of reciprocity of forces and velocity is discussed in [133], while invariance problems are analyzed in [66]. Models of
elastic systems with semi-deﬁnite stiﬀness and compliance matrices are presented in [176]. The concept of hybrid force/motion control was introduced
in [184] and the explicit inclusion of the manipulator dynamic model is presented in [114]. In [57] a systematic approach to modelling and control of
interaction in the case of dynamic environment is introduced. Hybrid control
in the presence of constraints in the Cartesian space is presented in [247, 249],
while in [152] the constraints are formulated in the joint space. The use of
impedance control in a hybrid framework is discussed in [5].
Adaptive versions of force/motion control schemes are proposed in [235].
The case of complex contact situations and time-varying constraints is presented in [28]. In [228, 62] the issues of controlling contact transitions is discussed, in order to overcome the instability problems evidenced in [70].

Problems
9.1. Derive expressions (9.10), (9.11).
9.2. Show that the equilibrium equations for the compliance control scheme
are expressed by (9.22), (9.23).
9.3. Consider the planar arm in contact with the elastically compliant plane
in Fig. 9.16. The plane forms an angle of π/4 with axis x0 and its undeformed
position intersects axis x0 in the point of coordinates (1, 0); the environment
stiﬀness along axis yc is 5 · 103 N/m. With the data of the arm in Sect. 8.7,
design an impedance control. Perform a computer simulation of the interaction
of the controlled manipulator along the rectilinear path from position pi =

Problems

405

√
√
[ 1 + 0.1 2 0 ]T to pf = [ 1.2 + 0.1 2 0.2 ]T with a trapezoidal velocity
proﬁle and a trajectory duration tf = 1 s. Implement the control in discretetime with a sampling time of 1 ms.
9.4. Show that the equilibrium position for the parallel force/position control
scheme satisﬁes (9.49).
9.5. Show that expression (9.54) with (9.55) is the solution which minimizes
the norm he − S f (q)λ with weighting matrix W .
9.6. Show that stiﬀness matrix (9.70) can be expressed in the form K  =
P f K.
9.7. For the manipulation task of driving a screw in a hole illustrated in
Fig. 9.17, ﬁnd the natural constraints and artiﬁcial constraints with respect
to a suitably chosen constraint frame.
9.8. Show that the hybrid control scheme of Example 9.4, in the force controlled subspace, is equivalent to a force control scheme with inner velocity
loop.
9.9. For the arm and environment of Example 9.4 compute the expressions
of S †f and S †v in the constraint frame and in the base frame.
9.10. For the arm and environment of Problem 9.3, design a hybrid control
in which a motion control law operates along axis xc while a force control law
operates along axis yc ; let the desired contact force along axis yc be 50 N. Perform a computer simulation of the interaction of the controlled manipulator
along a trajectory on the plane equivalent to that of Problem 9.3. Implement
the control in discrete-time with a sampling time of 1 ms.
9.11. Show that control law (9.95) ensures tracking of a desired position r d (t).

10
Visual Servoing

Vision allows a robotic system to obtain geometrical and qualitative information on the surrounding environment to be used both for motion planning
and control. In particular, control based on feedback of visual measurements is
termed visual servoing. In the ﬁrst part of this chapter, some basic algorithms
for image processing, aimed at extracting numerical information referred to as
image feature parameters, are presented. These parameters, relative to images
of objects present in the scene observed by a camera, can be used to estimate
the pose of the camera with respect to the objects and vice versa. To this
end, analytic pose estimation methods, based on the measurement of a certain number of points or correspondences are presented. Also, numerical pose
estimation methods, based on the integration of the linear mapping between
the camera velocity in the operational space and the time derivative of the feature parameters in the image plane, are introduced. In cases in which multiple
images of the same scene, taken from diﬀerent viewpoints, are available, additional information can be obtained using stereo vision techniques and epipolar
geometry. A fundamental operation is also camera calibration; to this end, a
calibration method based on the measurement of a certain number of correspondences is presented. Then, the two main approaches to visual servoing
are introduced, namely position-based visual servoing and image-based visual
servoing, as well as a scheme, termed hybrid visual servoing, which combines
the beneﬁts of both approaches.

10.1 Vision for Control
Vision plays a key role in a robotic system, as it can be used to obtain geometrical and qualitative information on the environment where the robot
operates, without physical interaction. Such information may be employed by
the control system at diﬀerent levels, for the sole task planning and also for
feedback control.

408

10 Visual Servoing

As an example, consider the case of a robot manipulator, equipped with a
camera, which has to grasp an object using a gripper. Through vision the robot
may acquire information capable of identifying the relative pose of the object
with respect to the gripper. This information allows the control system to plan
a trajectory leading the manipulator in an appropriate grasping conﬁguration,
computed on the basis of the pose and of the shape of the object, from which
the closure of the gripper can be commanded.
The planned trajectory can be executed using a simple motion controller.
In this approach, termed look-and-move, visual measurements are used in open
loop, making the system very sensitive to uncertainties due, for instance, to
poor positioning accuracy of the manipulator or to the fact that the object
may have moved while the gripper reaches the grasp position.
On the other hand, in vision-based control or visual servoing, the visual
measurements are fed back to the control to compute an appropriate error
vector deﬁned between the current pose of the object and the pose of the
manipulator’s end-eﬀector.
A key characteristic of visual servoing, compared to motion and force control, is the fact that the controlled variables are not directly measured by the
sensor, but are obtained from the measured quantities through complex elaborations, based on algorithms of image processing and computational vision.
In Sect. 5.4.3 it was shown that a monochrome camera simply provides
a two-dimensional matrix of values of light intensity. From this matrix, the
so-called image feature parameters are to be extracted in real time. The geometric relationships between one or more two-dimensional views of a scene
and the corresponding 3D space are the basis of techniques of pose estimation
of objects in the manipulator workspace or of the end-eﬀector with respect
to the surrounding objects. In this regard, of fundamental importance is the
operation of camera calibration, which is necessary for calculating the intrinsic parameters, relating the quantities measured in the image plane to those
referred to the camera frame, and the extrinsic parameters, relating the latter
to quantities deﬁned in a frame attached to the manipulator.
The vision-based control schemes can be divided into two categories,
namely, those that realize visual servoing in operational space, also termed
position-based visual servoing, and those that realize visual servoing in the
image space, also known as image-based visual servoing. The main diﬀerence
lies in the fact that the schemes of the ﬁrst category use visual measurements
to reconstruct the relative pose of the object with respect to the robot, or vice
versa, while the schemes of the second category are based on the comparison
of the feature parameters of the image of the object between the current and
the desired pose. There are also schemes combining characteristics common
to both categories, that can be classiﬁed as hybrid visual servoing.
Another aspect to be considered for vision-based control is the type of
camera (colour or monochrome, resolution, ﬁxed or variable focal length, CCD
or CMOS technology). In this chapter, only the case of monochrome cameras
with ﬁxed focal length will be considered.

10.1 Vision for Control

409

Equally important is the choice of the number of cameras composing the
visual system and their location; this issue is brieﬂy discussed in the following.
10.1.1 Conﬁguration of the Visual System
A visual system may consist of only one camera, or two or more cameras. If
more cameras are used to observe the same object of a scene, it is possible to
retrieve information about its depth by evaluating its distance with respect to
the visual system. This situation is referred to as 3D vision or stereo vision,
where the term stereo derives from the Greek and means solid. The human
capability of perceiving objects in three dimensions relies on the fact that the
brain receives the same images from two eyes, observing the same scene from
slightly diﬀerent angles.
It is clear that 3D vision can be achieved even with one camera, provided
that two images of the same object, taken from two diﬀerent poses, are available. If only a single image is available, the depth can be estimated on the basis of certain geometrical characteristics of the object known in advance. This
means that, in many applications, mono-camera systems are often preferred
to multi-camera systems, because they are cheaper and easier to calibrate,
although characterized by lower accuracy.
Another feature that distinguishes visual systems for robot manipulators
is the placement of cameras. For mono-camera systems there are two options:
the ﬁxed conﬁguration, often referred to as eye-to-hand , where the camera is
mounted in a ﬁxed location, and the mobile conﬁguration, or eye-in-hand , with
the camera attached to the robot. For multi-camera systems, in addition to the
mentioned solutions, it is also possible to consider the hybrid conﬁguration,
consisting of one or more cameras in eye-to-hand conﬁguration, and one or
more cameras in eye-in-hand conﬁguration.
In the eye-to-hand conﬁguration, the visual system observes the objects to
be manipulated by a ﬁxed pose with respect to the base frame of the manipulator. The advantage is that the camera ﬁeld of view does not change during
the execution of the task, implying that the accuracy of such measurements
is, in principle, constant. However, in certain applications, such as assembly,
it is diﬃcult to prevent that the manipulator, moving in the camera ﬁeld of
view, occludes, in part or in whole, the view of the objects.
In the eye-in-hand conﬁguration, the camera is placed on the manipulator and can be mounted both before and after the wrist. In the ﬁrst case,
the camera can observe the end-eﬀector by a favourable pose and without
occlusions caused by the manipulator arm; in the latter case, the camera is
attached to the end-eﬀector and typically observes only the object. In both
situations, the camera ﬁeld of view changes signiﬁcantly during the motion
and this produces a high variability in the accuracy of measurements. However, when the end-eﬀector is close to the object, the accuracy becomes almost
constant and is usually higher than that achievable with eye-to-hand cameras,
with the advantage that occlusions are virtually absent.

410

10 Visual Servoing

Finally, hybrid conﬁguration combines the beneﬁts of the other two conﬁgurations, namely, ensures a good accuracy throughout the workspace, while
avoiding the problems of occlusions.
A separate category is represented by robotic heads, which are typically
equipped with a stereo vision system consisting of two cameras mounted on
motorized mechanisms that allow for yaw motion, or pan, and pitch motion,
or tilt, hence the name of pan-tilt cameras.
In this chapter, only schemes based on a single eye-in-hand camera will
be considered. The extension of the algorithms to the case of a eye-to-hand
camera, or to the case of multiple cameras, requires only minor modiﬁcations.

10.2 Image Processing
Visual information, unlike the information provided by other types of sensors,
is very rich and varied and thus requires complex and computational expensive transformations before it can be used for controlling a robotic system.
The objective of these transformations is the extraction of numerical information from the image, which provides a synthetic and robust description
of the objects of interest in the scene, through the so-called image feature
parameters.
To this end, two basic operations are required. The ﬁrst is so-called segmentation, which aims at obtaining a representation suitable for the identiﬁcation
of measurable features of the image. The subsequent operation, termed interpretation is concerned with the measurement of the feature parameters of the
image.
The source information is contained in a framestore, namely the twodimensional memory array representing the spatial sample of the image. On
the set of pixels the so-called image function is deﬁned which, in general,
is a vector function whose components represent the values of one or more
physical quantities related to the pixel, in a sampled and quantized form.
For example, in the case of color images, the image function deﬁned on
a pixel of coordinates (XI , YI ) has three components Ir (XI , YI ), Ig (XI , YI )
and Ib (XI , YI ), corresponding to the light intensity in the wavelengths of red,
green and blue. For a monocrome black-and-white image, the image function is
scalar and coincides with the light intensity in shades of gray I(XI , YI ), also
referred to as gray level . In the following, for simplicity, only monochrome
images will be considered.
The number of gray levels depends on the adopted grey-scale resolution.
In all cases, the gray scale is bounded by two gray levels, black and white,
corresponding to the minimum and maximum measurable light intensity respectively. Most current acquisition equipments adopt a scale consisting of
256 gray levels, that can be represented by a single byte of memory.

10.2 Image Processing

411

4

3

x 10

2

1

0
0

50

100

150

200

250

Fig. 10.1. Black-and-white image and corresponding gray-level histogram on the
right

A representation of the framestore which is particularly useful for subsequent processing is the gray-level histogram, which provides the frequency of
occurrence of each gray level in the image.
Where the gray levels are quantized from 0 to 255, the value h(p) of the
histogram at a particular gray level p ∈ [0, 255] is the number of image pixels
with gray level p. If this value is divided by the total number of pixels, the
histogram is termed normalized histogram.
Figure 10.1 shows a black-and-white image and the corresponding graylevel histogram. Proceeding from left to right, three main peaks can be observed — from left to right — corresponding to the darkest object, the lightest
object, and the background.
10.2.1 Image Segmentation
Segmentation consists of a grouping process, by which the image is divided
into a certain number of groups, referred to as segments, so that the component of each group are similar with respect to one or more characteristics.
Typically, distinct segments of the image correspond to distinct objects of the
environment, or homogeneous object parts.
There are two complementary approaches to the problem of image segmentation: one is based on ﬁnding connected regions of the image, the other
is concerned with detection of boundaries. The objective of region-based segmentation is that of grouping sets of pixels sharing common features into
two-dimensional connected areas, with the implicit assumption that the resulting regions correspond to real-world surfaces or objects. On the other
hand, boundary-based segmentation is aimed at identifying the pixels corresponding to object contours and isolating them from the rest of the image.
The boundary of an object, once extracted, can be used to deﬁne the position
and shape of the object itself.

412

10 Visual Servoing

The complementarity of the two approaches relies on the fact that a boundary can be achieved by isolating the contours of a region and, conversely, a
region can be achieved simply by considering the set of pixels contained within
a closed boundary.
The problem of segmentation is not trivial and there exist many solutions,
some of which are sketched below. From the point of view of memory usage,
boundary-based segmentation is more convenient, since boundaries contain
a reduced number of pixels. However, from the computational load point of
view, region-based segmentation is faster because it requires a reduced number
of memory accesses.
Region-based segmentation
The central idea underlying region-based segmentation techniques is that of
obtaining connected regions by continuous merging of initially small groups
of adjacent pixels into larger ones.
Merging of two adjacent regions may happen only if the pixels belonging to
these regions satisfy a common property, termed uniformity predicate. Often
the uniformity predicate requires that the gray level of the pixels of the region
belongs to a given interval.
In many applications of practical interest a thresholding approach is
adopted and a light intensity scale composed of only two values (0 and 1)
is considered. This operation is referred to as binary segmentation or image
binarization, and corresponds to separating one or more objects present in
the image from the background by comparing the gray level of each pixel
with a threshold l. For light objects against a dark background, all the pixels
whose gray level is greater than the threshold are considered to belong to a
set So , corresponding to the objects, while all the other pixels are considered
to belong to a set Sb corresponding to the background. It is obvious that this
operation can be reversed for dark objects against a light background. When
only an object is present in the image, the segmentation ends with the detection of sets So and Sb , representing two regions; in the presence of multiple
objects, a further elaboration is required to separate the connected regions
corresponding to the single objects. The image obtained assigning a light intensity equal to 0 to all the pixels of set So , and a light intensity equal to 1
to all the pixels of set Sb , or vice versa, is termed binary image.
A crucial factor for the success of binary segmentation is the choice of the
threshold. A widely adopted method for selecting the threshold is based on
the gray-level histogram, under the assumption that it contains clearly distinguishable minimum and maximum values, corresponding to the gray levels
of the objects and of the background; the peaks of the histogram are also
termed modes. For dark objects against a light background, the background
corresponds to the mode which is located further to the right — as, for example, in the case of Fig. 10.1 — and the threshold can be chosen at the
closest minimum to the left. For light objets against a dark background, the

10.2 Image Processing

413

Fig. 10.2. Binary image corresponding to image of Fig. 10.1

background corresponds to the mode which is located further to the left and
the threshold should be selected accordingly. With reference to Fig. 10.1, the
threshold can be set to l = 152. The corresponding binary image is reported
in Fig. 10.2.
In practice, the gray-scale histogram is noisy and the modes are diﬃcult
to identify. Often, there is no clear separation between the gray levels of
the objects and those of the background. To this end, various techniques
have been developed to increase the robustness of binary segmentation, which
require appropriate ﬁltering of the image before binarization and the adoption
of algorithms for automatic selection of the threshold.
Boundary-based segmentation
Boundary-based segmentation techniques usually obtain a boundary by grouping many single local edges, corresponding to local discontinuities of image
gray level. In other words, local edges are sets of pixels where the light intensity changes abruptly.
The algorithms for boundary detection ﬁrst derive an intermediate image
based on local edges from the original gray-scale image, then they construct
short-curve segments by edge linking, and ﬁnally obtain the boundaries by
joining these curve segments through geometric primitives often known in
advance.
Boundary-based segmentation algorithms vary in the amount of a priori
knowledge they incorporate in associating or linking the edges and their effectiveness clearly depends on the quality of the intermediate image based
on local edges. The more reliable the local edges in terms of their position,
orientation and ‘authenticity’, the easier the task of the boundary detection
algorithm.
Notice that edge detection is essentially a ﬁltering process and can often be
implemented via hardware, whereas boundary detection is a higher level task
usually requiring more sophisticated software. Therefore, the current trend

414

10 Visual Servoing

is that of using the most eﬀective edge detector to simplify the boundary
detection process. In the case of simple and well-deﬁned shapes, boundary
detection becomes straightforward and segmentation reduces to the sole edge
detection.
Several edge detection techniques exist. Most of them require the calculation of the gradient or of the Laplacian of function I(XI , YI ).
Since a local edge is deﬁned as a transition between two regions of signiﬁcantly diﬀerent gray levels, it is obvious that the spatial gradient of function
I(XI , YI ), which measures the rate of change of the gray level, will have large
magnitude close to these transitional boundary areas. Therefore, edge detection can be performed by grouping the pixels where the magnitude of the
gradient is greater than a threshold. Moreover, the direction of the gradient
vector will be the direction of maximum variation of the gray level.
Again, the choice of the value of the threshold is extremely important; in
the presence of noise, the threshold is the result of a trade-oﬀ between the
possibility of losing valid edges and that of detecting false edges.
For gradient computation, it suﬃces to evaluate the directional derivatives
of function I(XI , YI ) along two orthogonal directions. Since this function is
deﬁned on a discrete set of pixels, the derivatives are computed in an approximate way. The essential diﬀerences between gradient-based edge detection
techniques are the directions used for the computation of the derivatives and
the manner in which they approximate these derivatives and compute the
gradient magnitude.
The most common operator for the computation of the gradient is that
approximating the derivative along directions XI and YI with the ﬁrst diﬀerences:
Δ1 = I(XI + 1, YI ) − I(XI , YI )
Δ2 = I(XI , YI + 1) − I(XI , YI ).
Other operators, less sensitive to noise eﬀects are, for example, the Roberts
operator , based on the ﬁrst diﬀerences computed along the diagonals of a
(2 × 2) square of pixels:
Δ1 = I(XI + 1, YI + 1) − I(XI , YI )
Δ2 = I(XI , YI + 1) − I(XI + 1, YI ),
and the Sobel operator , deﬁned on a (3 × 3) square of pixels:
Δ1 = (I(XI + 1, YI − 1) + 2I(XI + 1, YI ) + I(XI + 1, YI + 1))
−(I(XI − 1, YI − 1) + 2I(XI − 1, YI ) + I(XI − 1, YI + 1))
Δ2 = (I(XI − 1, YI + 1) + 2I(XI , YI + 1) + I(XI + 1, YI + 1))
−(I(XI − 1, YI − 1) + 2I(XI , YI − 1) + I(XI + 1, YI − 1)).

10.2 Image Processing

415

Fig. 10.3. Contours of image of Fig. 10.1 obtained using Roberts (left) and Sobel
(right) operators

Then, the approximated magnitude, or norm, of gradient G(XI , YI ) can be
evaluated using one of the following two expressions:

G(XI , YI ) = Δ21 + Δ22
G(XI , YI ) = |Δ1 | + |Δ2 |,
and direction θ(XI , YI ) with the relationship
θ(XI , YI ) = Atan2 (Δ2 , Δ1 ) .
Figure 10.3 shows the images obtained from that of Fig. 10.1 by applying
the gradient operators of Sobel and Roberts and binarization; the thresholds
have been set to l = 0.02 and l = 0.0146, respectively.
An alternative edge detection method is based on the Laplacian operator ,
which requires the computation of the second derivatives of function I(XI , YI )
along two orthogonal directions. Also in this case, suitable operators are used
to discretize the computation of derivatives. One of the most common approximations is the following:
1
(I(XI , YI + 1) + I(XI , YI − 1)
4
+ I(XI + 1, YI ) + I(XI − 1, YI )) .

L(XI , YI ) = I(XI , YI ) −

In this case, the pixels of the contour are those where the Laplacian is lower
than a threshold. The reason is that the Laplacian is null at the points of
maximum magnitude of the gradient. The Laplacian, unlike the gradient, does
not provide directional information; moreover, being based on the calculation
of second derivatives, it is more sensitive to noise than the gradient.

416

10 Visual Servoing

10.2.2 Image Interpretation
Image interpretation is the process of calculating the image feature parameters
from the segments, whether they are represented in terms of boundaries or in
terms of regions.
The feature parameters used in visual servoing applications sometimes require the computation of the so-called moments. These parameters are deﬁned
on a region R of the image and can be used to characterize the position, orientation and shape of the two-dimensional object corresponding to the region
itself.
The general deﬁnition of moment mi,j of a region R of a framestore, with
i, j = 0, 1, 2, . . ., is the following:

mi,j =
I(XI , YI )XIi YIj .
XI ,YI ∈R

In the case of binary images, by assuming the light intensity equal to one for
all the points of region R, and equal to zero for all the points not belonging
to R, the following simpliﬁed deﬁnition of moment is obtained:

mi,j =
XIi YIj .
(10.1)
XI ,YI ∈R

In view of this deﬁnition, moment m0,0 coincides with the area of the region,
computed in terms of the total number of pixels of region R.
The quantities
m1,0
m0,1
x̄ =
ȳ =
m0,0
m0,0
deﬁne the coordinates of the so-called centroid of the region. These coordinates
can be used to detect uniquely the position of region R on the image plane.
Using an analogy from mechanics, region R can be seen as a twodimensional rigid body of density equal to light intensity. Hence, moment
m0,0 corresponds to the mass of the body and the centroid corresponds to the
centre of mass.
The value of moment mi,j in (10.1) depends on the position of region
R in the image plane. Therefore, the so-called central moments are often
considered, deﬁned as

(XI − x̄)i (YI − ȳ)j ,
μi,j =
XI ,YI ∈R

which are invariant with respect to translation.
According to the mechanical analogy, it is easy to recognize that the central
moments of second order μ2,0 and μ0,2 have the meaning of inertia moments
with respect to axes XI and YI respectively, while μ1,1 is an inertia product,
and the matrix


μ2,0 μ1,1
I=
,
μ1,1 μ0,2

10.2 Image Processing

417

Fig. 10.4. Region of a binary image and some feature parameters

has the meaning of inertia tensor relative to the centre of mass. The eigenvalues of matrix I deﬁne the principal moments of inertia, termed principal
moments of the region and the corresponding eigenvectors deﬁne the principal
axes of inertia, termed principal axes of the region.
If region R is asymmetric, the principal moments of I are diﬀerent and it
is possible to characterize the orientation of R in terms of the angle α between
the principal axis corresponding to the maximum moment and axis X. This
angle can be computed with the equation (see Problem 10.1)


2μ1,1
1
−1
.
(10.2)
α = tan
2
μ2,0 − μ0,2
As an example, in Fig. 10.4, the region of a binary image is shown; centroid
C, the principal axes, and the angle α are evidenced.
Notice that the moments and the corresponding parameters can also be
computed from the boundaries of the objects; moreover, these quantities are
especially useful to characterize objects of generic form. Often, however, the
objects present in the scene, especially those manufactured, have geometric
characteristics which are useful to take into account for image interpretation.
For example, many objects have edges that, in the image plane, correspond
to the intersection of linear parts of contour or to contour points of high
curvature. The coordinates of these points on the image plane can be detected
using algorithms robust against the noise, and therefore can be used as feature
parameters of the image. They are usually termed feature points.
In other cases, it is possible to identify true geometric primitives such
as lines or line segments, which are projections of linear edges or solids of
revolution (cones, cylinders), or ellipses, obtained as projections of circles or
spheres. These primitives can be characterized on the image plane in terms of a
minimum set of parameters. For example, a line segment can be characterized
by the coordinates of its endpoints, or alternatively, by the coordinates of its
midpoint (centroid), its length (moment m0,0 ) and its orientation (angle α); in
both cases, the characterization of the line segment requires four parameters.

418

10 Visual Servoing

Fig. 10.5. Reference frames for an eye-in-hand camera

10.3 Pose Estimation
Visual servoing is based on the mapping between the feature parameters of an
object measured in the image plane of the camera and the operational space
variables deﬁning the relative pose of the object with respect to the camera
or, equivalently, of the camera with respect to the object. Often, it is suﬃcient
to derive a diﬀerential mapping in terms of velocity. As for the computation
of the inverse kinematics of a manipulator, the diﬀerential problem is easier
to solve because the velocity mapping is linear; moreover, the solution to
the diﬀerential problem can be used to compute the pose by using numerical
integration algorithms.
The set of feature parameters of an image deﬁnes a (k×1) vector s, termed
feature vector . In the following, to simplify notation, normalized coordinates
(X, Y ) deﬁned in (5.44) will be used in place of pixel coordinates (XI , YI ) to
deﬁne the feature vector. Since only pixel coordinates can be directly measured, the normalized coordinates should be computed from pixel coordinates
using the inverse of mapping (5.45), provided that the intrinsic parameters of
the camera are known.
The feature vector s of a point is deﬁned as
 
X
s=
,
(10.3)
Y
while

⎡

⎤
X
s=⎣Y ⎦
1

denotes its representation in homogeneous coordinates.

10.3 Pose Estimation

419

10.3.1 Analytic Solution
Consider a camera, for example an eye-in-hand camera, and a reference frame
Oc –xc yc zc attached to the camera; consider also a reference frame Oo –xo yo zo
attached to the object, supposed to be rigid, and let T co be the homogeneous
transformation matrix corresponding to the relative pose of the object with
respect to the camera, deﬁned as
 c

Ro occ,o
c
,
(10.4)
To =
0T
1
with occ,o = oco − occ , where occ is the position vector of the origin of the camera
frame with respect to the base frame, expressed in camera frame, oco is the
position vector of the origin of the object frame with respect to the base frame,
expressed in the camera frame, and Rco is the rotation matrix of the object
frame with respect to the camera frame (Fig. 10.5).
The problem to solve is that of computing the elements of matrix T co from
the measurements of object feature parameters in the camera image plane. To
this end, consider n points of the object and let r oo,i = poi − ooo , i = 1, . . . , n,
denote the corresponding position vectors with respect to the object frame.
These quantities are assumed to be known, for example, from a CAD model of
the object. The projections of these points on the image plane have coordinates
 
Xi
si =
,
Yi
and deﬁne the feature vector
⎡

⎤
s1
.
s = ⎣ .. ⎦ .

(10.5)

sn
The homogeneous coordinates of the points of the object with respect to the
camera frame can be expressed as
r co,i = T co r oo,i .
Therefore, using (5.44), the homogeneous coordinates of the projections of
these points on the image plane are given by
λi si = ΠT co r oo,i ,

(10.6)

with λi > 0.
Assume that n correspondences are available, namely n equations of the
form (10.6) for n points of the object, whose coordinates are known both
in the object frame and in the image plane. These correspondences deﬁne
a system of equations to be solved for the unknown elements of matrix T co .

420

10 Visual Servoing

Computing the solution is a diﬃcult task because, depending on the type
and on the number of correspondences, multiple solutions may exist. This
problem, in photogrammetry, is known as PnP (Perspective-n-Point) problem.
In particular, it can be shown that:
• P3P problem has four solutions, in the case of three non-collinear points.
• P4P and P5P problems each have at least two solutions, in the case of
non-coplanar points, while the solution is unique in the case of at least
four coplanar points and no triplets of collinear points.
• PnP problem, with n ≥ 6 non-coplanar points, has only one solution.
The analytic solution to PnP problem is rather laborious. However, the
derivation becomes simpler in some particular cases as, for example, in the
case of coplanar points.
Without loss of generality, assume that the plane containing the points
of the object coincides with one of the three coordinate planes of the object
frame, for instance, with the plane of equation zo = 0; this implies that all
the points of the plane have the third coordinate equal to zero. Multiplying
both sides of (10.6) by the skew-symmetric matrix S(si ), the product on the
left-hand side is zero, leading to the homogeneous equation
S(si )H [ rx,i

ry,i

T

1 ] = 0,

(10.7)

where rx,i and ry,i are the two non-null components of vector r oo,i and H is
the (3 × 3) matrix
(10.8)
H = [ r 1 r 2 occ,o ] ,
r 1 and r 2 being the ﬁrst and the second column of rotation matrix Rco , respectively.
Vector equation (10.7), deﬁned on homogeneous coordinates of points belonging to two planes, is know as planar homography; this denomination is
used also for matrix H.
Notice that Eq. (10.7) is linear with respect to H and, therefore, can be
rewritten in the form
Ai (si )h = 0,
where h is the (9 × 1) column vector obtained by staking the columns of
matrix H, while Ai is the (3 × 9) matrix
Ai (si ) = [ rx,i S(si )

ry,i S(si ) S(si ) ] .

(10.9)

Since the rank of S(·) is at most 2, then the rank of matrix Ai is also at most
2; therefore, to compute h (up to a scale factor), it is necessary to consider at
least 4 equations of the form (10.9) written for 4 points of the plane, leading
to the system of 12 equations with 9 unknowns
⎤
⎡
A1 (s1 )
⎢ A (s ) ⎥
⎢ 2 2 ⎥
(10.10)
⎥ h = A(s)h = 0,
⎢
⎣ A3 (s3 ) ⎦
A4 (s4 )
with s deﬁned in (10.5).

10.3 Pose Estimation

421

It can be shown that, considering a set of four points with no triplets of
collinear points, matrix A has rank 8 and the system of equations in (10.10)
admits a non-null solution ζh, deﬁned up to a scaling factor ζ (see Problem 10.2). As a result, matrix ζH can be computed up to a scaling factor ζ.
The presented derivation is general and can be applied to any kind of planar
homography deﬁned by an equation of the form (10.7).
In view of (10.8), it is
r 1 = ζh1
r 2 = ζh2
c
oc,o = ζh3
where hi denotes the i-th column of matrix H. The absolute value of constant
ζ can be computed by imposing the unit norm constraint to vectors r 1 and
r2 :
1
1
=
,
|ζ| =
h1 
h2 
while the sign of ζ can be determined by choosing the solution corresponding
to the object in front of the camera. Finally, the third column r 3 of matrix
Rco can be computed as
r3 = r1 × r2 .
Notice that, because of the noise aﬀecting the measurements of the coordinates in the image plane, the results of this derivation are aﬀected by errors
that can be reduced by considering a number n > 4 of correspondences and
computing the solution ζh to the 3n equations in (10.10), up to a scaling factor ζ, using least-squares techniques. This, however, does not guarantee that
the resulting matrix Q = [ r 1 r 2 r 3 ] is a rotation matrix.
A possible solution overcoming this problem consists of computing the
rotation matrix ‘closest’ to Q with respect to a given norm such as the matrix
which minimizes the Frobenius norm1


 1/2
c
c
c
T
Ro − QF = Tr (Ro − Q) (Ro − Q)
,
(10.11)
with the constraint that Rco is a rotation matrix. The problem of minimizing
norm (10.11) is equivalent to that of maximizing the trace of matrix Rco T Q.
It can be shown that the solution to this problem is
⎡
⎤
1 0 0
(10.12)
Rco = U ⎣ 0 1 0 ⎦ V T
0 0 σ
where U and V T are, respectively, the left and right orthogonal matrices of
the singular value decomposition of Q = U ΣV T . The choice σ = det(U V T )
ensures that the determinant of Rco is equal to one (see Problem 10.3).
1

The Frobenius norm is deﬁned in Sect. A.4.

422

10 Visual Servoing
feature points

0.3

yo

0.2

P4

[m]

0.1

P1

−0.1

P3

P4

0.2

P3

0.1
xo

0

image plane

0.3

0

P2

−0.2

P2

P1

−0.1
−0.2

−0.2 −0.1

0 0.1 0.2
[m]

−0.2 −0.1

0

0.1 0.2

Fig. 10.6. Planar object; left: object frame and feature points; right: feature points
projections on the normalized image plane of a camera in the pose of Example 10.1

Example 10.1
Consider a planar object characterized by four feature points shown in Fig. 10.6,
where the object frame is represented. The feature points P1 , P2 , P3 , P4 are the
vertices of a square with side length l = 0.1 m. In Fig. 10.6, the images of the
projections of the four points of the object on the normalized image plane of the
camera are shown as well, under the assumption that the relative pose of the object
frame with respect to the camera frame is characterized by rotation matrix


Rco

= Rz (0)Ry (π/4)Rx (0) =

0.7071
0
−0.7071

0
1
0

0.7071
0
0.7071



and position vector occ,o = [ 0 0 0.5 ]T m. The normalized coordinates of the
four points of the object can be computed from the position vectors in the object
frame r oo,1 = [ 0 0 0 ]T m, r oo,2 = [ 0.1 0 0 ]T m, r oo,3 = [ 0.1 0.1 0 ]T m,
r oo,4 = [ 0 0.1 0 ]T m, using (10.6), which gives

 
s1 =

0
0


s2 =

0.1647
0




s3 =

0.1647
0.2329




s4 =



0
.
0.2

To solve the inverse problem, namely that of computing matrix T co from the coordinates of the four points both in the image plane and in the object frame, it is
necessary to build matrix A(s) from four matrices Ai (si ) deﬁned in (10.9). It is easy
to verify that matrix A(s) has rank 8; moreover, a non-null solution to the system
of equations in (10.10) can be computed using the singular value decomposition
A = U ΣV T
and coincides with the last column of matrix V , namely, with the right eigenvector
corresponding to the null singular value of A. From this computation, matrix


ζH =

−0.4714
0
0.4714

0
−0.6667
0

0
0
−0.3333



10.3 Pose Estimation

423

can be obtained. Normalization of the ﬁrst column yields |ζ| = 1.5. It is possible to
verify that, with the choice ζ = −1.5, the exact solution for occ,o is obtained; moreover, matrix Q coincides numerically with the sought rotation matrix Rco , without
using any kind of approximate solution. This result was expected, due to the absence
of measurement noise aﬀecting image plane coordinates si .

The above derivation is a particular case of a method, termed direct linear
transformation, which is aimed at computing the elements of matrix T co by
solving a system of linear equations obtained using n correspondences relative
to points in generic conﬁguration. In detail, from equalities
S(si ) [ Rco

occ,o ] r oo,i = 0,

(10.13)

which coincide with (10.7) in the case that points r oo,i belong to plane z0 = 0,
two independent linear equations with 12 unknowns are obtained, taking into
account that matrix S(·) is, at most, of rank 2. Therefore, n correspondences
produce 2n equations.
It can be shown that, considering a set of 6 points not all coplanar, the
matrix of coeﬃcients of the corresponding system of 12 equations with 12
unknowns has rank 11; therefore, the solution is deﬁned up to a scaling factor.
Once this solution has been computed, the elements of rotation matrix Rco
and of vector occ,o can be obtained using a derivation similar to that presented
above. Notice that, in practical applications, due to the presence of noise, the
system of equations has rank 12 and admits only the null solution. In this
case, it is necessary to consider a number n > 6 of correspondences and to
compute the solution of the resulting system of equations, deﬁned up to a
scaling factor, using least-squares techniques.
In conclusion, the presented method permits the computation of matrix
T co , characterizing the relative pose of the object frame with respect to the
camera frame, from the projections of n points of the object on the camera
image plane. To this end, it is necessary to know the position of these points
with respect to the object frame, and thus the object geometry, besides the
camera intrinsic parameters. The latter are required for computing normalized
coordinates si from pixel coordinates.
Notice that, if it is required to compute the object pose with respect to the
base frame (as usually happens in the case of eye-to-hand cameras) or to the
end-eﬀector frame (as usually happens in the case of eye-in-hand cameras),
then it is also necessary to know the camera extrinsic parameters. In fact, in
the ﬁrst case, it is
(10.14)
T bo = T bc T co ,
where the elements of matrix T bc represent the extrinsic parameters of an
eye-to-hand camera; on the other hand, in the case of eye-in-hand camera, it
is
(10.15)
T eo = T ec T co ,

424

10 Visual Servoing

where the extrinsic parameters matrix T ec characterize the pose of the camera
with respect to the end-eﬀector frame.
10.3.2 Interaction Matrix
If the object is in motion with respect to the camera, the feature vector s is,
in general, time-varying. Therefore, it is possible to deﬁne a (k × 1) velocity
vector in the image plane ṡ.
The motion of the object with respect to the camera is characterized by
the relative velocity


ȯcc,o
c
v c,o =
,
(10.16)
RTc (ω o − ω c )
where ȯcc,o is the time derivative of vector occ,o = RTc (oo − oc ), representing
the relative position of the origin of the object frame with respect to the origin
of the camera frame, while ω o and ω c are the angular velocities of the object
frame and camera frame, respectively.
The equation relating ṡ to v cc,o is
ṡ = J s (s, T co )v cc,o ,

(10.17)

where J s is a (k × 6) matrix termed image Jacobian. This equation is linear
but J s depends, in general, on the current value of the feature vector s and
on the relative pose of the object with respect to the camera T co .
It is useful to consider also the mapping between the image plane velocity
ṡ, the absolute velocity of the camera frame
 T

Rc ȯc
c
vc =
RTc ω c ,
and the absolute velocity of the object frame
 T 
Rc ȯo
c
vo =
.
RTc ω o
To this end, vector ȯcc,o can be expressed as
ȯcc,o = RTc (ȯo − ȯc ) + S(occ,o )RTc ω c ,
which allows equality (10.16) to be rewritten in the compact form
v cc,o = v co + Γ (occ,o )v cc ,


with
Γ (·) =

−I
O


S(·)
.
−I

(10.18)

10.3 Pose Estimation

425

Therefore, Eq. (10.17) can be rewritten in the form
ṡ = J s v co + Ls v cc ,

(10.19)

Ls = J s (s, T co )Γ (occ,o )

(10.20)

where the (k × 6) matrix

is termed interaction matrix . This matrix, in view of (10.19), deﬁnes the linear
mapping between the absolute velocity of the camera v cc and the corresponding
image plane velocity ṡ, in the case that the object is ﬁxed with respect to the
base frame (v co = 0).
The analytic expression of the interaction matrix is, in general, simpler
than that of the image Jacobian. The latter can be computed from the interaction matrix using the equation
J s (s, T co ) = Ls Γ (−occ,o ),

(10.21)

obtained from (10.20), with Γ −1 (occ,o ) = Γ (−occ,o ). In the following, examples
of computation of interaction matrix and image Jacobian for some of the most
common cases in applications are provided.
Interaction matrix of a point
Consider a point P of the object characterized, with respect to the camera
frame, by the vector of coordinates
r cc = RTc (p − oc ),

(10.22)

where p is the position of point P with respect to the base frame. Choose
vector s of normalized coordinates (10.3) as the feature vector of the point.
In view of (5.44), the following expression holds:
s = s(r cc ),
with
s(r cc )
and r cc = [ xc yc
ing (10.24) yields

1
=
zc

xc
yc






X
=
,
Y

(10.24)

T

zc ] . Computing the time derivative of (10.23) and usṡ =

with



(10.23)

1
∂s(r cc )
=
c
∂r c
zc



∂s(r cc ) c
ṙ ,
∂r cc c

1 0 −xc /zc
0 1

−yc /zc



1
=
zc

(10.25)


1 0 −X
0 1

−Y


.

426

10 Visual Servoing

To compute the interaction matrix, vector ṙ cc can be obtained from the time
derivative of (10.22) under the assumption that p is constant:
ṙ cc = −RTc ȯc + S(r cc )RTc ω c = [ −I

S(r cc ) ] v cc .

(10.26)

Combining Eqs. (10.25), (10.26), the following expression of interaction matrix
of a point can be found:
⎡
⎢
Ls (s, zc ) = ⎣

−

1
zc

0

0
1
−
zc

X
zc
Y
zc

XY

−(1 + X 2 )

Y

1+Y2

−XY

−X

⎤
⎥
⎦,

(10.27)

revealing that this matrix depends on the components of vector s and the sole
component zc of vector r cc .
The image Jacobian of a point can be computed using (10.21), (10.27) and
has the expression

c
c
c
c 
X
ro,z
+ ro,x
X −ro,y
−ro,y
1 1 0 −X
J s (s, T co ) =
,
c
c
c
c
zc 0 1 −Y −(ro,z
+ ro,y
Y)
ro,x
Y
ro,x
c
c
c
, ro,y
, ro,z
are the components of vector r co = r cc − occ,o = Rco r oo , r oo
where ro,x
being the constant vector expressing the position of point P with respect to
the object frame.

Interaction matrix of a set of points
The interaction matrix of a set of n points of the object P1 , . . . Pn can be built
by considering the (2n × 1) feature vector (10.5). If Lsi (si , zc,i ) denotes the
interaction matrix corresponding to point Pi , then the interaction matrix of
the set of points will be the (2n × 6) matrix
⎤
⎡
Ls1 (s1 , zc,1 )
⎥
⎢
..
Ls (s, z c ) = ⎣
⎦,
.
Lsn (sn , zc,n )
with z c = [ z c,1 . . . zc,n ]T .
The image Jacobian of a set of points can be easily computed from the
interaction matrix, using (10.21).
Interaction matrix of a line segment
A line segment is the part of the line connecting two points P1 and P2 . The
projection on the image plane is still a line segment that can be characterized
in terms of the middle point coordinates x̄, ȳ, the length L, and the angle α

10.3 Pose Estimation

427

formed by the line with respect to axis X. Therefore, the feature vector can
be deﬁned as
⎤
⎡
⎡ ⎤
(X1 + X2 )/2
x̄
⎥
⎢
⎢ ȳ ⎥ ⎢ (Y1 + Y2 )/2 ⎥
(10.28)
s=⎣ ⎦=⎢ √
⎥ = s(s1 , s2 )
L
⎣ ΔX 2 + ΔY 2 ⎦
α
tan−1 (ΔY /ΔX)
with ΔX = X2 − X1 , ΔY = Y2 − Y1 and si = [ Xi
the time derivative of this equation yields

T

Yi ] , i = 1, 2. Computing

∂s
∂s
ṡ1 +
ṡ2
∂s1
∂s2


∂s
∂s
=
Ls1 (s1 , zc,1 ) +
Ls2 (s2 , zc,2 ) v cc ,
∂s1
∂s2

ṡ =

where Lsi is the interaction matrix of point Pi , under the assumption that the
line segment is ﬁxed with respect to the base frame. Therefore, the interaction
matrix of a line segment is
Ls (s, z c ) =

∂s
∂s
Ls (s1 , zc,1 ) +
Ls (s2 , zc,2 ),
∂s1 1
∂s2 2

with
⎡

1/2
∂s
0
⎢
=⎣
−ΔX/L
∂s1
ΔY /L2

⎤
0
1/2 ⎥
⎦
−ΔY /L
2
−ΔX/L

⎡

1/2
∂s
0
⎢
=⎣
ΔX/L
∂s2
−ΔY /L2

⎤
0
1/2 ⎥
⎦.
ΔY /L
2
ΔX/L

Notice that vectors s1 and s2 can be computed as functions of parameters x̄,
ȳ, L, α, using (10.28). Therefore, the interaction matrix can be expressed as
a function of the feature vector s = [ x̄ ȳ L α ]T , besides the components
zc,1 and zc,2 of the endpoints P1 and P2 of the line segment.
The image Jacobian of a line segment can be easily computed from the
interaction matrix using (10.21).
10.3.3 Algorithmic Solution
The interaction matrix Ls is, in general, a matrix of dimension (k × m),
where k is equal to the number of feature parameters of the image and m is
the dimension of velocity vector v cc . Usually m = 6, but it may happen that
m < 6, when the relative motion of the object with respect to the camera is
constrained.
The image Jacobian J s is also of dimension (k × m), being related to Ls
by mapping (10.21). Since this mapping is invertible, the rank of J s coincides
with that of Ls .

428

10 Visual Servoing

In the case that Ls is full rank, by using (10.17), it is possible to compute
v cc,o from ṡ.
In particular, if k = m, the velocity v cc,o can be obtained using the expression
(10.29)
v cc,o = Γ (occ,o )L−1
s ṡ,
which requires the computation of the inverse of the interaction matrix.
In the case k > m, the interaction matrix has more rows than columns
and Eq. (10.17) can be solved using a least-squares technique, whose solution
can be written in the form
v cc,o = Γ (occ,o )(LTs Ls )−1 LTs ṡ,

(10.30)

where (LTs Ls )−1 LTs is the left pseudo-inverse of Ls . This situation is rather
frequent in applications, because it permits using interaction matrices with
good condition numbers.
Finally, in the case k < m, the interaction matrix has more columns than
rows and Eq. (10.17) admits inﬁnite solutions. This implies that the number
of parameters of the observed image is not suﬃcient to determine uniquely
the relative motion of the object with respect to the camera. Hence, there
exist relative motions of the object with respect to the camera (or vice versa)
that do not produce variations of the image feature parameters. The velocities
associated with these relative motions belong to the null subspace of J s , which
has the same dimension of the null subspace of Ls . If the problem is that of
computing uniquely the relative pose of the object with respect to the camera
from feature parameters in the image plane, this case has no interest.

Example 10.2
The interaction matrix of a point P is a matrix with more columns than rows of
dimension (2 × 6) and rank 2; therefore, the null subspace has dimension 4. It can be
seen immediately that this subspace contains the velocity of the camera translational
motion along the visual ray projecting point P on the image plane, proportional to
vector
v 1 = [ X Y 1 0 0 0 ]T ,
as well as the velocity of the camera rotational motion about this visual ray, proportional to vector
v 2 = [ 0 0 0 X Y 1 ]T .
Vectors v 1 and v 2 are independent and belong to a base of the null subspace of
Ls . The remaining base vectors are not easy to ﬁnd geometrically, but can be easily
computed analytically.

The pose estimation problem may be cast in a form analogous to that
of inverse kinematics algorithms for robot manipulators. To this end, it is

10.3 Pose Estimation

429

necessary to represent the relative pose of the object with respect to the
camera using a minimum number of coordinates, in terms of the (m × 1)
vector
 c 
oc,o
,
(10.31)
xc,o =
φc,o
where occ,o characterizes the position of the origin of the object frame with
respect to the camera frame and φc,o characterizes the relative orientation. If
Euler angles are used to represent orientation, then φc,o is the vector of the
angles extracted from rotation matrix Rco and the mapping between v cc,o and
ẋc,o is expressed by


I
O
c
ẋc,o = T A (φc,o )ẋc,o .
v c,o =
(10.32)
O T (φc,o )

Example 10.3
Consider a camera mounted on the end-eﬀector of the SCARA manipulator of
Fig. 2.36. Choose the camera frame parallel to the end-eﬀector frame, with axis
zc pointing downward. Assume that the camera observes a ﬁxed planar object, parallel to the image plane, and that axis zo of the object frame is parallel to axis zc
and points downward.
The geometry of the problem suggests that the relative position of the object
with respect to the camera can be represented by a vector occ,o , whereas the relative
orientation can be deﬁned by the angle α between the object frame and the camera
frame about axis zc . Therefore, m = 4 and



xc,o =



occ,o
.
α

(10.33)

Moreover, the time derivative α̇ coincides with the component of ω cc,o along zc , and
this is the sole non-null component of the angular velocity of the relative motion of
the object frame with respect to the camera frame. Hence, in (10.32), T A (φc,o ) is
the (4 × 4) identity matrix.

Equation (10.17) can be rewritten in the form
ṡ = J As (s, xc,o )ẋc,o ,

(10.34)

J As (s, xc,o ) = Ls Γ (−occ,o )T A (φc,o )

(10.35)

where the matrix

has a meaning analogous to that of the analytic Jacobian of a manipulator.

430

10 Visual Servoing

Fig. 10.7. Pose estimation algorithm based on the inverse of image Jacobian

Equation (10.34) is the starting point of a numeric integration algorithm
) c,o
for the computation of xc,o , similar to inverse kinematics algorithms. Let x
denote the current estimate of vector xc,o and let
)
s = s()
xc,o )
be the corresponding vector of image feature parameters computed from the
) c,o ; the objective of this algorithm is the minimization of
pose speciﬁed by x
the error
s.
(10.36)
es = s − )
Notice that, for the purpose of numerical integration, vector s is constant while
the current estimate )
s depends on the current integration time. Therefore,
computing the time derivative of (10.36) yields
) c,o )x
)˙ c,o .
ės = −)
s˙ = −J As ()
s, x

(10.37)

Assumed that matrix J As is square and nonsingular, the choice
)˙ c,o = J −1
) c,o )K s es
x
s, x
As ()

(10.38)

leads to the equivalent linear system
ės + K s es = 0.

(10.39)

Therefore, if K s is a positive deﬁnite matrix (usually diagonal), system (10.39)
is asymptotically stable and the error tends to zero with a convergence speed
that depends on the eigenvalues of matrix K s . The convergence to zero of
) c,o to the true
error es ensures the asymptotic convergence of the estimate x
value xc,o .
The block scheme of the pose estimation algorithm is shown in Fig. 10.7,
where s(·) denotes the function computing the feature vector of the ‘virtual’
) c,o of the object pose with
image corresponding to the current estimate x
respect to the camera. This algorithm can be used as an alternative to the
analytic methods for pose estimation illustrated in Sect. 10.3.1. Obviously, the
convergence properties depend on the choice of the image feature parameters

10.3 Pose Estimation

431

) c,o (0), which may produce instability
and on the initial value of estimate x
problems related to the singularities of matrix J As .
Notice that, in view of (10.35), the singularities of matrix J As are both
the representation singularities of the orientation and those of the interaction
matrix. The most critical singularities are those of the interaction matrix,
since they depend on the choice of the image feature parameters.
To separate the eﬀects of the two types of singularities, it is convenient to
compute (10.38) in two steps, evaluating ﬁrst

and then

)cc,o = Γ (occ,o )L−1
v
s K s es ,

(10.40)

)˙ c,o = T −1
x
v cc,o .
A (φc,o ))

(10.41)

Assumed to work far from representation singularities, the problem of
singularities of Ls can be overcome by using a number k of feature parameters
greater than the minimum required m. This choice also allows a reduction of
the eﬀects of measurement noise. The resulting estimation algorithm requires
the use of the left pseudo-inverse of Ls in place of the inverse, namely
)cc,o = Γ (occ,o )(LTs Ls )−1 LTs K s es
v

(10.42)

in place of (10.40). The convergence of error (10.36) can be shown using the
direct Lyapunov method based on the positive deﬁnite function 2
V (es ) =

1 T
e K s es > 0
2 s

∀es = 0.

Computing the time derivative of this function, and using (10.37), (10.35),
(10.41), (10.42), yields
V̇ = −eTs K s Ls (LTs Ls )−1 LTs K s es ,
which is negative semi-deﬁnite because N (LTs ) = Ø, LTs being a matrix with
more columns than rows. Therefore, the system is stable but not asymptotically stable. This implies that the error is bounded, but in some cases the
algorithm can get stuck with es = 0 and K s es ∈ N (LTs ).
Notice that the pose estimation methods based on inverse Jacobian are as
eﬃcient in terms of accuracy, speed of convergence and computational load,
) c,o (0) is close to the true value xc,o . Therefore, these
as the initial estimate x
methods are mainly adopted for real-time ‘visual tracking’ applications, where
the estimate on an image taken at time t̄ is computed assuming as initial value
the estimate computed on the image taken at time t̄−T , T being the sampling
time of the image (multiple of the sampling time of the numerical integration
algorithm).
2

See Sect. C.3 for the illustration of the direct Lyapunov method.

432

10 Visual Servoing
error norm

0.2

0.2

0.15

0.1

0.1

0
−0.1

0.05
0
0

image plane

0.3

−0.2
0.01

0.02
[s]

0.03

0.04

−0.2 −0.1

0

0.1 0.2

Fig. 10.8. Time history of the norm of the estimation error and corresponding
paths of the feature points projections on image plane
pos

1
0.8

z

[rad]

[m]

θ

0.8

0.6
0.4
0.2
0
0

orien

1

0.6
0.4
0.2

x,y
0.01

0.02
[s]

0.03

0.04

0
0

φ,ψ
0.01

0.02
[s]

0.03

0.04

Fig. 10.9. Time history of camera pose estimate

Example 10.4
Consider the object of Example 10.1. Using the algorithmic solution, it is desired
to compute the relative pose of the object frame, with respect to the camera frame,
from the image plane coordinates of the projections of the four feature points of the
object. The same numerical values of Example 10.1 are used.
Since the image Jacobian has dimension (6 × 8), it is necessary to use the
algorithm based on the pseudo-inverse of the interaction matrix. This algorithm
was simulated on a computer by adopting the Euler numerical integration scheme
with an integration time Δt = 1 ms, matrix gain K s = 160I 8 , and initial estimate
x
)c,o = [ 0 0 1 0 π/32 0 ]T .
The results in Fig. 10.8 show that the norm of the estimation error of the feature
parameters es tends to zero asymptotically with convergence of exponential type;
moreover, due to the fact that matrix gain K s was chosen diagonal with equal
elements, the paths of the projections of the feature points on the image plane
(between the initial positions marked with crosses and the ﬁnal positions marked
with circles) are line segments.
)c,o for position
The corresponding time histories of the components of vector x
and orientation are reported in Fig. 10.9, together with the time histories of the
components of the true value xc,o = [ 0 0 0.5 0 π/4 0 ]T (represented with

10.4 Stereo Vision

433

dashed lines). It can be veriﬁed that, with the chosen value of K s , the algorithm
converges to the true value in about 0.03 s, corresponding to 30 iterations.
The time histories of Fig. 10.9 can be interpreted as the position and orientation
)c,o (0) and the
trajectories of a ‘virtual’ camera in motion between the initial pose x
ﬁnal pose xc,o .
Notice that, for the purpose of pose estimation, the illustrated algorithm may
converge also in the case that only three feature points are used. In fact, in this case,
J As is a square (6 × 6) matrix and the convergence is ensured provided that this
matrix is nonsingular. However, since P3P problem has four solutions, the algorithm
may converge to a solution diﬀerent from that sought, unless, as for visual tracking
)c,o (0) is close enough to the true value xc,o .
applications, the initial estimate x

10.4 Stereo Vision
The bidimensional image provided by a camera does not give any explicit
information on depth, namely, the distance of the observed object from the
camera. This information can be recovered in an indirect way from the geometric model of the object, assumed to be known.
On the other hand, the depth of a point can be directly computed in
the case that two images of the same scene are available, taken from two
diﬀerent points of view. The two images can be obtained using two cameras,
or sequentially, using a moving camera. These cases are referred to as stereo
vision.
In the framework of stereo vision, two fundamental problems can be devised. The ﬁrst is the correspondence problem, which consists of the identiﬁcation of the points of the two images that are projections of the same point of
the scene. These points are termed conjugate or corresponding. This problem
is not easy to solve, and the solution is based on the existence of geometric
constraints between two images of the same point, besides the fact that some
details of the scene appear to be similar in the two images.
The second problem, illustrated below in some fundamental aspects, is
that of 3D reconstruction which, in general, consists of the computation of
the relative pose of the cameras (calibrated and not) and thus, starting from
this pose, the position in the 3D space of the points of the observed object.
10.4.1 Epipolar Geometry
Assume that two cameras are available, with respective reference frames, denoted as 1 and 2. Moreover, let o11,2 denote the position vector and R12 the
rotation matrix of Frame 2 with respect to Frame 1 and let T 12 be the corresponding homogeneous transformation matrix. The coordinates of a point P
expressed in the two frames are related by equation
p1 = o11,2 + R12 p2 .

(10.43)

434

10 Visual Servoing

Fig. 10.10. Epipolar geometry

Let s1 and s2 be the coordinates of the projections of P on the image planes
of the cameras; in view of (5.44) it is
λi si = Π pi = pi ,

i = 1, 2.

(10.44)

Substituting (10.44) into (10.43) yields
λ1 s1 = o11,2 + λ2 R12 s2 .

(10.45)

Premultiplying both sides of (10.45) by S(o11,2 ) gives
λ1 S(o11,2 )s1 = λ2 S(o11,2 )R12 s2 .
Hence, premultiplying both sides of the above equation by sT1 , the following
equality is obtained
λ2 sT1 S(o11,2 )R12 s2 = 0,
which has to be satisﬁed for any value of scalar λ2 . Therefore, this equality is
equivalent to the so-called epipolar constraint equation
sT1 E s2 = 0,

(10.46)

where E = S(o11,2 )R12 is a (3 × 3) matrix known as essential matrix . Equation (10.46) expresses in analytic form the geometric constraint existing between the projections of the same point on the image planes of the two cameras.
The geometric interpretation of the epipolar constraint can be derived
from Fig. 10.10, where the projections of a point P on the image planes of
the two cameras are reported as well as the respective optical centers O1 and
O2 . Notice that points O1 , O2 and P are the vertices of a triangle whose sides
O1 P and O2 P belong to the visual rays projecting point P into the points of
coordinates s1 and s2 of the image plane, respectively. These rays lay along
the directions of vectors s1 and R12 s2 respectively, expressed with respect to
Frame 1. Line segment O1 O2 , termed base line, is represented by vector o11,2 .
The epipolar constraint (10.46) corresponds to imposing that vectors s1 , R12 s2

10.4 Stereo Vision

435

and o11,2 are coplanar. The plane containing these vectors is termed epipolar
plane.
Notice that line segment O1 O2 belongs to the visual ray projecting point
O2 on the image plane of Camera 1 and, at the same time, to the ray projecting
point O1 on the image plane of Camera 2. These projections, of coordinates
e1 and e2 , respectively, are termed epipoles. Line 1 , passing through the
points of coordinates s1 and e1 , and line 2 , passing through the points of
coordinates s2 and e2 , are termed epipolar lines. The epipolar lines can also
be obtained as the intersection of the epipolar plane with the image planes of
the two cameras. Notice that, by varying point P , the epipolar plane describes
a set of planes about the base line and the epipoles do not change.
For the purpose of computing the correspondences, the epipolar constraint
can be exploited to reduce the complexity of the problem to ﬁnd conjugate
points. In fact, if s1 is the image of a point of the visual ray passing through
O1 and the point of coordinates s1 , the corresponding conjugate point of
the image plane of Camera 2 must necessarily belong to the epipolar line 2 ,
which is known because the epipolar plane is uniquely deﬁned by O1 , O2 and
by the point of coordinates s1 . Finding correspondences, therefore, reduces
to searching for a point along the epipolar line and not on the whole image
plane.
In the framework of 3D reconstruction, diﬀerent scenarios may arise, depending on the type of information which is available a priori.
10.4.2 Triangulation
In the case that both intrinsic and extrinsic parameters of the two cameras
are known, the reconstruction problem consists of computing the position in
the scene of the points projected on the two image planes using a geometric
method known as triangulation. This method allows the computation of coT
ordinates p = [ px py pz ] of a point P with respect to the base frame,
T
T
starting from normalized coordinates s1 = [ X1 Y1 ] and s2 = [ X2 Y2 ]
of the projections of P on the image planes of the two cameras. Assume that,
for simplicity, the base frame coincides with Frame 1, then p1 = p, o11,2 = o
and R12 = R.
From (10.44), (10.45) the following equalities can be derived:
p = λ 1 s1
p = o + λ 2 R s2 ,

(10.47)
(10.48)

where the ﬁrst equality is the parametric equation of the visual ray passing
through O1 and the point of coordinates s1 , while the second represents the
parametric equation of the visual ray passing through O2 and the point of
coordinates s2 ; both equations are expressed in the base frame.
Therefore, the coordinates of point P at the intersection of the two visual
rays can be computed by solving the system of two Eqs. (10.47), (10.48) with

436

10 Visual Servoing

respect to p. To this end, from (10.47), by computing λ1 in the third equation
and replacing its value into the other two equations, the following system is
obtained:


1 0 −X1
p = 0.
(10.49)
0 1 −Y1
Using a similar derivation for the equation obtained by premultiplying both
sides of (10.48) by RT , the following system is obtained



 T
ox − oz X 2
r 1 − X2 r T3
p=
,
(10.50)
r T2 − Y2 r T3
oy − oz Y2
T

with R = [ r 1 r 2 r 3 ] and RT o = [ ox oy oz ] . Equations (10.49),
(10.50) deﬁne a system of four equations and three unknowns, which is linear
with respect to p. Of these equations, only three are independent in the ideal
case that the two visual rays intersect at point P . In practical applications,
because of noise, these equations are all independent and the system has no
solution; hence, suitable algorithms based on least-squares techniques have to
be adopted to compute an approximate solution.
Computation of p can be greatly simpliﬁed in the case, rather frequent
in applications, that the two cameras have parallel and aligned image planes,
T
with R = I and RT o = [ b 0 0 ] , b > 0 being the distance between the
origins of the two camera frames. This implies that the solution to the system
of Eqs. (10.49), (10.50) is
X1 b
X1 − X2
Y1 b
Y2 b
=
py =
X1 − X2
X1 − X2
b
.
pz =
X1 − X2

px =

(10.51)
(10.52)
(10.53)

10.4.3 Absolute Orientation
In the case of a calibrated system of two cameras observing a rigid object
of unknown shape, triangulation can be used to compute the variation of
pose of the object or of the system of cameras, due to the relative motion
of the system with respect to the cameras. This problem, known as absolute
orientation, requires the measurement of the positions of the projections of a
certain number of feature points of the object.
If the stereo camera system is moving and the object is ﬁxed, let p1 , . . . , pn
denote the position vectors of n points of the rigid object measured at time
t and p1 , . . . , pn the position vectors of the same points measured at time t
using triangulation. These vectors are all referred to Frame 1 and, under the
assumption of rigid motion, satisfy the equations
pi = o + Rpi

i = 1, . . . , n

(10.54)

10.4 Stereo Vision

437

where vector o and rotation matrix R deﬁne the position and orientation
displacement of Frame 1 between time t and time t . The absolute orientation
problem consists of the computation of R and o from pi and pi .
From rigid body mechanics it is known that this problem has a unique
solution in the case of three non-collinear points. In this case, nine nonlinear
equations can be derived from (10.54), in terms of the nine independent parameters which characterize o and R. However, since the points are obtained
using triangulation, the measurements are aﬀected by error and the system
may have no solution. In this case, it is convenient to consider a number n > 3
of points and compute o and R as the quantities which minimize the linear
quadratic function
n

pi − o − Rpi 2 ,
(10.55)
i=1

with the constraint that R is a rotation matrix. The problem of computing o
can be separated from the problem of computing R observing that the value
of o which minimizes function (10.55) is (see Problem 10.6)
o = p̄ − Rp̄

(10.56)

where p̄ and p̄ are the centroids of the set of points {pi } and {pi }, deﬁned
as
n
n
1
1 
p̄ =
pi ,
p̄ =
p.
n i=1
n i=1 i
Hence the problem becomes that of computing the rotation matrix R which
minimizes the linear quadratic function
n


p̄i − Rp̄i 2 ,

(10.57)

i=1

where p̄i = pi − p̄ and p̄i = pi − p̄ are the deviations with respect to the
centroids.
It can be proven that the matrix R which minimizes function (10.57) is
the matrix which maximizes the trace of RT K, with
K=

n


p̄i p̄i T ;

i=1

see Problem 10.7. Therefore, the solution has the form (10.12) where, for
the purpose of this problem, U and V are respectively the left and right
orthogonal matrices of the singular value decomposition of K. Once that
rotation matrix R is known, vector o can be computed using (10.56).

438

10 Visual Servoing

10.4.4 3D Reconstruction from Planar Homography
Another interesting case of application of 3D reconstruction occurs when the
feature points of the observed object lie on the same plane. This geometric
property represents an additional constraint between the projections of each
point on the image plane of the two cameras, besides the epipolar constraint.
This constraint is a planar homography.
Let p2 be the position vector of a point P of the object, expressed with
respect to Frame 2. Moreover, let n2 denote the unit vector orthogonal to
the plane containing the feature points and d2 > 0 the distance of the plane
from the origin of Frame 2. By virtue of simple geometric considerations, the
following equation can be derived:
1 2T 2
n p =1
d2
which deﬁnes the set of points p2 belonging to the plane. In view of the above
equality, Eq. (10.43) can be rewritten in the form
p1 = Hp2 ,
with
H = R12 +

1 1 2T
o n .
d2 1,2

(10.58)
(10.59)

Replacing (10.44) into (10.58) yields
s1 = λH s2 ,

(10.60)

where λ = λ2 /λ1 > 0 is an arbitrary constant. Premultiplication of both sides
of (10.60) by S(s1 ) yields the equality
S(s1 )H s2 = 0,

(10.61)

representing a planar homography deﬁned by matrix H.
Using a derivation similar to that presented in Sect. 10.3.1, it is possible
to compute numerically matrix ζH, up to a scaling factor ζ, starting from
the coordinates of n points of the plane, with n ≥ 4.
The value of the scaling factor ζ can be computed using a numerical algorithm based on expression (10.59) of matrix H; once H is known, it is
possible to compute quantities R12 , o11,2 /d2 and n2 in (10.59) — actually, it
can be shown that two admissible solutions exist.
This result is of a certain relevance for visual servoing applications. For
example, in the case of a camera in motion with respect to the object, if
Frames 1 and 2 represent the poses of the camera in two diﬀerent time instants,
the computation of H with decomposition (10.59) can be used to evaluate the
orientation displacement of the camera frame and the position displacement
of the origin, the latter deﬁned up to a scaling factor d2 . This information
can be achieved without knowing the object geometry, as long as the feature
points all belong to the same plane.

10.4 Stereo Vision

439

Example 10.5
Consider the SCARA manipulator of Example 10.3 and the planar object of Example 10.1. Assume that the feature vector of the four points of the object, measured
by the camera at time t , is
s = [ 0

0

0.2

0

0.2

0.2

0.2 ]T ,

0

while the feature vector, measured by the camera at time t , is
s = [ −0.1667 0.1667 −0.0833 0.0223 0.0610 0.1057 −0.0223 0.2500 ]T .
It is desired to compute the quantities R12 , (1/d2 )o11,2 and n2 of the planar homography (10.59).
For simplicity, assume that the orientation of the planar object is known, namely
n2 = [ 0

0

1 ]T .

A further simpliﬁcation to the problem derives from the fact that, in this case,
matrix R12 corresponds to a rotation about axis z of an angle β, namely R12 = Rz (β).
Therefore, in view of (10.59), planar homography H has the symbolic expression


H=

−sβ
cβ
0

cβ
sβ
0

ox /d2
oy /d2
1 + oz /d2


,

with o11,2 = [ ox oy oz ]T .
On the other hand, starting from numerical values of s and s and using a
derivation similar to that used in Example 10.1, the following matrix can be obtained:


0.3015 −0.5222 0.1373
ζH = 0.5222 0.3015 0.0368 ,
0
0
0.5025
which is the numerical value of the planar homography, up to a scaling factor ζ.
The symbolic expression of H reveals that the ﬁrst and second column of this
matrix have unit norm. This property can be used to compute |ζ| = 1.6583. The
sign of ζ can be evaluated by imposing, for any of the feature points of the object,
the constraint
T
p1T p1 = p1T H p2 = λ1 λ2 s1 H s2 > 0.
Since scalars λi are positive, this inequality is equivalent to
T

s1 H s2 > 0,
hence, in this case, it is ζ > 0. Therefore



H=

0.5
0.8660
0

−0.8660
0.5
0



0.2277
0.0610 .
0.8333

At this point, the value of angle β can be computed from the elements h11 and
h21 of matrix H using equation
β = Atan2(h21 , h11 ) =

π
.
3

440

10 Visual Servoing

Finally, vector (1/d2 )o11,2 can be computed from the elements of the last row of
matrix H using the equality



1 1
o1,2 =
d2

h13
h23
h33 − 1





=



0.2277
0.0610 .
−0.0667

Notice that the derivation illustrated in Example 10.5, in the case that n2
is unknown and R12 is a generic rotation matrix, becomes much more complex.

10.5 Camera Calibration
An important problem for visual servoing applications is the calibration of
the camera, which is the sensor providing the information fed back to the
controller. Calibration consists of the estimation of the intrinsic parameters,
characterizing matrix Ω deﬁned in (5.41), and of the extrinsic parameters,
characterizing the pose of the camera frame with respect to the base frame (for
eye-to-end cameras) or to the end-eﬀector frame (for eye-in-hand cameras).
Various calibration techniques exist, which are based on algorithms similar to
those used for the pose estimation of an object with respect to a camera.
In particular, the solution method of a PnP problem with n coplanar
points illustrated in Sect. 10.3.1 can be directly used for the computation of
the extrinsic parameters of the camera, if the intrinsic parameters are known.
In fact, in view of (10.14), extrinsic parameters of an eye-to-hand camera
can be computed as
(10.62)
T bc = T bo (T co )−1 ,
where matrix T co is the output of the algorithm solving a PnP planar problem,
provided that matrix T bo , expressing the position and the orientation of the
object frame with respect to the base frame, is known. Similarly, in view
of (10.15), the extrinsic parameters of an eye-in-hand camera can be computed
as
(10.63)
T ec = T eo (T co )−1 ,
provided that matrix T eo , expressing the pose of the object frame with respect
to the end-eﬀector frame, is known.
If the intrinsic parameters are not known, the derivation of Sect. 10.3.1 has
to be suitably extended and can be broken down in the three phases described
below.
Phase 1
A planar homography can be computed starting from pixel coordinates


XIi
ci =
YIi

10.5 Camera Calibration

441

in place of normalized coordinates si . In detail, an equation formally identical
to (10.7) can be obtained:
S(ci )H  [ rx,i

ry,i

T

1 ] = 0,

(10.64)

where H  is the (3 × 3) matrix
H  = ΩH,

(10.65)

Ω being the matrix of intrinsic parameters (5.41) and H the matrix deﬁned
in (10.8). Using an algorithm similar to that presented in Sect. 10.3.1, planar homography ζH  can be computed, up to a scaling factor ζ, from the
coordinates of n points of the plane, with n ≥ 4.
Phase 2
Matrix Ω can be computed from the elements of matrix ζH  . In fact, taking
into account (10.65), and the deﬁnition of H in (10.8), yields
ζ [ h1

h2

h3 ] = Ω [ r 1

r2

occ,o ] ,

where hi denotes the i-th column of matrix H  . Computing r 1 and r 2 from
this equation, and imposing the orthogonality and unit norm constraints on
these vectors, the following two scalar equations are obtained:
h1 T Ω −T Ω −1 h2 = 0
h1 T Ω −T Ω −1 h1 = h2 T Ω −T Ω −1 h2
which, being linear, can be rewritten in the form
A b = 0.

(10.66)

In the above equation, A is a (2 × 6) matrix of coeﬃcients depending on
T
h1 , h2 , while b = [ b11 b12 b22 b13 b23 b33 ] , where bij is the generic
element of the symmetric matrix
⎤
⎡
0
−X0 /αx2
1/αx2
⎥
⎢
1/αy2
−Y0 /αy2
B = Ω −T Ω −1 = ⎣ ∗
(10.67)
⎦.
2
2
2
2
∗
∗
1 + X0 /αx + Y0 /αy
By repeating Phase 1 k times, with the same plane placed each time in a
diﬀerent pose, 2k equations of the form (10.66) are obtained. These equations,
in the case k ≥ 3, have a unique solution γb deﬁned up to a scaling factor γ.
From matrix γB, in view of (10.67), the following expressions for the intrinsic
parameters can be found:
X0 = −b13 /b11

442

10 Visual Servoing

Fig. 10.11. Example of calibration plane

Y0 = −b23 /b22

αx = γ/b11

αy = γ/b22 ,
where bi,j = γbi,j and γ can be computed as
γ = b13 + b23 + b33 .
Phase 3
Once the matrix Ω of intrinsic parameters has been evaluated, it is possible
to compute H from H  , up to a scaling factor ζ, using (10.65) for one of
the k poses of the plane. Hence, matrix T co can be computed from H as
for the case of the solution of a PnP problem shown in Sect. 10.3.1. Finally,
using equations (10.62), (10.63), the extrinsic parameters of the camera can
be evaluated.
The method illustrated above is merely conceptual, because it does not
provide satisfactory solutions in the presence of measurement noise or lens
distortion — especially for the intrinsic parameters; however, the accuracy of
the result can be improved using models that take into account the distortion
phenomena of the lenses, together with nonlinear optimization techniques.
From the experimental point of view, the calibration method described
above requires the use of calibration planes where a certain number of points
can be easily detected; also, the position of these points with respect to a
suitable reference frame must be known with high accuracy. An example of
calibration plane with a chessboard pattern is shown in Fig. 10.11.
Finally, notice that a calibration method can also be set up starting from
the solution of a nonplanar PnP problem, using the direct linear transformation method.

10.6 The Visual Servoing Problem

443

Fig. 10.12. General block scheme of position-based visual servoing

10.6 The Visual Servoing Problem
Visual measurements allow a robot to collect information on the surrounding
environment. In the case of robot manipulators, such information is typically
used to compute the end-eﬀector pose with respect to an object observed by
the camera. The objective of visual servoing is to ensure that the end-eﬀector,
on the basis of visual measurements elaborated in real time, reaches and keeps
a (constant or time-varying) desired pose with respect to the observed object.
It is worth remarking that the direct measurements provided by the visual
system are concerned with feature parameters in the image plane, while the
robotic task is deﬁned in the operational space, in terms of the relative pose
of the end-eﬀector with respect to the object. This fact naturally leads to
considering two kinds of control approaches, illustrated by the block schemes
of Figs. 10.12 and 10.13, namely position-based visual servoing, also termed
visual servoing in the operational space, and image-based visual servoing, also
termed visual servoing in the image space. In these schemes, the case of eyein-hand camera is considered; for eye-to-hand cameras, similar schemes can
be adopted.
The position-based visual servoing approach is conceptually similar to the
operational space control illustrated in Fig. 8.2. The main diﬀerence is that
feedback is based on the real-time estimation of the pose of the observed
object with respect to the camera using visual measurements. Estimation
can be performed analytically or using iterative numerical algorithms. Its
conceptual advantage regards the possibility of acting directly on operational
space variables. Therefore, the control parameters can be selected on the basis
of suitable speciﬁcations imposed to the time response of the end-eﬀector
motion variables, both at steady state and during the transient. The drawback
of this approach is that, due to the absence of direct control of the image
features, the object may exit from the camera ﬁeld of view during the transient
or as a consequence of planning errors; hence, the feedback loop turns out to
be open due to lack of visual measurements and instability may occur.
In the image-space visual servoing approach, the control action is computed on the basis of the error deﬁned as the diﬀerence between the value of

444

10 Visual Servoing

Fig. 10.13. General block scheme of image-based visual servoing

the image feature parameters in the desired conﬁguration — computed using
perspective transformation or directly measured with the camera in the desired pose — and the value of the parameters measured with the camera in the
current pose. The conceptual advantage of this solution regards the fact that
the real-time estimate of the pose of the object with respect to the camera
is not required. Moreover, since the control acts directly in the image feature
parameters, it is possible to keep the object within the camera ﬁeld of view
during the motion. However, due to the nonlinearity of the mapping between
the image feature parameters and the operational space variables, singular
conﬁgurations may occur, which cause instability or saturation of the control action. Also, the end-eﬀector trajectories cannot be easily predicted in
advance and may produce collisions with obstacles or joint limits violation.
To compare the two control strategies it is also worth considering the operating conditions. Of particular importance is the issue of camera calibration.
It is easy to understand that position-based visual servoing is more sensitive
to camera calibration errors compared to image-based visual servoing. In fact,
for the ﬁrst approach, the presence of uncertainties on calibration parameters,
both intrinsic and extrinsic, produces errors on the estimate of operational
space variables that may be seen as an external disturbance acting on the
feedback path of the control loop, where disturbance rejection capability is
low. On the other hand, in the image-based visual servoing approach, the
quantities used for the computation of the control action are directly deﬁned
in the image plane and measured in pixel units; moreover, the desired value
of the feature parameters is measured using the camera. This implies that
the uncertainty aﬀecting calibration parameters can be seen as a disturbance
acting on the forward path of the control loop, where disturbance rejection
capability is high.
A further aspect to analyze concerns knowledge of the geometric model
of the object. It is evident that, for position-based visual servoing, the object
geometry must be known if only one camera is used, because it is necessary
for pose estimation, while it may be unknown when a stereo camera system is

10.7 Position-based Visual Servoing

445

employed. On the other hand, image-based visual servoing does not require, in
principle, knowledge of the object geometry, even for mono-camera systems.
On the above premises, in the following, the main position-based and
image-based visual servoing schemes are illustrated. For both approaches, the
problem of regulation to a constant set-point is presented and the object is
assumed to be ﬁxed with respect to the base frame. Without loss of generality, the case of a single calibrated camera, mounted on the manipulator’s
end-eﬀector, is considered (see Fig. 10.5); moreover, the end-eﬀector frame is
chosen so as to coincide with the camera frame.

10.7 Position-based Visual Servoing
In position-based visual servoing schemes, visual measurements are used to
estimate in real time the homogeneous transformation matrix T co , representing the relative pose of the object frame with respect to the camera frame.
From matrix T co , the (m × 1) vector of independent coordinates xc,o , deﬁned
in (10.31), can be extracted.
Assumed that the object is ﬁxed with respect to the base frame, the
position-based visual servoing problem can be formulated by imposing a desired value to the relative pose of the object frame with respect to the camera
frame. This quantity can be speciﬁed in terms of homogeneous transformation
matrix T do , where superscript d denotes the desired pose of the camera frame.
From this matrix, the (m × 1) operational space vector xd,o can be extracted.
Matrices T co and T do can be used to obtain the homogeneous transformation matrix
 d

d
R
o
c
d,c
T dc = T do (T co )−1 =
,
(10.68)
0T
1
expressing the position and orientation displacement of the camera frame in
the current pose with respect to the desired pose. From this matrix, a suitable
error vector in the operational space can be computed, deﬁned as
 d 
od,c
,
(10.69)
x=−
φd,c
where φd,c is the vector of the Euler angles extracted from rotation matrix
Rdc . Vector x does not depend on the object pose and represents the error
between the desired pose and the current pose of the camera frame. It is
worth observing that this vector does not coincide with the diﬀerence between
xd,o and xc,o , but it can be computed from the corresponding homogeneous
transformation matrices, using (10.68), (10.69).
The control has to be designed so that the operational space error x tends
to zero asymptotically.
Notice that, for the choice of the set point xd,o , the knowledge of the object
pose is not required. However, the control objective can be satisﬁed provided

446

10 Visual Servoing

that the desired pose of the camera frame with respect to the base frame,
corresponding to the homogeneous transformation matrix


R d od
,
(10.70)
T d = T c (T dc )−1 =
0T
1
belongs to the dexterous workspace of the manipulator. If the object is ﬁxed
with respect to the base frame, this matrix is constant.
10.7.1 PD Control with Gravity Compensation
Position-based visual servoing can be implemented using PD control with
gravity compensation, suitably modiﬁed with respect to that used for motion
control.
Computing the time derivative of (10.69), for the position part, gives
ȯdd,c = ȯdc − ȯdd = RTd ȯc ,
while, for the orientation part, it gives
φ̇d,c = T −1 (φd,c )ω dd,c = T −1 (φd,c )RTd ω c .
To compute the above expressions, equalities ȯdd = 0 and ω dd = 0 have been
taken into account, observing that od and Rd are constant. Therefore, x˙ has
the expression
 T

O
˙x = −T −1 (φ ) Rd
(10.71)
vc .
d,c
A
O RTd
Since the end-eﬀector frame and the camera frame coincide, the following
equality holds:
x˙ = −J Ad (q, x)q̇,
(10.72)
where the matrix
J Ad (q, x) =

T −1
A (φd,c )



RTd
O

O
RTd


J (q)

(10.73)

has the meaning of analytic Jacobian of the manipulator in the operational
space, as for the Jacobian in (9.14).
Position-based visual servoing of PD type with gravity compensation has
the expression
u = g(q) + J TAd (q, x)(K P x − K D J Ad (q, x)q̇),

(10.74)

analogous to motion control law (8.110), but using a diﬀerent deﬁnition of
operational space error. The asymptotic stability of the equilibrium pose corresponding to x = 0, under the assumption of symmetric and positive deﬁnite
matrices K P , K D , can be proven using the Lyapunov function
V (q̇, x) =

1 T
1
q̇ B(q)q̇ + xT K P x > 0
2
2

similarly to the case of control law (8.110).

∀q̇, x = 0,

10.7 Position-based Visual Servoing

447

Fig. 10.14. Block scheme of position-based visual servoing of PD type with gravity
compensation

Notice that, for the computation of control law (10.74), the estimation of
xc,o and the measurements of q and q̇ are required. Moreover, the derivative
term can also be chosen as −K D q̇.
The block scheme of position-based visual servoing of PD type with gravity
compensation is shown in Fig. 10.14. Notice that the sum block computing
error x and that computing the output of the controlled system have a purely
conceptual meaning and do not correspond to algebraic sums.
10.7.2 Resolved-velocity Control
The information deriving from visual measurements is computed at a frequency lower or equal to the camera frame rate. This quantity, especially for
CCD cameras, is at least of one order of magnitude lower than the typical
frequencies used for motion control of robot manipulators. As a consequence,
in the digital implementation of control law (10.74), to preserve stability of
the closed-loop system, the control gains must be set to values much lower
than those typically used for motion control; therefore, the performance of the
closed-loop system in terms of speed of convergence and disturbance rejection
capability turns out to be poor.
This problem can be avoided assuming that the manipulator is equipped
with a high-gain motion controller in the joint space or in the operational
space. Neglecting the eﬀects on the tracking errors deriving from manipulator
dynamics and disturbances, the controlled manipulator can be considered as
an ideal positioning device. This implies that, in the case of joint space motion
control, the following equality holds:
q(t) ≈ q r (t),
q r (t) being the imposed reference trajectory for the joint variables.

(10.75)

448

10 Visual Servoing

Fig. 10.15. Block scheme of resolved-velocity position-based visual servoing

Therefore, visual servoing can be achieved by computing the trajectory
q r (t) on the basis of visual measurements, so that the operational space tracking error (10.69) goes asymptotically to zero.
To this end, Eq. (10.72) suggests the following choice for the joint space
reference velocity:
(10.76)
q̇ r = J −1
Ad (q r , x)K x
which, replaced in (10.72), by virtue of equality (10.75), yields the linear
equation
x˙ + K x = 0.
(10.77)
This equality, for a positive deﬁnite matrix K, implies that the operational
space error tends to zero asymptotically with a convergence of exponential
type and speed depending on the eigenvalues of matrix K; the larger the
eigenvalues, the faster the convergence.
The above scheme is termed resolved-velocity control in the operational
space, because it is based on the computation of velocity q̇ r from the operational space error. Trajectory q r (t) is computed from (10.76) via a simple
integration.
The block scheme of resolved-velocity position-based visual servoing is
reported in Fig. 10.15. Also in this case, the sum block computing the error
x and that computing the output of the scheme have a purely conceptual
meaning and do not correspond to algebraic sums.
Notice that the choice of K inﬂuences the transient behaviour of the trajectory of the camera frame, which is the solution to the diﬀerential equation (10.77). If K is a diagonal matrix with the same gains for the positional
part, the origin of the camera frame follows the line segment connecting the
initial position to the desired position. On the other hand, the orientation trajectory depends on the particular choice of Euler angles and, more in general,
of the orientation error. The possible choices of the orientation error are those
presented in Sect. 3.7.3, with the appropriate deﬁnition of Jacobian (10.73).
The possibility of knowing in advance the trajectory of the camera is important because, during the motion, the object may exit from the camera ﬁeld of
view, making visual measurements unavailable.

10.8 Image-based Visual Servoing

449

10.8 Image-based Visual Servoing
If the object is ﬁxed with respect to the base frame, image-based visual servoing can be formulated by stipulating that the vector of the object feature
parameters has a desired constant value sd corresponding to the desired pose
of the camera. Therefore, it is implicitly assumed that a desired pose xd,o
exists so that the camera pose belongs to the dexterous workspace of the
manipulator and
(10.78)
sd = s(xd,o ).
Moreover, xd,o is supposed to be unique. To this end, the feature parameters
can be chosen as the coordinates of n points of the object, with n ≥ 4 for
coplanar points (and no triplets of collinear points) or n ≥ 6 in case of noncoplanar points. Notice that, if the operational space dimension is m < 6, as
for the case of SCARA manipulator, a reduced number of points can be used.
The interaction matrix Ls (s, z c ) depends on variables s and z c with z c =
[ z c,1 . . . zc,n ]T , zc,i being the third coordinate of the generic feature point of
the object.
It is worth noticing that the task is assigned directly in terms of feature
vector sd , while pose xd,o does not need to be known. In fact, sd can be
computed by measuring the feature parameters when the object is in the
desired pose with respect to the camera.
The control law must be designed so as to guarantee that the image space
error
(10.79)
es = sd − s
tends asymptotically to zero.
10.8.1 PD Control with Gravity Compensation
Image-based visual servoing can be implemented using a PD control with
gravity compensation deﬁned on the basis of the image space error.
To this end, consider the following positive deﬁnite quadratic form as Lypapunov function candidate:
V (q̇, es ) =

1 T
1
q̇ B(q)q + eTs K P s es > 0
2
2

∀q̇, es = 0,

(10.80)

with K P s symmetric and positive deﬁnite (k × k) matrix.
Computing the time derivative of (10.80) and taking into account the
expression (8.7) of the joint space dynamic model of the manipulator and
property (7.49) yields
V̇ = −q̇ T F q̇ + q̇ T (u − g(q)) + ėTs K P s es .

(10.81)

Since ṡd = 0 and the object is ﬁxed with respect to the base frame, the
following equality holds:
ės = −ṡ = −J L (s, z c , q)q̇,

(10.82)

450

10 Visual Servoing

Fig. 10.16. Block scheme of image-based visual servoing of PD type with gravity
compensation

where



RTc
J L (s, z c , q) = Ls (s, z c )
O

O
RTc


J (q),

(10.83)

the camera frame and the end-eﬀector frame being coincident.
Therefore, with the choice
u = g(q) + J TL (s, z c , q) (K P s es − K Ds J L (s, z c , q)q̇) ,

(10.84)

where K Ds is a symmetric and positive deﬁnite (k × k) matrix, Eq. (10.81)
becomes
V̇ = −q̇ T F q̇ − q̇ T J TL K Ds J L q̇.
(10.85)
Control law (10.84) includes a nonlinear compensation action of gravitational forces in the joint space and a linear PD action in the image space. The
last term, in view of (10.82), corresponds to a derivative action in the image
space and has been added to increase damping. The resulting block scheme is
reported in Fig. 10.16.
The direct measurement of ṡ would permit the computation of the derivative term as −K Ds ṡ; this measurement, however, is not available. As an alternative, the derivative term can simply be set as −K D q̇, with K D symmetric
and positive deﬁnite (n × n) matrix.
Equation (10.85) reveals that, for all trajectories of the system, the Lyapunov function decreases until q̇ = 0. Therefore the system reaches an equilibrium state, characterized by
J TL (s, z c , q)K P s es = 0.

(10.86)

Equations (10.86), (10.83) show that, if the interaction matrix and the geometric Jacobian of the manipulator are full rank, then es = 0, which is the
sought result.

10.8 Image-based Visual Servoing

451

Fig. 10.17. Block scheme of resolved-velocity image-based visual servoing

Notice that control law (10.84) requires not only the measurement of s but
also the computation of vector z c which, in the image-based visual servoing
philosophy, should be avoided. In some applications z c is known with good
approximation, as in the case that the relative motion of the camera with
respect to the object belongs to a plane. Alternatively, estimated or constant
values can be used for z c , as the value in the initial conﬁguration or that in
) s of the
the desired conﬁguration. This is equivalent to using an estimate L
interaction matrix. In such cases, however, the stability proof becomes much
more complex.
10.8.2 Resolved-velocity Control
The concept of resolved-velocity control can easily be extended to the image
space. In such a case, Eq. (10.82) suggests the following choice of the reference
velocity in joint space
q̇ r = J −1
L (s, z c , q r )K s es ,

(10.87)

under the assumption of invertible matrix J L . This control law, replaced
in (10.82), yields the linear equation
ės + K s es = 0.

(10.88)

Therefore, if K s is a positive deﬁnite matrix, Eq. (10.88) is asymptotically stable and error es tends asymptotically to zero with convergence of exponential
type and speed depending on the eigenvalues of matrix K s . The convergence
to zero of the image space error es ensures the asymptotic convergence of xc,o
to the desired pose xd,o .
The block scheme of resolved-velocity image-based visual servoing is shown
in Fig. 10.17.
Notice that this control scheme requires the computation of the inverse of
matrix J L ; therefore it is aﬀected by problems related to the singularities of
this matrix which, in view of (10.83), are both those of the geometric Jacobian
and those of the interaction matrix. The most critical singularities are those

452

10 Visual Servoing

of the interaction matrix, since they depend on the choice of the image feature
parameters.
Therefore, it is convenient to compute control law (10.87) in two steps.
The ﬁrst step is the computation of vector
v cr = L−1
s (s, z c )K s es .

(10.89)

The second step is the computation of the joint space reference velocity using
the relationship


Rc O
q̇ r = J −1 (q)
(10.90)
v cr .
O Rc
Far from the kinematic singularities of the manipulator, the problem of
the singularities of the interaction matrix can be overcome by using a number
k of feature parameters greater than the minimum required m, similarly to
the case considered in Sect. 10.3.3. The control law can be modiﬁed by using
the left pseudo-inverse of interaction matrix Ls in place of the inverse, namely
v cr = (LTs Ls )−1 LTs K s es

(10.91)

in place of (10.89). Stability of the closed-loop system with control law (10.90),
(10.91) can be shown using the direct Lyapunov method based on the positive
deﬁnite function
V (es ) =

1 T
e K s es > 0
2 s

∀es = 0.

Computing the time derivative of this function and taking into account (10.82),
(10.83), (10.90), (10.91), yields
V̇ = −eTs K s Ls (LTs Ls )−1 LTs K s es
which is negative semi-deﬁnite because N (LTs ) = Ø, LTs being a matrix with
more columns than rows. Therefore, the closed-loop system is stable but not
asymptotically stable. This implies that the error is bounded, but in some
cases the system may reach an equilibrium with es = 0 and K s es ∈ N (LTs ).
Another problem connected with the implementation of control law (10.89)
or (10.91) and (10.90) depends on the fact that the computation of interaction
matrix Ls requires knowledge of z c . Similar to Sect. 10.8.1, this problem can
) −1 (or of its pseudo-inverse). In this
be solved by using an estimate of matrix L
s
case, the Lypapunov method can be used to prove that the control scheme
) −1 is positive deﬁnite. Notice that z c
remains stable provided that matrix Ls L
s
is the only information depending on object geometry. Therefore, it can also
be seen that image-based visual servoing, in the case that only one camera is
used, does not require exact knowledge of object geometry.
The choice of the elements of matrix K s inﬂuences the trajectories of the
feature parameters, which are solution to diﬀerential equation (10.88). In the

10.9 Comparison Among Various Control Schemes

453

case of feature points, if a diagonal matrix K s with equal elements is set, the
projections of these points on the image plane will follow line segments. The
corresponding camera motion, however, cannot be easily predicted, because of
the nonlinearity of the mapping between image plane variables and operational
space variables.

10.9 Comparison Among Various Control Schemes
In order to make a comparison between the various control schemes presented,
consider the SCARA manipulator of Example 10.3 and the planar object of
Example 10.1. The base frame of the SCARA manipulator of Fig. 2.36 is set
with the origin at the intersection of the axis of joint 1 with the horizontal
plane containing the origin of the end-eﬀector frame when d3 = 0, d3 being
the prismatic joint variable; the axis z of the base frame points downward.
The operational space, of dimension m = 4, is characterized by vector xc,o
in (10.33).
With reference to the dynamic model of Problem 7.3, the same data of
Example 7.2 are considered; in addition, m 3 = 2 kg and I 4 = 1 kg·m2 , while
the contribution of the motors of the last two links are neglected.
For position-based visual servoing schemes, the real-time estimation of
vector xc,o from a suitable feature vector is required. To this end, the algorithm of Sect. 10.3.3 can be used, based on the inverse of the image Jacobian.
This is a classical visual tracking application that can be accomplished using
only two feature points, because the corresponding Jacobian J As is a (4 × 4)
matrix. The selected points are P1 and P2 of the object of Fig. 10.6.
The same points can also be used for image-based visual servoing, because
the corresponding Jacobian J L is a (4 × 4) matrix.
It is assumed that at time t = 0 the pose of the camera frame, with respect
to the base frame, is deﬁned by the operational space vector
T

xc (0) = [ 1 1 1 π/4 ] m
and the pose of the object frame, with respect to the camera frame, is deﬁned
by the operational space vector
xc,o (0) = [ 0

0 0.5

T

0 ] m.

The desired pose of the object frame with respect to the camera frame is
deﬁned by vector
xd,o = [ −0.1

0.1

0.6

T

−π/3 ] m.

This quantity is assumed as the initial value of the pose estimation algorithm
used by position-based visual servoing schemes.

454

10 Visual Servoing

For image-based visual servoing schemes, the desired value of the feature
parameters of points P1 and P2 of the object, in the desired pose xd,o , is
sd = [ −0.1667

0.1667

−0.0833

T

0.0223 ] .

For all the schemes, a discrete-time implementation of the controller with
sampling time of 0.04 s has been adopted, corresponding to a 25 Hz frequency.
This value coincides with the minimum frame rate of analog cameras and
allows the use of visual measurements also in the worst case.
In the numerical simulations, the following control schemes have been utilized:
A. Position-based visual servoing of PD type with gravity compensation with
the following data:
K P = diag{500, 500, 10, 10}
K D = diag{500, 500, 10, 10}.
B. Resolved-velocity position-based visual servoing with the following data:
K = diag{1, 1, 1, 2},
corresponding to a time constant of 1 s for the three position variables and
of 0.5 s for the orientation variable.
C. Image-based visual servoing of PD type with gravity compensation with
the following data:
K P s = 300I 4

K Ds = 330I 4 .

D. Resolved-velocity image-based visual servoing with the following data:
K s = I 4,
corresponding to a time constant of 1 s for the feature parameters.
For the simulation of resolved-velocity control schemes, the dynamics of
the velocity controlled manipulator has been neglected. Therefore, a pure
kinematic model has been considered, based on the analytic Jacobian of the
manipulator.
For position-based control schemes, the pose estimation algorithm based
on the inverse of the image Jacobian has been adopted, with integration step
Δt = 1 ms and gain K s = 160I 4 . As shown in Example 10.4, this implies
that the algorithm converges in a time of about 0.03 s, which is lower than the
sampling time of the control, as required for a correct operation of positionbased control.
For image-based control schemes, matrix Ls (s, z c ) has been approximated
) s = Ls (s, zd ), where zd is the third component of vector xd,o .
with matrix L

10.9 Comparison Among Various Control Schemes

455

The parameters of the various controllers have been chosen in such a way
as to show the particular features of the diﬀerent control laws and, at the same
time, to allow a signiﬁcant comparison of the performance of each scheme in
response to congruent control actions. In particular, it can be observed what
follows:
• The gains in schemes A and C have been tuned in simulation so as to
obtain transient behaviors similar to those of schemes B and D.
• In control scheme B the gains of the position variables have been intentionally chosen equal to one another but diﬀerent from the gain of the
orientation variable to show that a desired dynamics can be imposed to
each operational space variable.
• In control scheme D the gains have all been chosen equal to one another,
since imposing diﬀerent dynamics to diﬀerent coordinates of the projections of the feature points on the image plane is not signiﬁcant.
The results obtained with the various control schemes are illustrated in
Figs. 10.18–10.25 in terms of:
• The time history of position and orientation of the camera frame with
respect to the base frame and corresponding desired values (represented
with dashed lines).
• The time history of feature parameters and corresponding desired values
(represented with dashed lines).
• The path of feature points projections on the camera image plane, from
initial positions (marked with crosses) to ﬁnal positions (marked with circles).
Regarding performance of the various control schemes, the following considerations can be drawn from the obtained results.
In principle, if position-based visual servoing is adopted, a desired transient behaviour can be assigned to the operational space variables. This is only
partially true for control scheme A, because the dynamics of the closed-loop
system, for PD control with gravity compensation, is nonlinear and coupled.
Therefore, the transient behaviour shown in Fig. 10.18 may be diﬀerent if
the manipulator starts from a diﬀerent initial pose or has to reach a diﬀerent
desired pose. Vice versa, for control scheme B, the time history of operational
space variables reported in Fig. 10.20 shows transient behaviours of exponential type, whose characteristics depend only on the choice of matrix K.
For both schemes A and B, the trajectories of the projections of the feature
points and the corresponding paths in the image plane (Figs. 10.19 and 10.21
respectively) have evolutions that cannot be predicted in advance. This implies
that, although the feature points projections are inside the image plane both in
the initial and in the desired conﬁguration, they may exit from the image plane
during the transient, thus causing problems of convergence to the controlled
system.

456

10 Visual Servoing
y−pos
1.4

1.3

1.3
[m]

[m]

x−pos
1.4

1.2
1.1

1.1

1
0

2

4
[s]

6

1
0

8

z−pos

2

0.4

4
[s]

6

8

6

8

alpha

2

0.5

1.5
[rad]

[m]

1.2

0.3

1

0.2
0.1
0

2

4
[s]

6

0.5
0

8

2

4
[s]

Fig. 10.18. Time history of camera frame position and orientation with control A
param

0.3

0.3

0.2

0.2

0.1

0.1

0

0

−0.1

−0.1

−0.2

−0.2

0

2

4
[s]

6

8

image plane

−0.2 −0.1

0

0.1 0.2

Fig. 10.19. Time history of feature parameters and corresponding path of feature
points projections on image plane with control A

If image-based control is adopted, a desired transient behaviour can be
assigned to the time histories of feature parameters and not to the operational
space variables, in a dual fashion with respect to position-based control. This
is conﬁrmed by the results shown in Figs. 10.22–10.25, relative to control
schemes C and D, respectively. In detail, especially in the case of control C,
the time histories of the operational space variables are quite diﬀerent from
those reported in Figs. 10.18 and 10.20, despite the same initial and ﬁnal
conﬁguration and a similar transient duration. This implies that the camera
path, being unpredictable, may lead to joint limits violation or to collision of

10.9 Comparison Among Various Control Schemes
y−pos

1.4

1.4

1.3

1.3
[m]

[m]

x−pos

1.2
1.1

1.2
1.1

1
0

2

4
[s]

6

1
0

8

z−pos

2

0.4

4
[s]

6

8

6

8

alpha

2

0.5

1.5
[rad]

[m]

457

0.3

1

0.2
0.1
0

2

4
[s]

6

0.5
0

8

2

4
[s]

Fig. 10.20. Time history of camera frame position and orientation with control B
param

0.3

0.3

0.2

0.2

0.1

0.1

0

0

−0.1

−0.1

−0.2

−0.2

0

2

4
[s]

6

8

image plane

−0.2 −0.1

0

0.1 0.2

Fig. 10.21. Time history of feature parameters and corresponding path of feature
points projections on image plane with control B

the manipulator with an obstacle. In the speciﬁc case of control scheme C, a
300% overshoot is present on the z component of the camera trajectory (see
Fig. 10.22), which corresponds to a camera retreat movement with respect to
the object of amplitude much higher than the distance reached at the end of
the transient. The overshoot on the z component is present also for control
scheme D, but is ‘only’ of 50% (see Fig. 10.24).
Notice that, for control scheme C, the presence of large displacements on
some operational space variables does not correspond to signiﬁcant deviations
of the feature parameters with respect to their ﬁnal values during the transient

458

10 Visual Servoing
y−pos
1.4

1.3

1.3
[m]

[m]

x−pos
1.4

1.2
1.1

1.1

1
0

2

4
[s]

6

1
0

8

z−pos

2

0.4

4
[s]

6

8

6

8

alpha

2

0.5

1.5
[rad]

[m]

1.2

0.3

1

0.2
0.1
0

2

4
[s]

6

0.5
0

8

2

4
[s]

Fig. 10.22. Time history of camera frame position and orientation with control C
param

0.3

0.3

0.2

0.2

0.1

0.1

0

0

−0.1

−0.1

−0.2

−0.2

0

2

4
[s]

6

8

image plane

−0.2 −0.1

0

0.1 0.2

Fig. 10.23. Time history of feature parameters and corresponding path of feature
points projections on image plane with control C

(see Fig. 10.23). Indeed, the paths of the feature points projections do not
deviate much from the line segments connecting these points.
Figure 10.25, relative to control scheme D, reveals that the trajectories
of the feature parameters are of exponential type. In this case, the transient
behaviour depends only on matrix K s ; choosing a diagonal matrix with equal
elements implies that the paths of the feature points projections are linear.
In the case at hand, in view of the approximation Ls (s, z c ) ≈ Ls (s, zd ), the
paths of the feature points projections shown in Fig. 10.25 are not perfectly
linear.

10.9 Comparison Among Various Control Schemes
y−pos

1.4

1.4

1.3

1.3
[m]

[m]

x−pos

1.2
1.1

1.2
1.1

1
0

2

4
[s]

6

1
0

8

z−pos

2

0.4

4
[s]

6

8

6

8

alpha

2

0.5

1.5
[rad]

[m]

459

0.3

1

0.2
0.1
0

2

4
[s]

6

0.5
0

8

2

4
[s]

Fig. 10.24. Time history of camera frame position and orientation with control D
param

0.3

0.3

0.2

0.2

0.1

0.1

0

0

−0.1

−0.1

−0.2

−0.2

0

2

4
[s]

6

8

image plane

−0.2 −0.1

0

0.1 0.2

Fig. 10.25. Time history of feature parameters and corresponding path of feature
points projections on image plane with control D

To conclude, Fig. 10.26 shows the paths of the origin of the camera frame
obtained using the four control schemes. It can be observed that, with control
scheme B, a perfectly linear path is obtained, thanks to the choice of a diagonal
gain matrix K s with equal weights for the positional part. Using control
scheme A, the path is almost linear, because, unlike case B, this type of
control does not guarantee a decoupled dynamics for each operational space
variable. Vice versa, using control schemes C and D, the path of the origin of
the camera frame is far from being linear. In both cases, the phenomenon of
camera retreat with respect to the object can be observed. To this end, notice

460

10 Visual Servoing
scheme B

0.5

0.5

0.4

0.4
[m]

[m]

scheme A

0.3
0.2
0.1
1.4

0.3
0.2

1.2
[m]

1

1

1.2
[m]

0.1
1.4

1.4

1.2
[m]

0.5

0.5

0.4

0.4
[m]

[m]

1

1.2
[m]

1.4

scheme D

scheme C

0.3

0.3
0.2

0.2
0.1
1.4

1

1.2
[m]

1

1

1.2
[m]

1.4

0.1
1.4

1.2
[m]

1

1

1.2
[m]

1.4

Fig. 10.26. Paths of camera frame origin in Cartesian space with the four control
schemes

the axis z of the base frame of the SCARA manipulator and the axis zc of
the camera frame are aligned to the vertical direction and point downward;
therefore, with respect to the reference frames of Fig. 10.26, the object is on
the top and the camera points upward.
The phenomenon of camera retreat appears whenever the camera is required to perform large rotations about the optical axis; this phenomenon
can be intuitively explained through a simple example. Assume that a pure
rotation of the camera about the axis zc is required and control scheme D
is adopted. Therefore, visual servoing imposes that the feature points projections on the image plane follow rectilinear paths from the initial to the desired
positions, whereas simple camera rotation would have required circular paths.
The constraint on the path in the image plane implies that, during rotation,
the origin of the camera frame must move, with respect to the object, ﬁrst
backward and then forward to reach again, asymptotically, the initial position.
It can be shown that, if the desired rotation tends to π, then the distance of
the camera from the object tends to ∞ and the system becomes unstable.

10.10 Hybrid Visual Servoing
An approach which combines the beneﬁts of position-based and image-based
visual servoing is hybrid visual servoing. The name stems from the fact that

10.10 Hybrid Visual Servoing

461

the control error is deﬁned in the operational space for some components and
in the image space for the others. This implies that a desired motion can be
speciﬁed, at least partially, in the operational space so that the camera trajectory during visual servoing can be predicted in advance for some components.
On the other hand, the presence of error components in the image space helps
keep the image features in the camera ﬁeld of view, which is a diﬃcult task
in position-based approaches.
Hybrid visual servoing requires the estimation of some operational space
variables. Assume that the object has a planar surface where at least four
feature points, and no triplets of collinear points, can be selected. Using the
coordinates of these points in the camera image plane, both in the current and
in the desired pose of the camera frame, it is possible to compute the planar
homography H as described in Sect. 10.4.4. Notice that, for this computation,
knowledge of the current and the desired camera pose is not required, provided
that the feature vectors s and sd are known.
In view of (10.59), assuming that Frame 1 coincides with the camera frame
in the current pose and Frame 2 coincides with the camera frame in the desired
pose, the following equality holds:
H = Rcd +

1 c dT
o n ,
dd c,d

where Rcd is the rotation matrix between the desired orientation and the
current orientation of the camera frame, occ,d is the position vector of the
origin of the camera frame in the desired pose with respect to the current
pose, nd is the unit vector normal to the plane containing the feature points,
and dd is the distance between this plane and the origin of the camera frame
in the desired pose. The quantities Rcd , nd , (1/dd )occ,d , in the current camera
pose, can be computed at each sampling time from matrix H.
Adopting a resolved-velocity approach, the control objective consists of
computing the reference absolute velocity of the camera frame
 c
νr
c
vr =
ω cr
from a suitably deﬁned error vector.
To this end, the orientation error between the desired and the current
camera pose can be computed from matrix Rcd , as for position-based visual
servoing. If φc,d denotes the vector of the Euler angles extracted from Rcd , the
control vector ω cr can be chosen as
ω cr = −T (φc,d )K o φc,d ,

(10.92)

where K o is a (3 × 3) matrix. With this choice, the equation of the orientation
error has the form
(10.93)
φ̇c,d + K o φc,d = 0.

462

10 Visual Servoing

Equation (10.93), if K o is a symmetric and positive deﬁnite matrix, implies
that the orientation error tends to zero asymptotically with convergence of
exponential type and speed depending on the eigenvalues of matrix K o .
The control vector ν cr should be selected so that the positional part of the
error between the desired and the current camera pose converges to zero. The
position error could be deﬁned as the diﬀerence of the coordinates of a point
of the object in the desired camera frame r cd = [ xd yd zd ]T and those in
the current camera frame r cc = [ xc yc zc ]T , namely r cd − r cc . These coordinates, however, cannot be directly measured, unlike the corresponding coordinates in the image plane, deﬁning the feature vectors sp,d = [ Xd Yd ]T =
[ xd /zd yd /zd ]T and sp = [ X Y ]T = [ xc /zc yc /zc ]T .
The information deriving from the computation of homography H can be
used to rewrite the ratio
ρz = zc /zd
in terms of known or measurable quantities in the form
ρz =
with

dc ndT sp,d
dd ncT sp

occ,d
dc
= 1 + ncT
= det(H),
dd
dd

(10.94)

(10.95)

and nc = Rcd nd , where vectors sp and sp,d denote the representations in
homogeneous coordinates of sp and sp,d , respectively (see Problem 10.12).
The position error, expressed in terms of known or measurable quantities,
can be deﬁned as
⎤
⎡
Xd − X
⎥
⎢
ep (r cd , r cc ) = ⎣ Yd − Y ⎦ .
lnρz
Notice that, in view of (5.44), convergence to zero of ep implies convergence
to zero of r cd − r cc and vice versa.
Computing the time derivative of ep yields
ėp =

∂ep (r cc ) c
ṙ c ,
∂r cc

r cd being constant. By taking into account (10.26) and the decomposition
 c
νc
c
vc =
ω cc
with ν cc = RTc ȯc , the above expression can be rewritten in the form
ėp = −J p ν cc − J o ω cc ,

(10.96)

10.10 Hybrid Visual Servoing
y−pos

1.4

1.4

1.3

1.3
[m]

[m]

x−pos

1.2
1.1
1
0

2

4
[s]

6

1
0

8

z−pos

2

4
[s]

6

8

6

8

alpha

2

0.4

1.5
[rad]

[m]

1.2
1.1

0.5

0.3

1

0.2
0.1
0

463

2

4
[s]

6

0.5
0

8

2

4
[s]

Fig. 10.27. Time history of position and orientation of camera frame with hybrid
visual servoing

with (see Problem 10.13)
⎡
Jp =

and

⎡

−1

1 ⎢
⎣ 0
zd ρz
0

XY
⎢
Jo = ⎣ 1 + Y 2
−Y

0
−1
0

−1 − X 2
−XY
X

X

⎤

⎥
Y ⎦
−1
⎤
Y
⎥
−X ⎦ .
0

Equation (10.96) suggests the following choice of control vector ν cr
c
ν cr = J −1
p (K p ep − J o ω r ),

(10.97)

J p being a nonsingular matrix.
Notice that, for the computation of J −1
p , knowledge of the constant quantity zd is required.
If zd is known, control law (10.97), in view of assumptions ȯcc ≈ ν cr and
c
ω c ≈ ω cr , yields the following error equation:
ėp + K p ep = 0,

464

10 Visual Servoing
param

0.3

0.3

0.2

0.2

0.1

0.1

0

0

−0.1

−0.1

−0.2

−0.2

0

2

4
[s]

6

8

image plane

−0.2 −0.1

0

0.1 0.2

Fig. 10.28. Time history of feature parameters and corresponding path of feature
points projections on image plane with hybrid visual servoing

which implies the exponential convergence of ep to zero, provided that K p is
a positive deﬁnite matrix.
If zd is not known, an estimate z)d can be adopted. Therefore, in control
) −1
law (10.97), matrix J −1
p can be replaced by an estimate J p . In view of the
equality
) −1 = z)d J −1 ,
J
p
zd p
the following error equation is obtained:


z)d
z)d
ėp + K p ep = 1 −
J o ω cr .
zd
zd
This equation shows that the use of an estimate z)d in place of the true value
zd implies a simple gain scaling in the error equation, and asymptotic stability
is preserved. Moreover, due to the presence in the right-hand side of the above
equation of a term depending on ω cr , the time history of ep is inﬂuenced by
the orientation error, which evolves according to (10.93).

Example 10.6
For the SCARA manipulator and the task of Sect. 10.9, consider the hybrid visual
servoing law with gains
K p = I 3 ko = 2,
and compute the positional part of the error with respect to point P1 . The planar
homography and the corresponding parameters are estimated as in Example 10.5,
using four points. The results are reported in Figs. 10.27 and 10.28, in terms of the
same variables shown in Sect. 10.9. Notice that the time histories of the variables in
the operational space of Fig. 10.27 are quite similar to that obtained with resolvedvelocity position-based visual servoing (Fig. 10.20). On the other hand, the time
histories of the feature parameters of Fig. 10.28 are substantially similar to those
obtained with resolved-velocity image-based visual servoing (Fig. 10.25) except for

Bibliography

465

0.5

[m]

0.4
0.3
0.2
0.1
1.4

1.2
[m]

1

1

1.2
[m]

1.4

Fig. 10.29. Path of camera frame origin in Cartesian space with hybrid visual
servoing

the fact that the path in the image plane of the projection of point P1 is perfectly
linear, as imposed by the control. The corresponding path of the camera frame
origin, reported in Fig. 10.29, shows a substantial improvement with respect to
those obtained with the image-based visual servoing schemes of Fig. 10.26.

The method illustrated above is only one of the possible visual servoing
approaches based on the computation of the planar homography and on its
decomposition. It is worth pointing out that knowledge of (1/dd )occ,d and Rcd
allows the computation of the operational space error (10.69), up to a scaling
factor for the positional part; therefore, it is also possible to use the positionbased visual servoing schemes presented in Sect. 10.7. On the other hand, in
hybrid visual servoing approaches, diﬀerent choices are possible for the error
components depending on feature parameters as well as for those depending
on the operational space variables.

Bibliography
The literature dealing with computational vision is quite extensive and various. Image processing is treated, e.g., in [93], while geometrical issues are
considered in [75] and [146]. The concept of interaction matrix was originally
proposed in [239] under a diﬀerent name, while the actual name was introduced in [71]. The pose estimation algorithm based on the inverse of the image
Jacobian is also termed virtual visual servoing [47]. Position-based visual servoing was proposed in [239] and, more recently, in [244] and [134]. Several
papers dealing with image-based visual servoing can be found, starting from
early works of [239] and [79]. A rigorous stability analysis is reported in [108]
for PD control with gravity compensation and in [71] for resolved-velocity
control. Hybrid visual servoing, presented in [148], is only one of advanced
control schemes based on decoupling of the camera DOFs, e.g., the partitioned approach [49] and the control based on image moments [35]. Finally,

466

10 Visual Servoing

visual servoing based on stereo vision is considered in [92]. An interesting
review of the state of the art of visual servoing until mid 1990s is presented
in [103], while a more recent review can be found in [36].

Problems
10.1. Derive Eq. (10.2).
10.2. Show that a non-null solution to (10.10) is the right eigenvector corresponding to the null singular value of matrix A.
10.3. Show that (10.12) is the matrix which minimizes Frobenius norm (10.11)
with the constraint that Rco is a rotation matrix, in the case σ > 0. [Hint: conT
T cT
sider that Tr(RcT
o U ΣV ) = Tr(V Ro U Σ); moreover, the absolute values
of the diagonal elements of matrix V T RcT
o U are lower or equal to 1.]
10.4. Consider the SCARA manipulator of Example 10.3. Perform a computer implementation of the pose estimation algorithm based on the inverse
of the image Jacobian considering the points P1 and P2 of the object of Example 10.1. Compute the homogeneous transformation matrix corresponding to
T
the feature vector s = [ −0.1667 0.1667 −0.0833 0.0223 ] . Assume that,
in the initial pose, the axes of the camera frame are parallel to those of the
object frame and the origin is at a distance of 0.5 m along the vertical axis.
10.5. Solve the previous problem using the feature parameters of the line
segment P1 P2 .
10.6. Show that the value of o which minimizes function (10.55) has expres

sion (10.56). [Hint: Use the equalities pi = p̄i + p̄ and
p̄ in (10.55),
/pni = p̄i +/
n
2
2
T
2
and the properties a+b = a +2a b+b and i=1 p̄i = i=1 p̄i = 0.]
10.7. Show that the matrix R which minimizes Frobenius norm (10.57) is the
matrix which maximizes the trace of RK.
10.8. For the SCARA manipulator of Example 10.3, design a position-based
visual servoing scheme of PD type with gravity compensation using the measurement of the feature parameters of line segment P1 P2 of Example 10.1.
Perform a computer simulation in the same operating conditions of Sect. 10.9
and compare the results.
10.9. For the SCARA manipulator of Example 10.3, design a resolved-velocity
position-based visual servoing scheme using the measurement of the feature
parameters of line segment P1 P2 of Example 10.1. Perform a computer simulation in the same operating conditions of Sect. 10.9 and compare the results.

Problems

467

10.10. For the SCARA manipulator of Example 10.3, design an image-based
visual servoing scheme of PD type with gravity compensation using the measurement of the feature parameters of the line segment P1 P2 of Example 10.1.
Perform a computer simulation in the same operating conditions of Sect. 10.9
and compare the results.
10.11. For the SCARA manipulator of Example 10.3, design a resolvedvelocity image-based visual servoing scheme using the measurement of the
feature parameters of line segment P1 P2 of Example 10.1. Perform a computer simulation in the same operating conditions of Sect. 10.9 and compare
the results.
10.12. Derive the expressions (10.94), (10.95).
10.13. Derive the expressions of J p and J o in (10.96).

11
Mobile Robots

The previous chapters deal mainly with articulated manipulators that represent the large majority of robots used in industrial settings. However, mobile
robots are becoming increasingly important in advanced applications, in view
of their potential for autonomous intervention. This chapter presents techniques for modelling, planning and control of wheeled mobile robots. The
structure of the kinematic constraints arising from the pure rolling of the
wheels is ﬁrst analyzed; it is shown that such constraints are in general nonholonomic and consequently reduce the local mobility of the robot. The kinematic model associated with the constraints is introduced to describe the
instantaneous admissible motions, and conditions are given under which it
can be put in chained form. The dynamic model , that relates the admissible
motions to the generalized forces acting on the robot DOFs, is then derived.
The peculiar nature of the kinematic model, and in particular the existence of
ﬂat outputs, is exploited to devise trajectory planning methods that guarantee
that the nonholonomic constraints are satisﬁed. The structure of minimumtime trajectories is also analyzed. The motion control problem for mobile
robots is then discussed, with reference to two basic motion tasks, i.e., trajectory tracking and posture regulation. The chapter concludes by surveying some
techniques for odometric localization that is necessary to implement feedback
control schemes.

11.1 Nonholonomic Constraints
Wheels are by far the most common mechanism to achieve locomotion in
mobile robots. Any wheeled vehicle is subject to kinematic constraints that
reduce in general its local mobility, while leaving intact the possibility of
reaching arbitrary conﬁgurations by appropriate manoeuvres. For example,
any driver knows by experience that, while it is impossible to move instantaneously a car in the direction orthogonal to its heading, it is still possible to

470

11 Mobile Robots

park it arbitrarily, at least in the absence of obstacles. It is therefore important
to analyze in detail the structure of these constraints.
In accordance with the terminology introduced in Sect. B.4, consider a
mechanical system whose conﬁguration q ∈ C is described by a vector of
generalized coordinates, and assume that the conﬁguration space C (i.e., the
space of all possible robot conﬁgurations) coincides1 with IRn . The motion of
the system that is represented by the evolution of q over time may be subject to constraints that can be classiﬁed under various criteria. For example,
they may be expressed as equalities or inequalities (respectively, bilateral or
unilateral constraints), and they may depend explicitly on time or not (rheonomic or scleronomic constraints). In this chapter, only bilateral scleronomic
constraints will be considered.
Constraints that can be put in the form
hi (q) = 0

i = 1, . . . , k < n

(11.1)

are called holonomic (or integrable). In the following, it is assumed that the
functions hi : C → IR are of class C ∞ (smooth) and independent. The eﬀect
of holonomic constraints is to reduce the space of accessible conﬁgurations to
a subset of C with dimension n − k. A mechanical system for which all the
constraints can be expressed in the form (11.1) is called holonomic.
In the presence of holonomic constraints, the implicit function theorem can
be used in principle to solve the equations in (11.1) by expressing k generalized
coordinates as a function of the remaining n − k, so as to eliminate them from
the formulation of the problem. However, in general this procedure is only
valid locally, and may introduce singularities. A convenient alternative is to
replace the original generalized coordinates with a reduced set of n − k new
coordinates that are directly deﬁned on the accessible subspace, in such a
way that the available DOFs are eﬀectively characterized. The mobility of the
reduced system thus obtained is completely equivalent to that of the original
mechanism.
Holonomic constraints are generally the result of mechanical interconnections between the various bodies of the system. For example, prismatic and
revolute joints used in robot manipulators are a typical source of such constraints, and joint variables are an example of reduced sets of coordinates in
the above sense. Constraints of the form (11.1) may also arise in particular
operating conditions; for example, one may mention the case of a kinematically redundant manipulator that moves while keeping the end-eﬀector ﬁxed
at a certain pose (self-motion).
Constraints that involve generalized coordinates and velocities
ai (q, q̇) = 0
1

i = 1, . . . , k < n

This assumption is taken for simplicity. In the general case, the conﬁguration
space C may be identiﬁed with a Euclidean space only on a local basis, because
its global geometric structure is more complex; this will be further discussed in
Chap. 12. The material presented in this chapter is, however, still valid.

11.1 Nonholonomic Constraints

471

are called kinematic. They constrain the instantaneous admissible motion of
the mechanical system by reducing the set of generalized velocities that can be
attained at each conﬁguration. Kinematic constraints are generally expressed
in Pfaﬃan form, i.e., they are linear in the generalized velocities:
aTi (q)q̇ = 0

i = 1, . . . , k < n,

(11.2)

or, in matrix form
AT (q)q̇ = 0.

(11.3)

Vectors ai : C → IR are assumed to be smooth as well as linearly independent.
Clearly, the existence of k holonomic constraints (11.1) implies that of an
equal number of kinematic constraints:
n

dhi (q)
∂hi (q)
=
q̇ = 0
dt
∂q

i = 1, . . . , k.

However, the converse is not true in general. A system of kinematic constraints
in the form (11.3) may or may not be integrable to the form (11.1). In the
negative case, the kinematic constraints are said to be nonholonomic (or nonintegrable). A mechanical system that is subject to at least one such constraint
is called nonholonomic.
Nonholonomic constraints reduce the mobility of the mechanical system
in a completely diﬀerent way with respect to holonomic constraints. To appreciate this fact, consider a single Pfaﬃan constraint
aT (q)q̇ = 0.

(11.4)

If the constraint is holonomic, it can be integrated and written as
h(q) = c,

(11.5)

where ∂h/∂q = γ(q) aT (q), with γ(q) = 0 an integrating factor and c an
integration constant. Therefore, there is a loss of accessibility in the conﬁguration space, because the motion of the mechanical system in C is conﬁned to
a particular level surface of the scalar function h. This surface, which depends
on the initial conﬁguration q 0 through the value of h(q 0 ) = c, has dimension
n − 1.
Assume instead that the constraint (11.4) is nonholonomic. In this case,
generalized velocities are indeed constrained to belong to a subspace of dimension n − 1, i.e., the null space of matrix aT (q). Nevertheless, the fact that
the constraint is non-integrable means that there is no loss of accessibility
in C for the system. In other words, while the number of DOFs decreases to
n − 1 due to the constraint, the number of generalized coordinates cannot be
reduced, not even locally.
The conclusion just drawn for the case of a single constraint is general.
An n-dimensional mechanical system subject to k nonholonomic constraints

472

11 Mobile Robots



r
?

Fig. 11.1. Generalized coordinates for a disk rolling on a plane

can access its whole conﬁguration space C, although at any conﬁguration its
generalized velocities must belong to an (n − k)-dimensional subspace.
The following is a classical example of nonholonomic mechanical system,
that is particularly relevant in the study of mobile robots.

Example 11.1
Consider a disk that rolls without slipping on the horizontal plane, while keeping
its sagittal plane (i.e., the plane that contains the disk) in the vertical direction
(Fig. 11.1). Its conﬁguration is described by three2 generalized coordinates: the
Cartesian coordinates (x, y) of the contact point with the ground, measured in a
ﬁxed reference frame, and the angle θ characterizing the orientation of the disk with
respect to the x axis. The conﬁguration vector is therefore q = [ x y θ ]T .
The pure rolling constraint for the disk is expressed in the Pfaﬃan form as
ẋ sin θ − ẏ cos θ = [ sin θ

−cos θ

0 ] q̇ = 0,

(11.6)

and entails that, in the absence of slipping, the velocity of the contact point has zero
component in the direction orthogonal to the sagittal plane. The angular velocity of
the disk around the vertical axis instead is unconstrained.
Constraint (11.6) is nonholonomic, because it implies no loss of accessibility in
the conﬁguration space of the disk. To substantiate this claim, consider that the
disk can be driven from any initial conﬁguration q i = [ xi yi θi ]T to any ﬁnal
conﬁguration q f = [ xf yf θf ]T through the following sequence of movements
that do not violate constraint (11.6):
1. rotate the disk around its vertical axis so as to reach the orientation θv for which
the sagittal axis (i.e., the intersection of the sagittal plane and the horizontal
plane) goes through the ﬁnal contact point (xf , yf );
2. roll the disk on the plane at a constant orientation θv until the contact point
reaches its ﬁnal position (xf , yf );
3. rotate again the disk around its vertical axis to change the orientation from θv
to θf .
2

One could add to this description an angle φ measuring the rotation of the disk
around the horizontal axis passing through its centre. Such a coordinate is however
irrelevant for the analysis presented in this chapter, and is therefore ignored in
the following.

11.1 Nonholonomic Constraints

473



r
R7

RW

?
Fig. 11.2. A local representation of the conﬁguration space for the rolling disk with
an example manoeuvre that transfers the conﬁguration from q i to q f (dashed line)

An example of this manoeuvre is shown in Fig. 11.2. Two possible directions of
instantaneous motion are shown at each conﬁguration: the ﬁrst, that is aligned with
the sagittal axis, moves the contact point while keeping the orientation constant
(rolling); the second varies the orientation while keeping the contact point ﬁxed
(rotation around the vertical axis).

It is interesting to note that, in addition to wheeled vehicles, there exist
other robotic systems that are nonholonomic in nature. For example, the pure
rolling constraint also arises in manipulation problems with round-ﬁngered
robot hands. Another kind of nonholonomic behaviour is found in multibody
systems that ‘ﬂoat’ freely (i.e., without a ﬁxed base), such as manipulators
used in space operations. In fact, in the absence of external generalized forces,
the conservation of the angular momentum represents a non-integrable Pfafﬁan constraint for the system.
11.1.1 Integrability Conditions
In the presence of Pfaﬃan kinematic constraints, integrability conditions can
be used to decide whether the system is holonomic or nonholonomic.
Consider ﬁrst the case of a single Pfaﬃan constraint:
aT (q)q̇ =

n


aj (q)q̇j = 0.

(11.7)

j=1

For this constraint to be integrable, there must exist a scalar function h(q)
and an integrating factor γ(q) = 0 such that the following condition holds:
γ(q)aj (q) =

∂h(q)
∂qj

j = 1, . . . , n.

(11.8)

474

11 Mobile Robots

The converse is also true: if there exists an integrating factor γ(q) = 0 such
that γ(q)a(q) is the gradient of a scalar function h(q), constraint (11.7) is
integrable. By using Schwarz theorem on the symmetry of second derivatives,
the integrability condition (11.8) may be replaced by the following system of
partial diﬀerential equations:
∂(γaj )
∂(γak )
=
∂qj
∂qk

j, k = 1, . . . , n,

j = k,

(11.9)

that does not contain the unknown function h(q). Note that condition (11.9)
implies that a Pfaﬃan constraint with constant coeﬃcients aj is always holonomic.

Example 11.2
Consider the following kinematic constraint in C = IR3 :
q̇1 + q1 q̇2 + q̇3 = 0.
The holonomy condition (11.9) gives
∂γ
∂γ
= γ + q1
∂q2
∂q1
∂γ
∂γ
=
∂q3
∂q1
∂γ
∂γ
q1
=
.
∂q3
∂q2
By substituting the second and third equations into the ﬁrst, it is easy to conclude
that the only solution is γ = 0. Therefore, the constraint is nonholonomic.
Example 11.3
Consider the pure rolling constraint (11.6). In this case, the holonomy condition (11.9) gives
∂γ
∂γ
= −cos θ
∂y
∂x
∂γ
cos θ
= γ sin θ
∂θ
∂γ
sin θ
= −γ cos θ.
∂θ
Squaring and adding the last two equations gives ∂γ/∂θ = ±γ. Assume for example
∂γ/∂θ = γ. Using this in the above equations leads to
sin θ

γ cos θ = γ sin θ
γ sin θ = −γ cos θ
whose only solution is γ = 0. The same conclusion is reached by letting ∂γ/∂θ = −γ.
This conﬁrms that constraint (11.6) is nonholonomic.

11.1 Nonholonomic Constraints

475

The situation becomes more complicated when dealing with a system of
k > 1 kinematic constraints of the form (11.3). In fact, in this case it may
happen that the single constraints are not integrable if taken separately, but
the whole system is integrable. In particular, if p ≤ k independent linear
combinations of the constraints
k


γji (q)aTi (q)q̇

j = 1, . . . , p

i=1

are integrable, there exist p independent scalar functions hj (q) such that

*
1
0
∂h1 (q)
∂hp (q)
,...,
∀q ∈ C.
span
⊂ span aT1 (q), . . . , aTk (q)
∂q
∂q
Therefore, the conﬁgurations that are accessible for the mechanical system
belong to the (n − p)-dimensional subspace consisting of the particular level
surfaces of the functions hj :
{q ∈ C: h1 (q) = c1 , . . . , hp (q) = cp }
on which the motion is started (see Problem 11.2). In the case p = k, the
system of kinematic constraints (11.3) is completely integrable, and hence
holonomic.

Example 11.4
Consider the system of Pfaﬃan constraints
q̇1 + q1 q̇2 + q̇3 = 0
q̇1 + q̇2 + q1 q̇3 = 0.
Taken separately, these constraints are found to be non-integrable (in particular,
the ﬁrst is the nonholonomic constraint of Example 11.2). However, subtracting the
second from the ﬁrst gives
(q1 − 1)(q̇2 − q̇3 ) = 0
so that q̇2 = q̇3 , because the constraints must be satisﬁed for any value of q. The
assigned system of constraints is then equivalent to
q̇2 = q̇3
q̇1 + (1 + q1 )q̇2 = 0,
which can be integrated as
q2 − q3 = c1
log(q1 + 1) + q2 = c2
with integration constants c1 , c2 .

476

11 Mobile Robots

The integrability conditions of a system of Pfaﬃan kinematic constraints
are quite complex and derive from a fundamental result of diﬀerential geometry known as Frobenius theorem. However, as shown later in the chapter, it is
possible to derive such conditions more directly from a diﬀerent perspective.

11.2 Kinematic Model
The system of k Pfaﬃan constraints (11.3) entails that the admissible generalized velocities at each conﬁguration q belong to the (n − k)-dimensional
null space of matrix AT (q). Denoting by {g 1 (q), . . . , g n−k (q)} a basis of
N (AT (q)), the admissible trajectories for the mechanical system can then
be characterized as the solutions of the nonlinear dynamic system
q̇ =

m


g j (q)uj = G(q)u

m = n − k,

(11.10)

j=1

where q ∈ IRn is the state vector and u = [ u1 . . . um ]T ∈ IRm is the input
vector. System (11.10) is said to be driftless because one has q̇ = 0 if the
input is zero.
The choice of the input vector ﬁelds g 1 (q), . . . , g m (q) (and thus of matrix G(q)) in (11.10) is not unique. Correspondingly, the components of u
may have diﬀerent meanings. In general, it is possible to choose the basis of
N (AT (q)) in such a way that the uj s have a physical interpretation, as will
be shown later for some examples of mobile robots. In any case, vector u may
not be directly related to the actual control inputs, that are in general forces
and/or torques. For this reason, Eq. (11.10) is referred to as the kinematic
model of the constrained mechanical system.
The holonomy or nonholonomy of constraints (11.3) can be established
by analyzing the controllability3 properties of the associated kinematic model
(11.10). In fact, two cases are possible:
1. If system (11.10) is controllable, given two arbitrary conﬁgurations q i and
q f in C, there exists a choice of u(t) that steers the system from q i to q f ,
i.e., there exists a trajectory that joins the two conﬁgurations and satisﬁes
the kinematic constraints (11.3). Therefore, these do not aﬀect in any way
the accessibility of C, and they are (completely) nonholonomic.
2. If system (11.10) is not controllable, the kinematic constraints (11.3) reduce the set of accessible conﬁgurations in C. Hence, the constraints are
partially or completely integrable depending on the dimension ν < n of
the accessible conﬁguration space. In particular:
3

Refer to Appendix D for a short survey of nonlinear controllability theory, including the necessary tools from diﬀerential geometry.

11.2 Kinematic Model

477

2a. If m < ν < n, the loss of accessibility is not maximal, and thus constraints (11.3) are only partially integrable. The mechanical system is
still nonholonomic.
2b. If ν = m, the loss of accessibility is maximal, and constraints (11.3) are
completely integrable. Therefore, the mechanical system is holonomic.
Note how this particular viewpoint, i.e., the equivalence between controllability and nonholonomy, was already implicitly adopted in Example 11.1,
where the controllability of the kinematic system was proven constructively,
i.e., by exhibiting a reconﬁguration manoeuvre. A more systematic approach
is to take advantage of the controllability conditions for nonlinear driftless
systems. In particular, controllability may be veriﬁed using the accessibility
rank condition
(11.11)
dim ΔA (q) = n,
where ΔA is the accessibility distribution associated with system (11.10), i.e.,
the involutive closure of distribution Δ = span{g 1 , . . . , g m }. The following
cases may occur:
1. If (11.11) holds, system (11.10) is controllable and the kinematic constraints (11.3) are (completely) nonholonomic.
2. If (11.11) does not hold, system (11.10) is not controllable and the kinematic constraints (11.3) are at least partially integrable. In particular, let
dim ΔA (q) = ν < n.
Then
2a. If m < ν < n, constraints (11.3) are only partially integrable.
2b. If ν = m, constraints (11.3) are completely integrable, and hence holonomic. This happens when ΔA coincides with Δ = span{g 1 , . . . , g m },
i.e., when the latter distribution is involutive.
It is easy to verify that, in the case of a single kinematic constraint (11.7),
the integrability condition given by (11.9) is equivalent to the involutivity of
Δ = span{g 1 , . . . , g n−1 }. Another remarkable situation is met when the number of Pfaﬃan constraints is k = n − 1; in this case, the associated kinematic
model (11.10) consists of a single vector ﬁeld g (m = 1). Hence, n − 1 Pfaﬃan
constraints are always integrable, because the distribution associated with a
single vector ﬁeld is always involutive. For example, a mechanical system with
two generalized coordinates that is subject to a scalar Pfaﬃan constraint is
always holonomic.
In the following, the kinematic models of two wheeled vehicles of particular
interest will be analyzed in detail. A large part of the existing mobile robots
have a kinematic model that is equivalent to one of these two.

478

11 Mobile Robots

r
K
4



F1


?

Fn

Fig. 11.3. Generalized coordinates for a unicycle

11.2.1 Unicycle
A unicycle is a vehicle with a single orientable wheel. Its conﬁguration is
completely described by q = [ x y θ ]T , where (x, y) are the Cartesian coordinates of the contact point of the wheel with the ground (or equivalently,
of the wheel centre) and θ is the orientation of the wheel with respect to the
x axis (see Fig. 11.3).
As already seen in Example 11.1, the pure rolling constraint for the wheel
is expressed as
ẋ sin θ − ẏ cos θ = [ sin θ

−cos θ

0 ] q̇ = 0,

(11.12)

entailing that the velocity of the contact point is zero in the direction orthogonal to the sagittal axis of the vehicle. The line passing through the contact
point and having such direction is therefore called zero motion line. Consider
the matrix
⎡
⎤
cos θ 0
G(q) = [ g 1 (q) g 2 (q) ] = ⎣ sin θ 0 ⎦ ,
0
1
whose columns g 1 (q) and g 2 (q) are, for each q, a basis of the null space of the
matrix associated with the Pfaﬃan constraint. All the admissible generalized
velocities at q are therefore obtained as a linear combination of g 1 (q) and
g 2 (q). The kinematic model of the unicycle is then
⎤
⎡ ⎤
⎡ ⎤ ⎡
ẋ
cos θ
0
⎣ ẏ ⎦ = ⎣ sin θ ⎦ v + ⎣ 0 ⎦ ω,
(11.13)
0
1
θ̇
where the inputs v and ω have a clear physical interpretation. In particular,
v is the driving velocity, i.e., the modulus4 (with sign) of the contact point
4

Note that v is given by the angular speed of the wheel around its horizontal axis
multiplied by the wheel radius.

11.2 Kinematic Model

479

velocity vector, whereas the steering velocity ω is the wheel angular speed
around the vertical axis.
The Lie bracket of the two input vector ﬁelds is
⎡
⎤
sin θ
[g 1 , g 2 ](q) = ⎣ −cos θ ⎦ ,
0
that is always linearly independent from g 1 (q), g 2 (q). Therefore, the iterative
procedure (see Sect. D.2) for building the accessibility distribution ΔA ends
with
dim ΔA = dim Δ2 = dim span{g 1 , g 2 , [g 1 , g 2 ]} = 3.
This indicates that the unicycle is controllable with degree of nonholonomy
κ = 2, and that constraint (11.12) is nonholonomic — the same conclusion
reached in Example 11.3 by applying the integrability condition.
A unicycle in the strict sense (i.e., a vehicle equipped with a single wheel)
is a robot with a serious problem of balance in static conditions. However,
there exist vehicles that are kinematically equivalent to a unicycle but more
stable from a mechanical viewpoint. Among these, the most important are
the diﬀerential drive and the synchro drive vehicles, already introduced in
Sect. 1.2.2.
For the diﬀerential drive mobile robot of Fig. 1.13, denote by (x, y) the
Cartesian coordinates of the midpoint of the segment joining the two wheel
centres, and by θ the common orientation of the ﬁxed wheels (hence, of the
vehicle body). Then, the kinematic model (11.13) of the unicycle also applies
to the diﬀerential drive vehicle, provided that the driving and steering velocities v and ω are expressed as a function of the actual velocity inputs, i.e.,
the angular speeds ωR and ωL of the right and left wheel, respectively. Simple
arguments (see Problem 11.6) can be used to show that there is a one-to-one
correspondence between the two sets of inputs:
r (ωR − ωL )
r (ωR + ωL )
ω=
,
(11.14)
2
d
where r is the radius of the wheels and d is the distance between their centres.
The equivalence with the kinematic model (11.13) is even more straightforward for the synchro drive mobile robot of Fig. 1.14, whose control inputs
are indeed the driving velocity v and the steering velocity ω, that are common
to the three orientable wheels. The Cartesian coordinates (x, y) may represent in this case any point of the robot (for example, its centroid), while θ is
the common orientation of the wheels. Note that, unlike a diﬀerential drive
vehicle, the orientation of the body of a synchro drive vehicle never changes,
unless a third actuator is added for this speciﬁc purpose.
v=

11.2.2 Bicycle
Consider now a bicycle, i.e., a vehicle having an orientable wheel and a ﬁxed
wheel arranged as in Fig. 11.4. A possible choice for the generalized coordi-

480

11 Mobile Robots

*

r
g
f


?
Fig. 11.4. Generalized coordinates and instantaneous centre of rotation for a bicycle

nates is q = [ x y θ φ ]T , where (x, y) are the Cartesian coordinates of the
contact point between the rear wheel and the ground (i.e., of the rear wheel
centre), θ is the orientation of the vehicle with respect to the x axis, and φ is
the steering angle of the front wheel with respect to the vehicle.
The motion of the vehicle is subject to two pure rolling constraints, one
for each wheel:
ẋf sin (θ + φ) − ẏf cos (θ + φ) = 0
ẋ sin θ − ẏ cos θ = 0,

(11.15)
(11.16)

where (xf , yf ) is the Cartesian position of the centre of the front wheel. The
geometric meaning of these constraints is obvious: the velocity of the centre of
the front wheel is zero in the direction orthogonal to the wheel itself, while the
velocity of the centre of the rear wheel is zero in the direction orthogonal to
the sagittal axis of the vehicle. The zero motion lines of the two wheels meet
at a point C called instantaneous centre of rotation (Fig. 11.4), whose position
depends only on (and changes with) the conﬁguration q of the bicycle. Each
point of the vehicle body then moves instantaneously along an arc of circle
with centre in C (see also Problem 11.7).
Using the rigid body constraint
xf = x +  cos θ
yf = y +  sin θ,
where  is the distance between the wheels, constraint (11.15) can be rewritten
as
ẋ sin (θ + φ) − ẏ cos (θ + φ) −  θ̇ cos φ = 0.
(11.17)
The matrix associated with the Pfaﬃan constraints (11.16), (11.17) is then


sin θ
−cos θ
0
0
T
A (q) =
,
sin (θ + φ) −cos (θ + φ) − cos φ 0

11.2 Kinematic Model

481

with constant rank k = 2. The dimension of its null space is n − k = 2, and
all the admissible velocities at q may be written as a linear combination of a
basis of N (AT (q)), for example
⎡ ⎤ ⎡
⎤
⎡ ⎤
ẋ
cos θ cos φ
0
⎢ ẏ ⎥ ⎢ sin θ cos φ ⎥
⎢0⎥
⎢ ⎥=⎣
⎦ u1 + ⎣ ⎦ u2 .
⎣ θ̇ ⎦
sin φ/
0
0
1
φ̇
Since the front wheel is orientable, it is immediate to set u2 = ω, where ω is
the steering velocity. The expression of u1 depends instead on how the vehicle
is driven.
If the bicycle has front-wheel drive, one has directly u1 = v, where v is the
driving velocity of the front wheel. The corresponding kinematic model is
⎡ ⎤ ⎡
⎤
⎡ ⎤
ẋ
cos θ cos φ
0
⎢ ẏ ⎥ ⎢ sin θ cos φ ⎥
0⎥
⎢
⎢ ⎥=⎣
(11.18)
⎦ v + ⎣ ⎦ ω.
⎣ θ̇ ⎦
sin φ/
0
0
1
φ̇
Denoting by g 1 (q) and g 2 (q) the two input vector ﬁelds, simple computations
give
⎡

⎤
cos θ sin φ
⎢ sin θ sin φ ⎥
g 3 (q) = [g 1 , g 2 ](q) = ⎣
⎦
−cos φ/
0

⎡

⎤
−sin θ/
⎢ cos θ/ ⎥
g 4 (q) = [g 1 , g 3 ](q) = ⎣
⎦,
0
0

both linearly independent from g 1 (q) and g 2 (q). Hence, the iterative procedure for building the accessibility distribution ΔA ends with
dim ΔA = dim Δ3 = dim span{g 1 , g 2 , g 3 , g 4 } = 4.
This means that the front-wheel drive bicycle is controllable with degree of
nonholonomy κ = 3, and constraints (11.15), (11.16) are (completely) nonholonomic.
The kinematic model of a bicycle with rear-wheel drive can be derived by
noting that in this case the ﬁrst two equations must coincide with those of
the unicycle model (11.13). It is then suﬃcient to set u1 = v/cos φ to obtain
⎡ ⎤ ⎡
⎤
⎡ ⎤
ẋ
cos θ
0
⎢ ẏ ⎥ ⎢ sin θ ⎥
0⎥
⎢
⎢ ⎥=⎣
(11.19)
⎦ v + ⎣ ⎦ ω,
⎣ θ̇ ⎦
tan φ/
0
0
1
φ̇

482

11 Mobile Robots

where v is the driving velocity of the rear wheel.5 In this case, one has
⎡
sin θ ⎤
⎡
⎤
−
0
⎢  cos 2 φ ⎥
⎥
⎢
⎢
⎥
0
⎢
⎢ cos θ ⎥
⎥
⎥
⎢
⎢
⎥
1
g 3 (q) = [g 1 , g 2 ](q) = ⎢ −
⎥ g 4 (q) = [g 1 , g 3 ](q) = ⎢  cos 2 φ ⎥ ,
⎥
⎢
⎣  cos 2 φ ⎦
⎦
⎣
0
0
0
again linearly independent from g 1 (q) and g 2 (q). Hence, the rear-wheel drive
bicycle is also controllable with degree of nonholonomy κ = 3.
Like the unicycle, the bicycle is also unstable in static conditions. Kinematically equivalent vehicles that are mechanically balanced are the tricycle
and the car-like robot, introduced in Sect. 1.2.2 and shown respectively in
Fig. 1.15 and 1.16. In both cases, the kinematic model is given by (11.18) or
by (11.19) depending on the wheel drive being on the front or the rear wheels.
In particular, (x, y) are the Cartesian coordinates of the midpoint of the rear
wheel axle, θ is the orientation of the vehicle, and φ is the steering angle.

11.3 Chained Form
The possibility of transforming the kinematic model (11.10) of a mobile robot
in a canonical form is of great interest for solving planning and control problems with eﬃcient, systematic procedures. Here, the analysis is limited to
systems with two inputs, like the unicycle and bicycle models.
A (2, n) chained form is a two-input driftless system
ż = γ 1 (z)v1 + γ 2 (z)v2 ,
whose equations are expressed as
ż1 = v1
ż2 = v2
ż3 = z2 v1
..
.
żn = zn−1 v1 .

(11.20)

Using the following notation for a ‘repeated’ Lie bracket:
adγ 1 γ 2 = [γ 1 , γ 2 ]
5

adkγ 1 γ 2 = [γ 1 , adk−1
γ 1 γ 2 ],

Note that the kinematic model (11.19) is no longer valid for φ = ±π/2, where the
ﬁrst vector ﬁeld is not deﬁned. This corresponds to the mechanical jam in which
the front wheel is orthogonal to the sagittal axis of the vehicle. This singularity
does not arise in the front-wheel drive bicycle (11.18), that in principle can still
pivot around the rear wheel contact point in such a situation.

11.3 Chained Form

483

one has for system (11.20)
⎡ ⎤
⎤
⎡
0
0
.
⎥
⎢
⎢1⎥
⎢ .. ⎥
⎥
⎢
⎢ ⎥
⎥
⎢
⎥
⎢
⎢0⎥
k
⎥ γ 2 = ⎢ ⎥ ⇒ adγ γ 2 = ⎢
(−1)k ⎥
γ1 = ⎢
⎥,
⎢
⎥
⎢
⎢0⎥
1
⎢ . ⎥
⎥
⎢
⎢.⎥
⎣ .. ⎦
⎦
⎣
⎣ .. ⎦
0
0
zn−1
⎡

1
0
z2
z3
..
.

⎤

where (−1)k is the (k + 2)-th component. This implies that the system is
controllable, because the accessibility distribution
ΔA = span {γ 1 , γ 2 , adγ 1 γ 2 , . . . , adn−2
γ 1 γ2}
has dimension n. In particular, the degree of nonholonomy is κ = n − 1.
There exist necessary and suﬃcient conditions for transforming a generic
two-input driftless system
q̇ = g 1 (q)u1 + g 2 (q)u2

(11.21)

in the chained form (11.20) via coordinate and input transformations
z = T (q)

v = β(q)u.

(11.22)

In particular, it can be shown that systems like (11.21) with dimension n not
larger than 4 can always be put in chained form. This applies, for example,
to the kinematic models of the unicycle and the bicycle.
There also exist suﬃcient conditions for transformability in chained form
that are relevant because they are constructive. Deﬁne the distributions
Δ0 = span {g 1 , g 2 , adg 1 g 2 , . . . , adn−2
g1 g2 }
Δ1 = span {g 2 , adg 1 g 2 , . . . , adn−2
g1 g2 }
Δ2 = span {g 2 , adg 1 g 2 , . . . , adn−3
g 1 g 2 }.
Assume that, in a certain set, it is dim Δ0 = n, Δ1 and Δ2 are involutive, and
there exists a scalar function h1 (q) whose diﬀerential dh1 satisﬁes
dh1 · Δ1 = 0

dh1 · g 1 = 1,

where the symbol · denotes the inner product between a row vector and a
column vector — in particular, ·Δ1 is the inner product with any vector
generated by distribution Δ1 . In this case, system (11.21) can be put in the
form (11.20) through the coordinate transformation6
z1 = h 1
6

This transformation makes use of the Lie derivative (see Appendix D).

484

11 Mobile Robots

z2 = Ln−2
g 1 h2
..
.
zn−1 = Lg 1 h2
zn = h 2 ,
where h2 must be chosen independent of h1 and such that dh2 · Δ2 = 0. The
input transformation is given by
v1 = u 1




n−2
v2 = Ln−1
g 1 h2 u1 + Lg 2 Lg 1 h2 u2 .
In general, the coordinate and input transformations are not unique.
Consider the kinematic model (11.13) of the unicycle. With the change of
coordinates
z1 = θ
z2 = x cos θ + y sin θ
z3 = x sin θ − y cos θ

(11.23)

and the input transformation
v = v 2 + z 3 v1
ω = v1 ,

(11.24)

one obtains the (2,3) chained form
ż1 = v1
ż2 = v2

(11.25)

ż3 = z2 v1 .
Note that, while z1 is simply the orientation θ, coordinates z2 and z3 represent
the position of the unicycle in a moving reference frame whose z2 axis is aligned
with the sagittal axis of the vehicle (see Fig. 11.3).
As for mobile robots with bicycle-like kinematics, consider for example the
model (11.19) corresponding to the rear-wheel drive case. Using the change
of coordinates
z1 = x
1
z2 = sec3 θ tan φ

z3 = tan θ
z4 = y

11.4 Dynamic Model

485

and the input transformation
v1
cos θ
3
1
ω = − v1 sec θ sin 2 φ + v2 cos 3 θ cos 2 φ,


v=

the (2,4) chained form is obtained:
ż1 = v1
ż2 = v2
ż3 = z2 v1
ż4 = z3 v1 .
This transformation is deﬁned everywhere in the conﬁguration space, with the
exception of points where cos θ = 0. The equivalence between the two models
is then subject to the condition θ = ±kπ/2, with k = 1, 2, . . . .

11.4 Dynamic Model
The derivation of the dynamic model of a mobile robot is similar to the manipulator case, the main diﬀerence being the presence of nonholonomic constraints on the generalized coordinates. An important consequence of nonholonomy is that exact linearization of the dynamic model via feedback is no
longer possible. In the following, the Lagrange formulation is used to obtain
the dynamic model of an n-dimensional mechanical system subject to k < n
kinematic constraints in the form (11.3), and it is shown how this model can
be partially linearized via feedback.
As usual, deﬁne the Lagrangian L of the mechanical system as the diﬀerence between its kinetic and potential energy:
L(q, q̇) = T (q, q̇) − U(q) =

1 T
q̇ B(q)q̇ − U(q),
2

(11.26)

where B(q) is the (symmetric and positive deﬁnite) inertia matrix of the
mechanical system. The Lagrange equations are in this case

T 
T
∂L
d ∂L
−
= S(q)τ + A(q)λ,
(11.27)
dt ∂ q̇
∂q
where S(q) is an (n × m) matrix mapping the m = n − k external inputs
τ to generalized forces performing work on q, A(q) is the transpose of the
(k × n) matrix characterizing the kinematic constraints (11.3), and λ ∈ IRm is
the vector of Lagrange multipliers. The term A(q)λ represents the vector of
reaction forces at the generalized coordinate level. It has been assumed that
the number of available inputs matches the number of DOFs (full actuation),

486

11 Mobile Robots

that is, in turn, equal to the number n of generalized coordinates minus the
number k of constraints.
Using (11.26), (11.27), the dynamic model of the constrained mechanical
system is expressed as
B(q)q̈ + n(q, q̇) = S(q)τ + A(q)λ
AT (q)q̇ = 0,

(11.28)
(11.29)

where
1
n(q, q̇) = Ḃ(q)q̇ −
2




T

 T
∂U(q)
∂
T
+
.
q̇ B(q)q̇
∂q
∂q

Consider now a matrix G(q) whose columns are a basis for the null space
of AT (q), so that AT (q)G(q) = 0. One can replace the constraint given
by (11.29) with the kinematic model
q̇ = G(q)v =

m


g i (q) vi ,

(11.30)

i=1

where v ∈ IRm is the vector of pseudo-velocities;7 for example, in the case
of a unicycle the components of this vector are the driving velocity v and
the steering velocity ω. Moreover, the Lagrange multipliers in (11.28) can be
eliminated premultiplying both sides of the equation by GT (q). This leads to
the reduced dynamic model
GT (q) (B(q)q̈ + n(q, q̇)) = GT (q)S(q)τ ,

(11.31)

a system of m diﬀerential equations.
Diﬀerentiation of (11.30) with respect to time gives
q̈ = Ġ(q)v + G(q)v̇.
Premultiplying this by GT (q)B(q) and using the reduced dynamic model
(11.31), one obtains
M (q)v̇ + m(q, v) = GT (q)S(q)τ ,

(11.32)

where
M (q) = GT (q)B(q)G(q)
m(q, v) = GT (q)B(q)Ġ(q)v + GT (q)n(q, G(q)v),
7

In the dynamic modeling context, the use of this term emphasizes the diﬀerence
between v and q̇, that are the actual (generalized) velocities of the mechanical
system.

11.4 Dynamic Model

487

with M (q) positive deﬁnite and
Ġ(q)v =


m 

∂g
vi i (q) G(q)v.
∂q
i=1

This ﬁnally leads to the state-space reduced model
q̇ = G(q)v
v̇ = M −1 (q)m(q, v) + M −1 (q)GT (q)S(q)τ ,

(11.33)
(11.34)

that represents in a compact form the kinematic and dynamic models of the
constrained system as a set of n + m diﬀerential equations.
Suppose now that


det GT (q)S(q) = 0,
an assumption on the ‘control availability’ that is satisﬁed in many cases
of interest. It is then possible to perform a partial linearization via feedback
of (11.33), (11.34) by letting

−1
τ = GT (q)S(q)
(M (q)a + m(q, v)) ,
(11.35)
where a ∈ IRm is the pseudo-acceleration vector. The resulting system is
q̇ = G(q)v
v̇ = a.

(11.36)
(11.37)

Note the structure of this system: the ﬁrst n equations are the kinematic
model, of which the last m — that represent the inclusion of m integrators on
the input channels — are a dynamic extension. If the system is unconstrained
and fully actuated, it is G(q) = S(q) = I n ; then, the feedback law (11.35)
simply reduces to an inverse dynamics control analogous to (8.57), and correspondingly the closed-loop system is equivalent to n decoupled double integrators.
The implementation of the feedback control (11.35) in principle requires
the measurement of v, and this may not be available. However, pseudovelocities can be computed via the kinematic model as

−1
GT (q)q̇,
(11.38)
v = G† (q)q̇ = GT (q)G(q)
provided that q and q̇ are measured. Note that the left pseudo-inverse of G(q)
has been used here.
By deﬁning the state x = (q, v) ∈ IRn+m and the input u = a ∈ IRm ,
system (11.36), (11.37) can be expressed as

 

G(q)v
0
ẋ = f (x) + G(x)u =
+
u,
(11.39)
0
Im

488

11 Mobile Robots

i.e., a nonlinear system with drift also known as the second-order kinematic
model of the constrained mechanical system. Accordingly, Eq. (11.36) is sometimes called ﬁrst-order kinematic model . In view of the results recalled in
Appendix D, the controllability of the latter guarantees the controllability of
system (11.39).
Summarizing, in nonholonomic mechanical systems — such as wheeled
mobile robots — it is possible to ‘cancel’ the dynamic eﬀects via nonlinear
state feedback, provided that the dynamic parameters are exactly known and
the complete state of the system (generalized coordinates and velocities q and
q̇) is measured.
Under these assumptions, the control problem can be directly addressed at
the (pseudo-)velocity level, i.e., by choosing v in such a way that the kinematic
model
q̇ = G(q)v
behaves as desired. From v, it is possible to derive the actual control inputs
at the generalized force level through (11.35). Since a = v̇ appears in this
equation, the pseudo-velocities v must be diﬀerentiable with respect to time.

Example 11.5
For illustration, the above procedure for deriving, reducing and partially linearizing
the dynamic model is now applied to the unicycle. Let m be the mass of the unicycle,
I its moment of inertia around the vertical axis through its centre, τ 1 the driving
force and τ 2 the steering torque. With the kinematic constraint expressed as (11.12),
the dynamic model (11.28), (11.29) takes on the form



m
0
0

0
m
0

0
0
I

 
ẍ
ÿ
θ̈


=

cos θ
sin θ
0




0  
sin θ
τ1
+ −cos θ λ
0
τ2
1
0

ẋ sin θ − ẏ cos θ = 0.
In this case one has
n(q, q̇) = 0
G(q) = S(q)
GT(q)S(q) = I
GT(q)B Ġ(q) = 0,
and thus the reduced model in state-space is obtained as
q̇ = G(q)v
v̇ = M −1 (q)τ



where
M −1 (q) =

1/m
0

0
1/I


.

11.5 Planning

489

By using the input transformation


τ = Mu =

m
0



0
u,
I

the second-order kinematic model is obtained as

⎡ v cos θ ⎤

⎡0⎤

⎡0⎤

⎢ v sin θ ⎥ ⎢ 0 ⎥
⎢0⎥
⎢ ⎥
⎢ ⎥
ω ⎥
⎦ + ⎣ 0 ⎦ u1 + ⎣ 0 ⎦ u2

ξ˙ = ⎢
⎣

with the state vector ξ = [ x

0
0

y

1
0

θ

v

0
1

ω ]T ∈ IR5 .

11.5 Planning
As with manipulators, the problem of planning a trajectory for a mobile robot
can be broken down in ﬁnding a path and deﬁning a timing law on the path.
However, if the mobile robot is subject to nonholonomic constraints, the ﬁrst
of these two subproblems becomes more diﬃcult than in the case of manipulators. In fact, in addition to meeting the boundary conditions (interpolation
of the assigned points and continuity of the desired degree) the path must
also satisfy the nonholonomic constraints at all points.
11.5.1 Path and Timing Law
Assume that one wants to plan a trajectory q(t), for t ∈ [ti , tf ], that leads a
mobile robot from an initial conﬁguration q(ti ) = q i to a ﬁnal conﬁguration
q(tf ) = q f in the absence of obstacles. The trajectory q(t) can be broken
down into a geometric path q(s), with dq(s)/ds = 0 for any value of s, and
a timing law s = s(t), with the parameter s varying between s(ti ) = si and
s(tf ) = sf in a monotonic fashion, i.e., with ṡ(t) ≥ 0, for t ∈ [ti , tf ]. A possible
choice for s is the arc length along the path; in this case, it would be si = 0
and sf = L, where L is the length of the path.
The above space-time separation implies that
q̇ =

dq
dq
=
ṡ = q  ṡ,
dt
ds

where the prime symbol denotes diﬀerentiation with respect to s. The generalized velocity vector is then obtained as the product of the vector q  , which
is directed as the tangent to the path in conﬁguration space, by the scalar ṡ,
that varies its modulus. Note that the vector [ x y  ]T ∈ IR2 is directed as

490

11 Mobile Robots

the tangent to the Cartesian path, and has unit norm if s is the cartesian arc
length (see Sect. 5.3.1).
Nonholonomic constraints of the form (11.3) can then be rewritten as
A(q)q̇ = A(q)q  ṡ = 0.
If ṡ(t) > 0, for t ∈ [ti , tf ], one has
A(q)q  = 0.

(11.40)

This condition, that must be veriﬁed at all points by the tangent vector on the
conﬁguration space path, characterizes the notion of geometric path admissibility induced by the kinematic constraint (11.3) that actually aﬀects generalized velocities. Similar to what has been done for trajectories in Sect. 11.2,
geometrically admissible paths can be explicitly deﬁned as the solutions of
the nonlinear system
(11.41)
q  = G(q)u,
where u is a vector of geometric inputs that are related to the velocity inputs
u by the relationship u(t) = u(s)ṡ(t). Once the geometric inputs u(s) are
assigned for s ∈ [si , sf ], the path of the robot in conﬁguration space is uniquely
determined. The choice of a timing law s = s(t), for t ∈ [ti , tf ], will then
identify a particular trajectory along this path.
For example, in the case of a mobile robot with unicycle-like kinematics,
the pure rolling constraint (11.6) entails the following condition for geometric
admissibility of the path:
[ sin θ

0 ] q  = x sin θ − y  cos θ = 0,

−cos θ

that simply expresses the fact that the tangent to the Cartesian path must be
aligned with the robot sagittal axis. As a consequence, a path whose tangent
is discontinuous (e.g., a broken line) is not admissible, unless the unicycle is
allowed to stop at discontinuity points by setting ṡ = 0 for the time necessary
to rotate on the spot so as to align with the new tangent.
Geometrically admissible paths for the unicycle are the solutions of the
system
x = v cos θ
y  = v sin θ

(11.42)

θ = ω,
where v, ω are related to v, ω by
v(t) = v(s)ṡ(t)

(11.43)

ω(t) = ω(s)ṡ(t).

(11.44)

11.5 Planning

491

11.5.2 Flat Outputs
Many kinematic models of mobile robots, including the unicycle and the bicycle, exhibit a property known as diﬀerential ﬂatness, that is particularly relevant in planning problems. A nonlinear dynamic system ẋ = f (x) + G(x)u
is diﬀerentially ﬂat if there exists a set of outputs y, called ﬂat outputs, such
that the state x and the control inputs u can be expressed algebraically as a
function of y and its time derivatives up to a certain order:
x = x(y, ẏ, ÿ, . . . , y (r) )
u = u(y, ẏ, ÿ, . . . , y (r) ).
As a consequence, once an output trajectory is assigned for y, the associated trajectory of the state x and history of control inputs u are uniquely
determined.
In the unicycle and bicycle cases, the Cartesian coordinates are indeed
ﬂat outputs. In the following, this property is established for the unicycle.
This can be done with reference to either the kinematic model (11.13) or
the geometric model (11.42). For simplicity, refer to the latter. Its ﬁrst two
equations imply that, given a Cartesian path (x(s), y(s)), the associated state
trajectory is q(s) = [ x(s) y(s) θ(s) ]T where
θ(s) = Atan2 (y  (s), x (s)) + kπ

k = 0, 1.

(11.45)

The two possible choices for k account for the fact that the same Cartesian
path may be followed moving forward (k = 0) or backward (k = 1). If the
initial orientation of the robot is assigned, only one of the choices for k is
correct. The geometric inputs that drive the robot along the Cartesian path
are easily obtained from (11.42),(11.45) as
v(s) = ± (x (s))2 + (y  (s))2
y  (s)x (s) − x (s)y  (s)
ω(s) =
.
(x (s))2 + (y  (s))2

(11.46)
(11.47)

These equations deserve some comments:
• The choice of the sign of v(s) depends on the type of motion (forward or
backward).
• If x (s̄ ) = y  (s̄ ) = 0 for some s̄ ∈ [si , sf ], one has v(s̄ ) = 0. This happens,
for example, in correspondence of cusps (motion inversions) in the Cartesian path. In these points, Eq. (11.45) does not deﬁne the orientation, that
can however be derived by continuity, i.e., as the limit of its right-hand side
for s → s̄− . Similar arguments can be repeated for the steering velocity ω
given by (11.47).
• The possibility of reconstructing θ and ω is lost when the Cartesian trajectory degenerates to a point, because in this case it is x (s) = y  (s) = 0
identically.

492

11 Mobile Robots

It is interesting to note that for driftless dynamic systems — like the
kinematic models of mobile robots — diﬀerential ﬂatness is a necessary and
suﬃcient condition for transformability in the chained form introduced in
Sect. 11.3. In particular, it is easy to prove that the ﬂat outputs of a (2, n)
chained form are z1 and zn , from which it is possible to compute all the other
state variables as well as the associated control inputs. For example, in the
case of the (2, 3) chained form (11.25) it is
z2 =

ż3
ż1

v1 = ż1

v2 =

ż1 z̈3 − z̈1 ż3
.
ż12

Note that z2 and v2 can be actually reconstructed only if ż1 (t) = 0, for
t ∈ [ti , tf ].
11.5.3 Path Planning
Whenever a mobile robot admits a set of ﬂat outputs y, these may be exploited
to solve planning problems eﬃciently. In fact, one may use any interpolation
scheme to plan the path of y in such a way as to satisfy the appropriate
boundary conditions. The evolution of the other conﬁguration variables, together with the associated control inputs, can then be computed algebraically
from y(s). The resulting conﬁguration space path will automatically satisfy
the nonholonomic constraints (11.40).
In particular, consider the problem of planning a path that leads a unicycle
from an initial conﬁguration q(si ) = q i = [ xi yi θi ]T to a ﬁnal conﬁguration q(sf ) = q f = [ xf yf θf ]T .
Planning via Cartesian polynomials
As mentioned above, the problem can be solved by interpolating the initial
values xi , yi and the ﬁnal values xf , yf of the ﬂat outputs x, y. Letting si = 0
and sf = 1, one may use the following cubic polynomials:
x(s) = s3 xf − (s − 1)3 xi + αx s2 (s − 1) + βx s(s − 1)2
y(s) = s3 yf − (s − 1)3 yi + αy s2 (s − 1) + βy s(s − 1)2 ,
that automatically satisfy the boundary conditions on x, y. The orientation
at each point being related to x , y  by (11.45), it is also necessary to impose
the additional boundary conditions
x (0) = ki cos θi
y  (0) = ki sin θi

x (1) = kf cos θf
y  (1) = kf sin θf ,

where ki = 0, kf = 0 are free parameters that must however have the same
sign. This condition is necessary to guarantee that the unicycle arrives in q f
with the same kind of motion (forward or backward) with which it leaves q i ;

11.5 Planning

493

in fact, since x(s) and y(s) are cubic polynomials, the Cartesian path does
not contain motion inversions in general.
For example, by letting ki = kf = k > 0, one obtains
  

  

αx
k cos θf − 3xf
βx
k cos θi + 3xi
=
=
.
αy
k sin θf − 3yf
βy
k sin θi + 3yi
The choice of ki and kf has a precise inﬂuence on the obtained path. In fact,
by using (11.46) it is easy to verify that
v(0) = ki

v(1) = kf .

The evolution of the robot orientation along the path and the associated
geometric inputs can then be computed by using Eqs. (11.45) and (11.46),
(11.47), respectively.
Planning via the chained form
Another technique, which can be immediately generalized to other kinematic
models of mobile robots (e.g., the bicycle), is planning the path in the chained
form coordinates z. To this end, it is ﬁrst necessary to compute the initial
and ﬁnal values z i and z f that correspond to q i and q f , by using the change
of coordinates (11.23). It is then suﬃcient to interpolate the initial and ﬁnal
values of z1 and z3 (the ﬂat outputs) with the appropriate boundary conditions
on the remaining variable z2 = z3 /z1 .
Again, it is possible to adopt a cubic polynomial to solve the problem. As
an alternative, one may use polynomials of diﬀerent degree for x and y in order
to reduce the number of unknown coeﬃcients to be computed. For example,
under the assumption z1,i = z1,f , consider the following interpolation scheme:
z1 (s) = z1,f s − (s − 1)z1,i
z3 (s) = s3 z3,f − (s − 1)3 z3,i + α3 s2 (s − 1) + β3 s(s − 1)2 ,
with s ∈ [0, 1]. Note that z1 (s) is constant and equal to z1,f − z1,i = 0. The
unknowns α3 , β3 must be determined by imposing the boundary conditions
on z2 :
z3 (0)
z3 (1)
= z2i
= z2f ,

z1 (0)
z1 (1)
from which
α3 = z2,f (z1,f − z1,i ) − 3z3,f
β3 = z2,i (z1,f − z1,i ) + 3z3,i .
This scheme cannot be directly applied when z1,i = z1,f , i.e., when θi = θf . To
handle this singular case, one may introduce a via point q v = [ xv yv θv ]T

494

11 Mobile Robots

such that θv = θi , and solve the original planning problem using two consecutive paths, the ﬁrst from q i to q v and the second from q v to q f . Another possibility, which avoids the introduction of the via point, is to let z1,f = z1,i +2π
(i.e., to replace θf with θf + 2π); this obviously corresponds to the same ﬁnal
conﬁguration of the unicycle. With the resulting manoeuvre, the robot will
reach its destination while performing a complete rotation of orientation along
the path.
Once the path has been planned for the chained form, the path q(s) in
the original coordinates and the associated geometric inputs u(s) are reconstructed by inverting the change of coordinates (11.23) and of inputs (11.24),
respectively.
Planning via parameterized inputs
A conceptually diﬀerent approach to path planning consists of writing the
inputs — rather than the path — in parameterized form, and computing the
value of the parameters so as to drive the robot from q i to q f . Again, it is
convenient to work on the chained form, whose equations are easily integrable
in closed form under appropriate inputs. For the sake of generality, refer to
the (2, n) chained form (11.20), whose geometric version is
z1 = v1

z2 = v2
z3 = z2 v1
..
.
zn = zn−1 v1 .
Let the geometric input be chosen as
v1 = sgn(Δ)

(11.48)

v2 = c0 + c1 s + . . . + cn−2 sn−2 ,

(11.49)

with Δ = z1,f − z1,i and s ∈ [si , sf ] = [0, |Δ|]. Parameters c0 , . . . , cn−2 must
be chosen so as to give z(sf ) = z f . It is possible to verify that such condition
is expressed as a linear system of equations
⎡
⎤
c0
⎢ c1 ⎥
⎥
D(Δ) ⎢
(11.50)
⎣ ... ⎦ = d(z i , z f , Δ)
cn−2
where matrix D(Δ) is invertible if Δ = 0. For example, in the case of the
(2, 3) chained form, one obtains




Δ2
|Δ|
z2,f − z2,i
2
D=
.
(11.51)
d=
2
z3,f − z3,i − z2,i Δ
Δ3
sgn(Δ) Δ2
6

11.5 Planning

495

If z1,i = z1,f a singular case is met that can be handled as before.
Both z(s) and v(s) must then be converted to q(s) and u(s) using the
inverse coordinate and input transformations that apply to the speciﬁc case.
The above method does not make explicit use of the ﬂat outputs, but
relies on the closed-form integrability of the chained form, whose existence,
as already mentioned, is equivalent to diﬀerential ﬂatness. Note also that in
this planning scheme, as in the previous two, parameter s does not represent
the arc length on the path.
Other classes of parameterized inputs that can be used in place of (11.48),
(11.49) are sinusoidal and piecewise constant functions.
Numerical results
For illustration, some numerical results of the planning methods so far described are now presented. The considered vehicle is a unicycle that must
perform various ‘parking’ manoeuvres.
Two typical paths produced by the planner that uses cubic Cartesian polynomials are shown in Fig. 11.5. As already noticed, the unicycle never inverts
its motion, that is forward in these two manoeuvres because k = 5 > 0. For
k < 0, the manoeuvres would have been performed in backward motion with
diﬀerent paths.
In Fig. 11.6, the same planner is used to solve a parallel parking problem,
in which the diﬀerence between the initial and the ﬁnal conﬁguration of the
unicycle is a pure displacement in the direction orthogonal to the sagittal
axis. Note how the path changes as ki = kf = k is changed; in particular,
an increase in k leads to elongated ‘take-oﬀ’ (from q i ) and ‘landing’ (on q f )
phases.
Figure 11.7 refers to the case in which q i and q f diﬀer only for the value
of θ (a pure reorientation); the unicycle leaves the initial position and follows
a path that leads back to it with the correct orientation. This behaviour is to
be expected, because with this planner a Cartesian path of nonzero length is
needed to achieve any kind of reconﬁguration.
To allow a comparison, the same planning problems have also been solved
with the method based on the use of parameterized inputs in conjunction with
the chained form.
Figure 11.8 shows the path obtained with this planner for the same two
parking problems of Fig. 11.5. While in the ﬁrst case the obtained manoeuvre
is similar to the one obtained before, in the second the path contains a cusp,
corresponding to a motion inversion. In fact, in view of its nature, this planner
can only generate paths along which the robot orientation stays between its
initial value θi and its ﬁnal value θf .
In Fig. 11.9, two diﬀerent solutions produced by this planner are reported
for the parallel parking problem of Fig. 11.6. The singularity due to θi = θf
has been solved in two diﬀerent ways: by adding a via point q v , and redeﬁning
θf as θf = θi + 2π. Note that in the latter case the path produced by the

496

11 Mobile Robots




R7
>P@

>P@



R7













RW







>P@



RW










>P@







Fig. 11.5. Two parking manoeuvres planned via cubic Cartesian polynomials; in
both cases k = 5 has been used

RW




>P@


>P@

RW










R7








R7



>P@














>P@







Fig. 11.6. Planning a parallel parking manoeuvre via cubic Cartesian polynomials;
left: with k = 10, right: with k = 20

RW
R7

RW
R7










>P@

>P@






















>P@











>P@

Fig. 11.7. Planning a pure reorientation manoeuvre via cubic Cartesian polynomials; left: with k = 10, right: with k = 20

planner leads the robot to the destination with a complete rotation of its
orientation.
Finally, the same pure reorientation manoeuvre of Fig. 11.7 has been considered in Fig. 11.10. The path on the left has been obtained as outlined
before, i.e., exploiting the transformations of coordinates (11.23) and of in-

11.5 Planning



>P@


>P@

R7



R7



497



RW








RW









>P@

















>P@

Fig. 11.8. Two parking manoeuvres planned via the chained form






>P@

>P@

RW



RW



R8





R7





R7









>P@


















>P@

Fig. 11.9. Planning a parallel parking manoeuvre via the chained form; left: adding
a via point q v , right: letting θf = θi + 2π





RW
R7



>P@

>P@





RW
R7












>P@












>P@







Fig. 11.10. Planning a pure reorientation manoeuvre via the chained form; left:
with the coordinate transformation (11.23), right: with the coordinate transformation (11.52)

puts (11.24) to put the system in chained form, and then using the parameterized inputs (11.48), (11.49). As in the previous case, the required reorientation is realized by a Cartesian path. This is a consequence of the structure
of (11.23), for which θi = θf implies in general z2,i = z2,f and z3,i = z3,f ,
even when xi = xf , yi = yf .

498

11 Mobile Robots

The manoeuvre on the right of Fig. 11.10, which is a rotation on the spot,
was achieved by using a diﬀerent change of coordinates to put the system in
(2,3) chained form. In particular, the following transformation has been used
z1 = θ − θ f
z2 = (x − xi ) cos θ + (y − yi ) sin θ

(11.52)

z3 = (x − xi ) sin θ − (y − yi ) cos θ,
which places the origin of the (z2 , z3 ) reference frame in correspondence of the
initial Cartesian position of the unicycle. With this choice one has z2,i = z2,f
and z3,i = z3,f for a pure reorientation, and thus the manoeuvre is eﬃciently
obtained as a simple rotation.
As a matter of fact, using (11.52) in place of (11.23) is always recommended. In fact, the analysis of (11.51) shows that in general the magnitude
of the coeﬃcients of v2 — and therefore, the length of the obtained path —
depends not only on the amount of reconﬁguration required for z2 and z3 , but
also on the value of z2,i itself. The adoption of (11.52), which implies z2,i = 0,
makes the size of the manoeuvre invariant with respect to the Cartesian position of the unicycle.
11.5.4 Trajectory Planning
Once a path q(s), s ∈ [si , sf ], has been determined, it is possible to choose
a timing law s = s(t) with which the robot should follow it. In this respect,
considerations similar to those of Sect. 5.3.1 apply. For example, if the velocity
inputs of the unicycle are subject to bounds of the form8
|v(t)| ≤ vmax

|ω(t)| ≤ ωmax

∀t,

(11.53)

it is necessary to verify whether the velocities along the planned trajectory
are admissible. In the negative case, it is possible to slow down the timing law
via uniform scaling. To this end, it is convenient to rewrite the timing law
by replacing t with the normalized time variable τ = t/T , with T = tf − ti .
From (11.43), (11.44) one has
ds 1
ds dτ
= v(s)
dτ dt
dτ T
ds 1
ds dτ
= ω(s)
,
ω(t) = ω(s)
dτ dt
dτ T
v(t) = v(s)

(11.54)
(11.55)

and therefore is is suﬃcient to increase T (i.e., the duration of the trajectory)
to reduce uniformly v and ω, so as to stay within the given bounds.
8

For a diﬀerential drive unicycle, the actual bounds aﬀect the wheel angular speeds
ωL and ωR . Through Eqs. (11.14), these bounds can be mapped to constraints
on v and ω (see Problem 11.9).

11.5 Planning

499

It is also possible to plan directly a trajectory without separating the
geometric path from the timing law. To this end, all the techniques presented
before can be used with the time variable t directly in place of the path
parameter s. A drawback of this approach is that the duration tf −ti = sf −si
of the trajectory is ﬁxed, and uniform scaling cannot be used to satisfy bounds
on the velocity inputs. In fact, an increase (or decrease) of tf −ti would modify
the geometric path associated with the planned trajectory.
11.5.5 Optimal Trajectories
The planning techniques so far presented can be used to compute trajectories
that lead the robot from an initial conﬁguration q i to a ﬁnal conﬁguration q f
while complying with the nonholonomic constraints and, possibly, bounds on
the velocity inputs. Often, other requirements are added, such as limiting the
path curvature, avoiding workspace obstacles or reducing energy consumption.
In general, these are integrated in the design procedure as the optimization
of a suitable cost criterion along the trajectory. For example, the previous
objectives will be respectively formulated as the minimization of the maximum
curvature, the maximization of the minimum distance between the robot and
the obstacles, or the minimization of the total energy needed by the mobile
robot to follow the path.
A simple technique for attacking the optimal planning problem consists
of over-parameterizing the adopted interpolation scheme, so as to pursue the
optimization — typically, via numerical techniques — of the cost criterion
by appropriately choosing the redundant parameters. Clearly, the obtained
trajectory will be optimal only with respect to the set of trajectories that
can be generated by the chosen scheme, and will be a suboptimal solution for
the original planning problem; this may or may not lead to the fulﬁlment of
the original speciﬁcations. For example, the planning scheme based on cubic
Cartesian polynomials contains two free parameters (ki and kf ), that may
be chosen so as to maximize the minimum distance along the path between
the unicycle and certain obstacles. However, depending on the placement of
the obstacles with respect to q i and q f , a collision-free path (i.e., a path for
which the above distance is always positive) may or may not9 exist within the
chosen family of cubic polynomials.
A more systematic approach to the problem relies on the use of optimal
control theory. The basic problem considered in this discipline is in fact the
determination of a control law that transfers a dynamic system between two
assigned states so as to minimize a chosen cost functional along the trajectory.
A powerful tool for solving this problem is the Pontryagin minimum principle
that provides necessary conditions for optimality. By exploiting these conditions in conjunction with the analysis of the speciﬁc characteristics of the
9

The complexity of the problem of planning collision-free motions in the presence of
obstacles is such that speciﬁc solution techniques are needed; these are presented
in Chap. 12.

500

11 Mobile Robots
L
gyq

L
gy

CQL


gy


gyq

CQ

Fig. 11.11. The elementary arcs that constitute the trajectories of the suﬃcient
family for the minimum-time planning problem for the unicycle

considered problem, it is often possible to identify a reduced set of candidate trajectories, also referred to as a suﬃcient family, among which there is
certainly the desired optimal solution (if it exists).
In any case, each optimal planning problem must be formulated in the
appropriate context. When minimizing curvature or avoiding static obstacles,
the timing law part of the trajectory is irrelevant, and the problem can be
solved by planning a path for the geometric model (11.41). If the cost criterion
depends on the path as well as on the timing law, it is necessary to plan
directly on the kinematic model (11.10). A particularly important example of
the latter situation is met when minimum-time trajectories are sought in the
presence of bounds on the velocity inputs.
Minimum-time trajectories
Consider the problem of transferring the unicycle (11.13) from the initial
conﬁguration q i to the ﬁnal conﬁguration q f while minimizing the functional
"
J = tf − t i =

tf

dt,
ti

under the assumption that the driving and steering velocity inputs v and ω are
bounded as in (11.53). By combining the conditions provided by the minimum
principle with geometrical arguments, it is possible to determine a suﬃcient
family for the solution to this problem. This family consists of trajectories
obtained by concatenating elementary arcs of two kinds only:
• arcs of circle of variable length, covered with velocities v(t) = ±vmax and
ω(t) = ±ωmax (the radius of the circle is always vmax /ωmax );
• line segments of variable length, covered with velocities v(t) = ±vmax and
ω(t) = 0.
These elementary arcs are shown in Fig. 11.11, where a compact notation
is also deﬁned for identifying them. In particular, Ca and Sd indicate an arc
of circle of duration a and a line segment of duration d, respectively (in the

11.5 Planning

501

particular case vmax = 1, a and d are also the lengths of these arcs). The
superscript indicates forward (+) o’r backward (−) motion, while for circular
arcs the second subscript indicates a rotation in the clockwise (r) or counterclockwise (l) direction. With this notation, and considering for simplicity
the case vmax = 1 and ωmax = 1, the trajectories of the suﬃcient family (also
called Reeds–Shepp curves) can be classiﬁed in the following nine groups:
I
II
III
IV
V
VI
VII
VIII
IX

Ca |Cb |Ce
Ca |Cb Ce
Ca Cb |Ce
Ca Cb |Cb Ce
Ca |Cb Cb |Ce
Ca |Cπ/2 Se Cπ/2 |Cb
Ca |Cπ/2 Se Cb
Ca Se Cπ/2 |Cb
Ca Se Cb

a ≥ 0, b ≥ 0, e ≥ 0, a + b + e ≤ π
0 ≤ a ≤ b, 0 ≤ e ≤ b, 0 ≤ b ≤ π/2
0 ≤ a ≤ b, 0 ≤ e ≤ b, 0 ≤ b ≤ π/2
0 ≤ a ≤ b, 0 ≤ e ≤ b, 0 ≤ b ≤ π/2
0 ≤ a ≤ b, 0 ≤ e ≤ b, 0 ≤ b ≤ π/2
0 ≤ a ≤ π/2, 0 ≤ b ≤ π/2, e ≥ 0
0 ≤ a ≤ π, 0 ≤ b ≤ π/2, e ≥ 0
0 ≤ a ≤ π/2, 0 ≤ b ≤ π, e ≥ 0
0 ≤ a ≤ π/2, 0 ≤ b ≤ π/2, e ≥ 0,

(11.56)

where the symbol “|” between two elementary arcs indicates the presence
of a cusp (motion inversion) on the path. Each group contains trajectories
consisting of a sequence of no more than ﬁve elementary arcs; the duration of
circular arcs is bounded by either π/2 or π, while the duration of line segments
depends on the Cartesian distance between q i and q f . The number of possible
sequences produced by each group is ﬁnite; they are obtained by instantiating
the elementary arcs depending on the direction of motion and of rotation. For
example, it is easy to show that group IX generates eight sequence types, each
corresponding to a trajectory entirely covered in forward or backward motion:
+
+ + +
+ + +
+
+
+
Se+ Ca,r
, Ca,r
Se+ Ca,l
, Ca,l
Se Ca,r , Ca,l
Se Ca,l
Ca,r
−
− − −
− − −
−
−
−
Ca,r
Se− Ca,r
, Ca,r
Se− Ca,l
, Ca,l
Se Ca,r , Ca,l
Se Ca,l .

By this kind of argument, it may be veriﬁed that the above groups generate
a total number of 48 diﬀerent sequences.
In practice, one may use an exhaustive algorithm to identify the minimumtime trajectory that leads the unicycle from q i to q f :
• Determine all the trajectories belonging to the suﬃcient family that join
q i and q f .
• Compute the value of the cost criterion tf − ti along these trajectories,
and choose the one to which the minimum value is associated.
The ﬁrst is clearly the most diﬃcult step. Essentially, for each of the aforementioned 48 sequences it is necessary to identify — if it exists — the corresponding trajectory going from q i to q f , and to compute the duration of
the associated elementary arcs. To this end, for each sequence, it is possibly

502

11 Mobile Robots

L
gy

CQL

RW

L
gyq

RW


gKq

gK

R7
L
gyq

R7

L
gy

Fig. 11.12. Two examples of minimum-time trajectories for the unicycle

to express and invert in closed form the relationship between the duration of
the arcs and the obtained change in conﬁguration. By doing so, the ﬁrst step
can also be completed very quickly.
Figure 11.12 shows two examples of minimum-time trajectories for the
unicycle. The ﬁrst (on the left) belongs to group IX and contains three elementary arcs without inversions of motion. The second (on the right) is a
group V trajectory consisting of four arcs of circle, along which there are two
motion inversions.

11.6 Motion Control
The motion control problem for wheeled mobile robots is generally formulated
with reference to the kinematic model (11.10), i.e., by assuming that the
control inputs determine directly the generalized velocities q̇. For example, in
the case of the unicycle (11.13) and of the bicycle (11.18) or (11.19) this means
that the inputs are the driving and steering velocity inputs v and ω. There
are essentially two reasons for taking this simplifying assumption. First, as
already seen in Sect. 11.4, under suitable assumptions it is possible to cancel
the dynamic eﬀects via state feedback, so that the control problem is actually
transferred to the second-order kinematic model, and from the latter to the
ﬁrst-order kinematic model. Second, in the majority of mobile robots it is not
possible to command directly the wheel torques, because there are low-level
control loops that are integrated in the hardware or software architecture.
These loops accept as input a reference value for the wheel angular speed,
that is reproduced as accurately as possible by standard regulation actions
(e.g., PID controllers). In this situation, the actual inputs available for highlevel controls are precisely the reference velocities.
In this section, a unicycle-like vehicle is again considered, although some
of the presented control schemes may be extended to other kinds of mobile
robots. Two basic control problems, illustrated in Fig. 11.13, will be considered
for system (11.13):

11.6 Motion Control

Rw0D

503

RQ


0

RQw0D

R(

Fig. 11.13. Control problems for a unicycle; left: trajectory tracking, right: posture
regulation

• Trajectory tracking: the robot must asymptotically track a desired Cartesian trajectory (xd (t), yd (t)), starting from an initial conﬁguration q 0 =
[ x0 y0 θ0 ]T that may or may not be ‘matched’ with the trajectory.
• Posture regulation: the robot must asymptotically reach a given posture,
i.e., a desired conﬁguration q d , starting from an initial conﬁguration q 0 .
From a practical point of view, the most relevant of these problems is
certainly the ﬁrst. This is because, unlike industrial manipulators, mobile
robots must invariably operate in unstructured workspaces containing obstacles. Clearly, forcing the robot to move along (or close to) a trajectory
planned in advance considerably reduces the risk of collisions. On the other
hand, a preliminary planning step is not required when posture regulation is
performed, but the Cartesian trajectory along which the robot approaches q d
cannot be speciﬁed by the user.
11.6.1 Trajectory Tracking
For the tracking problem to be soluble, it is necessary that the desired Cartesian trajectory (xd (t), yd (t)) is admissible for the kinematic model (11.13),
i.e., it must satisfy the equations
ẋd = vd cos θd
ẏd = vd sin θd
θ̇d = ωd

(11.57)

for some choice of the reference inputs vd and ωd . For example, this is certainly
true if the trajectory has been produced using one of the planning schemes of
the previous section. In any case, as seen in Sect. 11.5.2, since the unicycle coordinates x and y are ﬂat outputs ,the orientation along the desired trajectory
(xd (t), yd (t)) can be computed as
θd (t) = Atan2 (ẏd (t), ẋ(t)d ) + kπ

k = 0, 1,

(11.58)

504

11 Mobile Robots

as well as the reference inputs

vd (t) = ± ẋ2d (t) + ẏd2 (t)
ωd (t) =

ÿd (t)ẋd (t) − ẍd (t)ẏd (t)
.
ẋ2d (t) + ẏd2 (t)

(11.59)
(11.60)

Note that (11.58) and (11.59), (11.60), correspond respectively to (11.45)
and (11.46), (11.47) with s = t. In the following, it is assumed that the value
of k in (11.58) — and correspondingly, the sign of vd in (11.59) — has been
chosen.
By comparing the desired state q d (t) = [ xd (t) yd (t) θd (t) ]T with the
current measured state q(t) = [ x(t) y(t) θ(t) ]T it is possible to compute
an error vector that can be fed to the controller. However, rather than using
directly the diﬀerence between q d and q, it is convenient to deﬁne the tracking
error as
⎡ ⎤ ⎡
⎤⎡
⎤
e1
cos θ sin θ 0
xd − x
e = ⎣ e2 ⎦ = ⎣ −sin θ cos θ 0 ⎦ ⎣ yd − y ⎦ .
0
0
1
e3
θd − θ
The positional part of e is the Cartesian error ep = [ xd − x yd − y ]T expressed in a reference frame aligned with the current orientation θ of the robot
(see Fig. 11.13). By diﬀerentiating e with respect to time, and using (11.13)
and (11.57), one easily ﬁnds
ė1 = vd cos e3 − v + e2 ω
ė2 = vd sin e3 − e1 ω
ė3 = ωd − ω.

(11.61)

Using the input transformation
v = vd cos e3 − u1
ω = ωd − u 2 ,

(11.62)
(11.63)

which is clearly invertible, the following expression is obtained for the tracking
error dynamics:
⎡
⎤
⎤
⎤
⎡
⎡
0
ωd 0
0
1 −e2  
u
ė = ⎣ −ωd 0 0 ⎦ e + ⎣ sin e3 ⎦ vd + ⎣ 0 e1 ⎦ 1 .
(11.64)
u2
0
0
1
0
0 0
Note that the ﬁrst term of this dynamics is linear, while the second and third
are nonlinear. Moreover, the ﬁrst and second terms are in general time-varying,
due to the presence of the reference inputs vd (t) and ωd (t).

11.6 Motion Control

505

Control based on approximate linearization
The simplest approach to designing a tracking controller consists of using the
approximate linearization of the error dynamics around the reference trajectory, on which clearly e = 0. This approximation, whose accuracy increases
as the tracking error e decreases, is obtained from (11.64) simply setting
sin e3 = e3 and evaluating the input matrix on the trajectory. The result is
⎡
⎤
⎡
⎤
0
ωd 0
1 0  
u
ė = ⎣ −ωd 0 vd ⎦ e + ⎣ 0 0 ⎦ 1 .
(11.65)
u2
0
0
0
0 1
Note that the approximate system is still time-varying. Consider now the
linear feedback
u1 = −k1 e1

(11.66)

u2 = −k2 e2 − k3 e3

(11.67)

that leads to the following closed-loop linearized dynamics:
⎤
⎡
−k1 ωd
0
ė = A(t) e = ⎣ −ωd
0
vd ⎦ e.
0
−k2 −k3

(11.68)

The characteristic polynomial of matrix A is
p(λ) = λ(λ + k1 )(λ + k3 ) + ωd2 (λ + k3 ) + vd k2 (λ + k1 ).
At this point, it is suﬃcient to let
k1 = k3 = 2ζa

k2 =

a2 − ωd2
,
vd

(11.69)

with ζ ∈ (0, 1) and a > 0, to obtain
p(λ) = (λ + 2ζa)(λ2 + 2ζaλ + a2 ).
The closed-loop linearized error dynamics is then characterized by three constant eigenvalues: one real negative eigenvalue in −2ζa and a pair of complex
eigenvalues with negative real part, damping coeﬃcient ζ and natural frequency a. However, in view of its time-varying nature, there is no guarantee
that system (11.68) is asymptotically stable.
A notable exception is when vd and ωd are constant, as in the case of circular or rectilinear trajectories. In fact, the linearized system (11.68) is then
time-invariant and therefore asymptotically stable with the choice of gains
in (11.69). Hence, by using the control law (11.66), (11.67) with the same
gains, the origin of the original error system (11.64) is also asymptotically
stable, although this result is not guaranteed to hold globally. For suﬃciently

506

11 Mobile Robots

small initial errors, the unicycle will certainly converge to the desired Cartesian trajectory (either circular or rectilinear).
In general, the feedback controller (11.66), (11.67) is linear but also timevarying in view of the expression of k2 in (11.69). The actual velocity inputs
v and ω should be reconstructed from u1 and u2 through (11.62), (11.63).
In particular, it is easy to verify that v and ω tend to coincide with the
reference inputs vd and ωd (i.e., they reduce to a pure feedforward action) as
the tracking error e vanishes.
Finally, note that k2 in (11.69) diverges when vd goes to zero, i.e., when
the reference Cartesian trajectory tends to stop. Therefore, the above control
scheme can only be used for persistent Cartesian trajectories, i.e., trajectories
such that |vd (t)| ≥ v̄ > 0, ∀t ≥ 0. This also means that motion inversions
(from forward to backward motion, or vice versa) on the reference trajectory
are not allowed.
Nonlinear control
Consider again the exact expression (11.64) of the tracking error dynamics,
now rewritten for convenience in the ‘mixed’ form:
ė1 = e2 ω + u1
ė2 = vd sin e3 − e1 ω
ė3 = u2 ,

(11.70)

and the following nonlinear version of the control law (11.66), (11.67):
u1 = −k1 (vd , ωd ) e1
sin e3
e2 − k3 (vd , ωd ) e3 ,
u2 = −k2 vd
e3

(11.71)
(11.72)

where k1 (·, ·) > 0 and k3 (·, ·) > 0 are bounded functions with bounded derivatives, and k2 > 0 is constant. If the reference inputs vd and ωd are also bounded
with bounded derivatives, and they do not both converge to zero, the tracking
error e converges to zero globally, i.e., for any initial condition.
A sketch is now given of the proof of this result. Consider the closed-loop
error dynamics
ė1 = e2 ω − k1 (vd , ωd ) e1
ė2 = vd sin e3 − e1 ω
sin e3
ė3 = −k2 vd
e2 − k3 (vd , ωd ) e3 ,
e3
and the candidate Lyapunov function
V =

 e2
k2 2
e1 + e22 + 3 ,
2
2

(11.73)

11.6 Motion Control

507

whose time derivative along the system trajectories
V̇ = −k1 (vd , ωd )k2 e21 − k3 (vd , ωd )e23
is negative semi-deﬁnite. This means that V (which is bounded below) tends
to a limit value, and also that the norm of e is bounded. As system (11.73) is
time-varying, it is not possible to use La Salle theorem to gain further insight.
However, under the above assumptions, one may verify that V̈ is limited, and
therefore V̇ is uniformly continuous. Hence, Barbalat lemma (see Sect. C.3)
may be used to conclude that V̇ tends to zero, i.e., that e1 and e3 tend to
zero. From this and the system equations it is possible to prove that
lim (vd2 + ωd2 )e22 = 0,

t→∞

and thus e2 tends to zero as well, provided that at least one of the reference
inputs is persistent.
Again, the actual velocity inputs v and ω must be computed from u1 and
u2 using (11.62), (11.63). Note that the control law (11.71), (11.72) requires
the persistency of the state trajectory q(t), but not of the Cartesian trajectory.
In particular, the reference velocity vd (t) can converge to zero as long as ωd (t)
does not, and vice versa. For example, this controller can be used to track a
Cartesian trajectory that degenerates to a simple rotation on the spot.
Input/output linearization
A well-known systematic approach to the design of trajectory tracking controllers is based on input/output linearization via feedback (see Sect. 8.5.2).
In the case of the unicycle, consider the following outputs:
y1 = x + b cos θ
y2 = y + b sin θ,
with b = 0. They represent the Cartesian coordinates of a point B located
along the sagittal axis of the unicycle at a distance |b| from the contact point
of the wheel with the ground (see Fig. 11.3). In particular, B is ‘ahead’ of the
contact point if b is positive and ‘behind’ if it is negative.
The time derivatives of y1 and y2 are
  
 
 
ẏ1
cos θ −b sin θ
v
v
=
= T (θ)
.
(11.74)
sin θ b cos θ
ω
ω
ẏ2
Matrix T (θ) has determinant b, and is therefore invertible under the assumption that b = 0. It is then suﬃcient to use the following input transformation
  
 
 
u1
cos θ
sin θ
u1
v
−1
=
= T (θ)
− sin θ/b cos θ/b
u2
u2
ω

508

11 Mobile Robots
HUURUHFDUWHVLDQR







>P@

>P@
















>P@












>V@

Fig. 11.14. Tracking a circular reference trajectory (dotted ) with the controller
based on approximate linearization; left: Cartesian motion of the unicycle, right:
time evolution of the norm of the Cartesian error ep

to put the equations of the unicycle in the form
ẏ1 = u1
ẏ2 = u2
u2 cos θ − u1 sin θ
.
θ̇ =
b

(11.75)

An input/output linearization via feedback has therefore been obtained. At
this point, a simple linear controller of the form
u1 = ẏ1d + k1 (y1d − y1 )

(11.76)

u2 = ẏ2d + k2 (y2d − y2 ),

(11.77)

with k1 > 0, k2 > 0, guarantees exponential convergence to zero of the Cartesian tracking error, with decoupled dynamics on its two components. Note that
the orientation, whose evolution is governed by the third equation in (11.75),
is not controlled. In fact, this tracking scheme does not use the orientation
error; hence, it is based on the output error rather than the state error.
It should be emphasized that the reference Cartesian trajectory for point
B can be arbitrary, and in particular the associated path may exhibit isolated
points with discontinuous geometric tangent (like in a broken line) without
requiring the robot to stop and reorient itself at those points. This is true as
long as b = 0, and therefore such a possibility does not apply to the contact
point between the wheel and the ground, whose velocity, as already discussed,
cannot have a component in the direction orthogonal to the sagittal axis of
the vehicle.
Simulations
An example of application of the trajectory tracking controller (11.66), (11.67)
based on linear approximation is shown in Fig. 11.14. The desired trajectory

11.6 Motion Control

509

HUURUHFDUWHVLDQR









>P@

>P@





























>P@


>V@







Fig. 11.15. Tracking a ﬁgure of eight-shaped reference trajectory (dotted ) with the
nonlinear controller; left: Cartesian motion of the unicycle, right: time evolution of
the norm of the Cartesian error ep

in this case is the circle of centre (xc , yc ) and radius R described by the
parametric equations
xd (t) = xc + R cos (ωd t)
yd (t) = yc + R sin (ωd t),
with R = 3 m and ωd = 1/3 rad/s. Hence, the reference driving velocity on
the circle is constant and equal to vd = R ωd = 1 m/s. The controller gains
have been chosen according to (11.69) with ζ = 0.7 and a = 1. Note the
exponential convergence to zero of the Cartesian error.
In the second simulation (Fig. 11.15) the reference trajectory is ﬁgure
of eight-shaped, with centre in (xc , yc ), and is described by the parametric
equations
xd (t) = xc + R1 sin (2ωd t)
yd (t) = yc + R2 sin (ωd t),
with R1 = R2 = 3 m and ωd = 1/15 rad/s. The reference driving velocity vd (t)
in this case varies over time and must be computed using (11.59). The results
shown have been obtained with the nonlinear controller (11.71), (11.72), with
k1 and k3 again given by (11.69) in which ζ = 0.7 and a = 1, while k2 has
been set to 1. Also in this case, the error converges to zero very quickly.
The third Cartesian reference trajectory is a square with a side of 4 m, to
be traced with constant velocity. This requirement means that the unicycle
cannot stop at the vertices in order to reorient itself, and therefore the representative point (x, y) of the unicycle cannot follow the reference trajectory.
As a consequence, the tracking scheme based on input/output linearization
has been adopted. In particular, two simulations are presented, both obtained
using the control law (11.76), (11.77) with k1 = k2 = 2. In the ﬁrst, the point
B that tracks the reference trajectory is located at a distance b = 0.75 m from
the contact point between the wheel and the ground. As shown in Fig. 11.16,

510

11 Mobile Robots
GULYLQJYHORFLW\

>PV@










>P@



























VWHHULQJYHORFLW\
>UDGV@










>P@





















>V@



Fig. 11.16. Tracking a square reference trajectory (dotted ) with the controller based
on input/output linearization, with b = 0.75; left: Cartesian motion of the unicycle,
right: time evolution of the velocity inputs v and ω
GULYLQJYHORFLW\

>PV@







>P@











>V@
VWHHULQJYHORFLW\
























>UDGV@










>P@













>V@



Fig. 11.17. Tracking a square reference trajectory (dotted ) with the controller based
on input/output linearization, with b = 0.2; left: Cartesian motion of the unicycle,
right: time evolution of the velocity inputs v and ω

the unicycle actually moves on a ‘smoothed’ trajectory; therefore, an obstaclefree channel around the trajectory must be available to account for the area
swept by the unicycle in correspondence of the square vertices.
The second simulation (Fig. 11.17) shows what can happen when b is
reduced in order to achieve more accurate tracking for the unicycle representative point; in particular, b = 0.2 m was chosen in this case. While it is true
that the unicycle tracks the square more closely with respect to the ﬁrst simulation, the steering velocity is much higher in correspondence of the vertices,
and this might be a problem in the presence of saturations on the velocity
inputs. This situation is consistent with the fact that matrix T in (11.74)
tends to become singular when b approaches zero.
11.6.2 Regulation
Consider now the problem of designing a feedback control law that drives the
unicycle (11.13) to a desired conﬁguration q d . A reasonable approach could

11.6 Motion Control

511

be to plan ﬁrst a trajectory that stops in q d , and then track it via feedback.
However, none of the tracking schemes so far described can be used to this end.
In fact, the controller based on approximate linearization and the nonlinear
controller both require a persistent state trajectory. The scheme based on
input/output linearization can also track non-persistent trajectories, but will
drive point B to the destination rather than the representative point of the
unicycle. Besides, the ﬁnal orientation at the destination will not be controlled.
Actually, the diﬃculty of identifying feedback control laws for tracking
non-persistent trajectories is structural. It is in fact possible to prove that, due
to the nonholonomy of the system, the unicycle does not admit any universal
controller, i.e., a controller that can asymptotically stabilize arbitrary state
trajectories, either persistent or not. This situation is completely diﬀerent from
the case of manipulators, for which the scheme based on inverse dynamics is
an example of universal controller. As a consequence, the posture regulation
problem in nonholonomic mobile robots must be addressed using purposely
designed control laws.
Cartesian regulation
Consider ﬁrst the problem of designing a feedback controller for a partial regulation task, in which the objective is to drive the unicycle to a given Cartesian
position, without specifying the ﬁnal orientation. This simpliﬁed version of the
regulation problem is of practical interest. For example, a mobile robot exploring an unknown environment must visit a sequence of Cartesian positions
(view points) from where it perceives the characteristics of the surrounding
area using its on-board sensors. If these are isotropically distributed on the
robot (as in the case of a ring of equispaced ultrasonic sensors, a rotating laser
range ﬁnder, or a panoramic camera), the orientation of the robot at the view
point is irrelevant.
Without loss of generality, assume that the desired Cartesian position is
the origin; the Cartesian error ep is then simply [ −x −y ]T . Consider the
following control law
v = −k1 (x cos θ + y sin θ)
ω = k2 (Atan2(y, x) − θ + π),

(11.78)
(11.79)

where k1 > 0, k2 > 0. These two commands have an immediate geometric
interpretation: the driving velocity v is proportional to the projection of the
Cartesian error ep on the sagittal axis of the unicycle, whereas the steering
velocity ω is proportional to the diﬀerence between the orientation of the
unicycle and that of vector ep (pointing error ).
Consider the following ‘Lyapunov-like’ function:
V =

1 2
(x + y 2 ),
2

512

11 Mobile Robots

that is only positive semi-deﬁnite at the origin, because it is zero in all conﬁgurations such that x = y = 0, independently from the value of the orientation θ.
Using the unicycle equations in (11.13) and the control inputs (11.78), (11.79)
one obtains
V̇ = −k1 (x cos θ + y sin θ)2 ,
that is negative semi-deﬁnite at the origin. This indicates that V , which is
bounded below, tends to a limit value, and also that the position error ep is
bounded in norm. It is easy to verify that V̈ is also bounded, and thus V̇ is
uniformly continuous. Barbalat lemma10 implies that V̇ tends to zero. Hence
lim (x cos θ + y sin θ) = 0,

t→∞

i.e., the projection of the Cartesian error vector ep on the sagittal axis of the
unicycle tends to vanish. This cannot happen in a point diﬀerent from the
origin, because the steering velocity (11.79) would then force the unicycle to
rotate so as to align with ep . One may then conclude that the Cartesian error
tends to zero for any initial conﬁguration.
Posture regulation
To design a feedback controller that is able to regulate the whole conﬁguration vector (Cartesian position and vehicle orientation) of the unicycle, it
is convenient to formulate the problem in polar coordinates. It is again assumed, without loss of generality, that the desired conﬁguration is the origin
q d = [ 0 0 0 ]T .
With reference to Fig. 11.18, let ρ be the distance between the representative point (x, y) of the unicycle and the origin of the Cartesian plane, γ the
angle between vector ep and the sagittal axis of the vehicle, and δ the angle
between the same vector and the x axis. In formulae:
ρ = x2 + y 2
γ = Atan2(y, x) − θ + π
δ = γ + θ.
In these coordinates, the kinematic model of the unicycle is expressed as
ρ̇ = −v cos γ
sin γ
γ̇ =
v−ω
ρ
sin γ
δ̇ =
v.
ρ

(11.80)

Note that the input vector ﬁeld associated with v is singular for ρ = 0.
10

La Salle theorem cannot be used because V is not positive deﬁnite at the origin.

11.6 Motion Control

513

r

&
'

?




Fig. 11.18. Deﬁnition of polar coordinates for the unicycle

Deﬁne the feedback control as11
v = k1 ρ cos γ
ω = k2 γ + k1

(11.81)
sin γ cos γ
(γ + k3 δ),
γ

(11.82)

with k1 > 0, k2 > 0. The kinematic model (11.80) under the action of the control law (11.81), (11.82) asymptotically converges to the desired conﬁguration
[ ρ γ δ ]T = [ 0 0 0 ]T .
The proof of this result is based on the following Lyapunov candidate:
V =


1 2
ρ + γ 2 + k3 δ 2 ,
2

whose time derivative along the closed-loop system trajectories
V̇ = −k1 cos 2 γ ρ2 − k2 γ 2
is negative semi-deﬁnite. As a consequence, V tends to a limit value and the
system state is bounded. It can also be shown that V̈ is bounded, so that V̇
is uniformly continuous. In view of Barbalat lemma, it can be inferred that
V̇ tends to zero, and likewise do ρ and γ. Further analysis of the closed-loop
system leads to concluding that δ converges to zero as well.
Note that angles γ and δ are undeﬁned for x = y = 0. They are, however,
always well-deﬁned during the motion of the unicycle, and asymptotically tend
to the desired zero value.
11

It is easy to verify that the expression (11.81) for v coincides with (11.78), the
driving velocity prescribed by the Cartesian regulation scheme, except for the
presence of ρ, whose eﬀect is to modulate v according to the distance of the robot
from the destination. As for the steering velocity, (11.82) diﬀers from (11.79) for
the presence of the second term, that contains the orientation error θ (through
δ) in addition to the pointing error.

514

11 Mobile Robots

It should be noted that the control law (11.81), (11.82), once mapped back
to the original coordinates, is discontinuous at the origin of the conﬁguration
space C. As a matter of fact, it can be proven that any feedback law that can
regulate the posture of the unicycle must be necessarily discontinuous with
respect to the state and/or time-varying.12
Simulations
To illustrate the characteristics of the above regulation schemes, simulation
results are now presented for two parking manoeuvres performed in feedback
by a unicycle mobile robot.
Figure 11.19 shows the robot trajectories produced by the Cartesian regulator (11.78), (11.79), with k = 1 and k2 = 3, for two diﬀerent initial conﬁgurations. Note that the ﬁnal orientation of the robot varies with the approach
direction, and that the unicycle reaches the destination in forward motion,
after inverting its motion at most once (like in the second manoeuvre). It is
possible to prove that such behaviour is general with this controller.
The results of the application of the posture regulator (11.81), (11.82)
starting from the same initial conditions are shown in Fig. 11.20. The gains
have been set to k1 = 1, k2 = 2.5 and k3 = 3. The trajectories obtained are
quite similar to the previous ones, but as expected the orientation is driven
to zero as well. As before, the ﬁnal approach to the destination is always in
forward motion, with at most one motion inversion in the transient phase.

11.7 Odometric Localization
The implementation of any feedback controller requires the availability of the
robot conﬁguration at each time instant. Unlike the case of manipulators,
in which the joint encoders provide a direct measurement of the conﬁguration, mobile robots are equipped with incremental encoders that measure the
rotation of the wheels, but not directly the position and orientation of the
vehicle with respect to a ﬁxed world frame. It is therefore necessary to devise
a localization procedure that estimates in real time the robot conﬁguration.
Consider a unicycle moving under the action of velocity commands v and
ω that are constant within each sampling interval. This assumption, which
12

This result, which actually applies to all nonholonomic robots, derives from the
application of a necessary condition (Brockett theorem) for the smooth stabilizability of control systems. In the particular case of a driftless system of the
form (11.10), in which there are fewer inputs than states and the input vector
ﬁelds are linearly independent, such a condition is violated and no control law
that is continuous in the state q can asymptotically stabilize an equilibrium point.
Brockett theorem does not apply to time-varying stabilizing controllers that may
thus be continuous in q.

11.7 Odometric Localization







>P@

>P@

515
















>P@










>P@





Fig. 11.19. Regulation to the origin of the Cartesian position of the unicycle with
the controller (11.78), (11.79), for two diﬀerent initial conﬁgurations




>P@

>P@

















>P@










>P@





Fig. 11.20. Regulation to the origin of the posture of the unicycle with the controller (11.81), (11.82), for two diﬀerent initial conﬁgurations

is generally satisﬁed13 in digital control implementations, implies that during
the interval the robot moves along an arc of circle of radius R = vk /ωk , which
degenerates to a line segment if ωk = 0. Assume that the robot conﬁguration
q(tk ) = q k at the sampling time tk is known, together with the value of
the velocity inputs vk and ωk applied in the interval [tk , tk+1 ). The value
of the conﬁguration variables q k+1 at the sampling time tk+1 can then be
reconstructed by forward integration of the kinematic model (11.13).
A ﬁrst possibility is to use the approximate formulae
xk+1 = xk + vk Ts cos θk
yk+1 = yk + vk Ts sin θk

(11.83)

θk+1 = θk + ωk Ts ,
where Ts = tk+1 −tk is the duration of the sampling interval. These equations,
which correspond to the use of the Euler method for numerical integration
13

In particular, this is certainly true if the velocity commands computed by the
digital controller are converted to control inputs for the robot through a zeroorder hold (ZOH).

516

11 Mobile Robots

of (11.13), introduce an error in the computation of xk+1 and yk+1 , that is
performed as if the orientation θk remained constant throughout the interval.
This error becomes smaller as Ts is decreased. The third formula instead is
exact.
If the accuracy of the Euler method proves to be inadequate, one may use
the following estimate with the same Ts :


ωk Ts
xk+1 = xk + vk Ts cos θk +
2


ωk Ts
yk+1 = yk + vk Ts sin θk +
(11.84)
2
θk+1 = θk + ωk Ts ,
corresponding to the adoption of the second-order Runge–Kutta integration
method. Note how the ﬁrst two formulae use the average value of the unicycle
orientation in the sampling interval.
To obtain an exact reconstruction of q k+1 under the assumption of constant velocity inputs within the sampling interval one may use simple geometric arguments or exploit the transformability of the unicycle kinematic model
in the chained form (11.25). As already seen, this form is easily integrable
in closed form, leading to an exact expression for z k+1 . The conﬁguration
q k+1 can then be computed by inverting the coordinate and input transformations (11.23) and (11.24). This procedure, which can be generalized to any
mobile robot that can be put in chained form, provides the formulae:
vk
(sin θk+1 − sin θk )
ωk
vk
= yk −
(cos θk+1 − cos θk )
ωk
= θ k + ωk Ts .

xk+1 = xk +
yk+1
θk+1

(11.85)

Note that the ﬁrst two are still deﬁned for ωk = 0; in this case, they coincide
with the corresponding formulae of Euler and Runge–Kutta methods (which
are exact over line segments). In the implementation, however, it is necessary
to handle this situation with a conditional instruction.
Figure 11.21 allows a comparison among the conﬁgurations q k+1 reconstructed via the three aforementioned integration methods. In practice, the
diﬀerence is obviously much less dramatic, and tends to disappear as the duration Ts of the sampling interval is decreased.
In the previous formulae it has been assumed that the velocity inputs vk
and ωk applied in the sampling interval are available. In view of the nonideality of any actuation system, rather than relying on the ‘commanded’
values, it is convenient to reconstruct vk and ωk using the robot proprioceptive
sensors. First of all, note that
vk Ts = Δs

ωk Ts = Δθ

vk
Δs
,
=
ωk
Δθ

(11.86)

11.7 Odometric Localization

517

R>Ld
R>Ld

R>Ld

X
X

X
p

R>

p

p

R>

R>






Fig. 11.21. Odometric localization for a unicycle moving along an elementary tract
corresponding to an arc of circle; left: integration via Euler method, centre: integration via Runge–Kutta method, right: exact integration

where Δs is the length of the elementary tract travelled by the robot and Δθ
is the total variation of orientation along the tract. For example, in the case
of a diﬀerential drive unicycle, denote by ΔφR and ΔφL the rotation of the
right and left wheel, respectively, as measured by the incremental encoders
during the sampling interval. From (11.14) one easily ﬁnds
Δs =

r
(ΔφR + ΔφL )
2

Δθ =

r
(ΔφR − ΔφL )
d

that, used in (11.86), allow the implementation of all the previous formulae
for the reconstruction of q k+1 .
The forward integration of the kinematic model using the velocity commands reconstructed via the proprioceptive sensors — the encoders of the
wheel actuators — is referred to as odometric localization or passive localization or dead reckoning — the latter is a term of uncertain etymology used in
marine navigation. This method, relying on the iterated use of the previous
formulae starting from an estimate of the initial conﬁguration, provides an
estimate whose accuracy cannot be better than that of q 0 . In any case, odometric localization — independently from the adopted integration method —
is subject in practice to an error that grows over time (drift) and quickly becomes signiﬁcant over suﬃciently long paths. This error is the result of several
causes, that include wheel slippage, inaccuracy in the calibration of kinematic
parameters (e.g., the radius of the wheels), as well as the numerical error introduced by the integration method, if Euler or Runge–Kutta methods are
used. It should also be noted that, once an odometric localization technique
has been chosen, its performance also depends on the speciﬁc kinematic arrangement of the robot; for example, diﬀerential drive is usually better than
synchro drive in this respect.
A more robust solution is represented by active localization methods. For
example, this kind of approach can be adopted when the robot is equipped
with proximity exteroceptive sensors (like a laser range ﬁnder) and knows a

518

11 Mobile Robots

map of the workspace, either given in advance or built by the robot itself
during the motion. It is then possible to correct the estimate provided by
the passive localization methods by comparing the expected measures of the
exteroceptive sensors with the actual readings. These techniques, which make
use of tools from Bayesian estimation theory such as the Extended Kalman
Filter or the Particle Filter , provide greater accuracy than pure odometric
localization and are therefore essential in navigation tasks over long paths.

Bibliography
The literature on mobile robots is very rich and includes some comprehensive
treatises, among which one of the most recent is [210].
Many books deal with the realization of mobile robots, with particular emphasis on electro-mechanical design and sensor equipment, e.g., [106, 72, 21].
For the modelling of systems subject to nonholonomic constraints, see [164].
A general classiﬁcation of wheeled mobile robots based on the number, placement and type of wheels is proposed in [18], where the derivation of the
kinematic and dynamic models is also presented.
Conditions for transforming driftless systems in chained form are discussed
in detail in [159], while a general reference on ﬂat outputs is [80]. The presented schemes for Cartesian trajectory tracking based on linear and nonlinear
control are taken from [34], while the method for posture regulation based on
polar coordinates was proposed in [2]. Complete (input/state) linearization of
the unicycle kinematic model can be obtained using a dynamic feedback law,
as shown for example in [174]. The non-existence of a universal controller for
nonholonomic robots is proven in [135].
A detailed extension of some of the planning and control techniques described in this chapter to the case of bicycle-like kinematics is given in [58].
The design of time-varying and/or discontinuous feedback control laws for posture regulation in mobile robots was addressed in many works, including [193]
and [153].
The reader interested in sensor-based robot localization and map building
can refer, e.g., to [231].

Problems
11.1. Consider the mobile robot obtained by connecting N trailers to a rearwheel drive tricycle. Each trailer is a rigid body with an axle carrying two ﬁxed
wheels, that can be assimilated to a single wheel located at the midpoint of the
axle, and is hinged to the midpoint of the preceding axle through a revolute
joint. Find a set of generalized coordinates for the robot.

Problems

519

11.2. Consider an omnidirectional mobile robot having three Mecanum wheels
placed at the vertices of an equilateral triangle, each oriented in the direction
orthogonal to the bisectrix of its angle. Let q1 and q2 be the Cartesian coordinates of the centre of the robot, q3 the vehicle orientation, while q4 , q5 , and
q6 represent the angle of rotation of each wheel around its axis. Also, denote
by r the radius of the wheels and by  the distance between the centre of the
robot and the centre of each wheel. This mechanical system is subject to the
following Pfaﬃan constraints:
⎡
⎤
⎡ ⎤
q̇4
q̇1
AT1 (q) ⎣ q̇2 ⎦ + AT2 ⎣ q̇5 ⎦ = 0,
q̇3
q̇6
where

⎡√

3
1
⎢ 2 cos q3 −√2 sin q3
⎢
A1 (q) = ⎢ 1
3
⎣ cos q3 +
sin q3
2
2


sin q3
−cos q3


√
⎤
3
1
cos q3 ⎥
− sin q3 −
2
√2
⎥
⎥
1
3
cos q3 −
sin q3 ⎦
2
2


and A2 = r I 3 . Show that this set of constraints is partially integrable, and
that in particular the orientation of the vehicle is a linear function of the wheel
rotation angles. [Hint: add the kinematic constraints side-by-side.]
11.3. Show that, for a single kinematic constraint in the form (11.7), the
integrability condition expressed by (11.9) coincides with the involutivity of
the distribution Δ = span{g 1 , . . . , g n−1 } associated with the input vector
ﬁelds of the corresponding kinematic model.
11.4. Using the controllability condition (11.11), show that a set of Pfaﬃan
constraints that does not depend on the generalized coordinates is always
integrable.
11.5. With reference to the kinematic model (11.13) of the unicycle, consider
the following sequence of velocity inputs:
v(t) = 1 ω(t) = 0,
v(t) = 0 ω(t) = 1
v(t) = −1 ω(t) = 0,
v(t) = 0 ω = −1

t ∈ [0, ε)
t ∈ [ε, 2ε)
t ∈ [2ε, 3ε)
t ∈ [3ε, 4ε).

By forward integration of the model equations, show that when ε is inﬁnitesimal the displacement of the unicycle in conﬁguration space is aligned with
the Lie bracket of the input vector ﬁelds.
11.6. Prove the relationships (11.14) that allow the transformation of the
velocity inputs of a diﬀerential drive vehicle to those of the equivalent unicycle.

520

11 Mobile Robots

[Hint: for the velocity of the midpoint of the segment joining the two wheels,
it is suﬃcient to diﬀerentiate with respect to time the Cartesian coordinates
of such a point expressed as a function of the coordinates of the wheel centres.
As for ω, use the formula (B.4) for the composition of the velocities of a rigid
body.]
11.7. For a generic conﬁguration of a bicycle mobile robot, determine the
Cartesian position of the instantaneous centre of rotation, and derive the
expression of the angular velocity of the body as a function of the robot
conﬁguration q and of the modulus of the velocity of rear wheel centre. In the
particular case of the rear-wheel drive bicycle, show that such expression is
consistent with the evolution of θ predicted by the kinematic model (11.19).
Moreover, compute the velocity vP of a generic point P on the robot chassis.
11.8. Derive the kinematic model of the tricycle robot towing N trailers considered in Problem 11.1. Denote by  the distance between the front wheel
and rear wheel axle of the tricycle, and by i the joint-to-joint length of the
i-th trailer.
11.9. Consider a diﬀerential drive robot whose angular speed inputs — one
for the right wheel and one for the left wheel — are subject to the following
bounds:
|ωL (t)| ≤ ωRL
∀t,
|ωR (t)| ≤ ωRL
that correspond to a square admissible region in the ωR , ωL plane. Derive the
expression of the resulting constraints for the driving and steering velocity v
and ω of the equivalent unicycle model. In particular, show that the admissible
region in the v, ω plane is a rhombus.
11.10. Compute the expression of matrix D(Δ) and vector d(z i , z f , Δ)
in (11.50) for the case of the (2,4) chained form.
11.11. Modify the path planning scheme based on parameterized inputs so
as to obtain sf = 1.
11.12. Show that the path planning schemes based on polynomials of diﬀerent
degree and on parameterized inputs give exactly the same result in the case
of the (2,3) chained form.
11.13. Implement in a computer program the path planning method for a
unicycle based on cubic Cartesian polynomials. Use the program to plan a
path leading the robot from the conﬁguration q i = [ 0 0 0 ]T [m,m,rad] to
the conﬁguration q f = [ 2 1 π/2 ]T [m,m,rad]. Then, determine a timing
law over the path so as to satisfy the following velocity bounds:
|v(t)| ≤ 1 m/s

|ω(t)| ≤ 1 rad/s

∀t.

Problems

521

11.14. Formulate the trajectory tracking problem for a (2,3) chained form
and derive the corresponding error dynamics. Derive a feedback controller
using the linear approximation around the reference trajectory.
11.15. Consider the kinematic model (11.19) of the rear-wheel drive bicycle.
In analogy to the case of the unicycle, identify two outputs y1 , y2 for which
it is possible to perform a static input/output linearization. [Hint: consider a
point P on the line passing through the centre of the front wheel and oriented
as the wheel itself.]
11.16. Implement in a computer program the Cartesian regulator (11.78),
(11.79), including a modiﬁcation to allow the unicycle to reach the origin
either in forward or in backward motion. [Hint: modify (11.79) so as to force
the robot to orient itself as vector ep or vector −ep , depending on which choice
is the most convenient.]
11.17. Prove formulae (11.85) for the exact odometric localization of a unicycle under velocity inputs that are constant in the sampling interval.
11.18. Implement in a computer program the unicycle posture regulator
based on polar coordinates, with the state feedback computed through the
Runge–Kutta odometric localization method.

12
Motion Planning

The trajectory planning methods presented in Chaps. 4 and 11, respectively
for manipulators and mobile robots, operate under the simplifying assumption
that the workspace is empty. In the presence of obstacles, it is necessary to plan
motions that enable the robot to execute the assigned task without colliding
with them. This problem, referred to as motion planning, is the subject of
this chapter. After deﬁning a canonical version of the problem, the concept of
conﬁguration space is introduced in order to achieve an eﬃcient formulation. A
selection of representative planning techniques is then presented. The method
based on the notion of retraction characterizes the connectivity of the free
conﬁguration space using a roadmap, i.e., a set of collision-free paths, while
the cell decomposition method identiﬁes a network of channels with the same
property. The PRM and bidirectional RRT techniques are probabilistic in
nature and rely on the randomized sampling of the conﬁguration space and
the memorization of those samples that do not cause a collision between the
robot and the obstacles. The artiﬁcial potential method is also described as
a heuristic approach particularly suited to on-line planning problems, where
the geometry of the workspace obstacles is unknown in advance. The chapter
ends with a discussion of the application of the presented planning methods
to the robot manipulator case.

12.1 The Canonical Problem
Robotic systems are expected to perform tasks in a workspace that is often
populated by physical objects, which represent an obstacle to their motion.
For example, a manipulator working in a robotized cell must avoid collision
with its static structures, as well as with other moving objects that may access
it, such as other manipulators. Similarly, a mobile robot carrying baggage in
an airport has to navigate among obstacles that may be ﬁxed (ﬁttings, conveyor belts, construction elements) or mobile (passengers, workers). Planning
a motion amounts then to deciding which path the robot must follow in order

524

12 Motion Planning

to execute a transfer task from an initial to a ﬁnal posture without colliding with the obstacles. Clearly, one would like to endow the robot with the
capability of autonomously planning its motion, starting from a high-level description of the task provided by the user and a geometric characterization of
the workspace, either made available entirely in advance (oﬀ-line planning) or
gathered by the robot itself during the motion by means of on-board sensors
(on-line planning).
However, developing automatic methods for motion planning is a very
diﬃcult endeavour. In fact, the spatial reasoning that humans instinctively
use to move safely among obstacles has proven hard to replicate and codify in
an algorithm that can be executed by a robot. To this date, motion planning is
still an active topic of research, with contributions coming from diﬀerent areas
such as algorithm theory, computational geometry and automatic control.
To address the study of methods for motion planning, it is convenient to
introduce a version of the problem that highlights its fundamental issues. The
canonical problem of motion planning is then formulated as follows.
Consider a robot B, which may consist of a single rigid body (mobile robot)
or of a kinematic chain whose base is either ﬁxed (standard manipulator) or
mobile (mobile robot with trailers or mobile manipulator). The robot moves
in a Euclidean space W = IRN , with N = 2 or 3, called workspace. Let
O1 , . . . , Op be the obstacles, i.e., ﬁxed rigid objects in W. It is assumed that
both the geometry of B, O1 , . . . , Op and the pose of O1 , . . . , Op in W are
known. Moreover, it is supposed that B is free-ﬂying, that is, the robot is
not subject to any kinematic constraint. The motion planning problem is the
following: given an initial and a ﬁnal posture of B in W, ﬁnd if exists a path,
i.e., a continuous sequence of postures, that drives the robot between the
two postures while avoiding collisions (including contacts) between B and the
obstacles O1 , . . . , Op ; report a failure if such a path does not exist.
In the particular case in which the robot is a single body moving in IR2 ,
the canonical motion planning problem is also known as the piano movers’
problem, as it captures the diﬃculties faced by movers when manoeuvring a
piano (without lifting it) among obstacles. The generalized movers’ problem
is the canonical problem for a single-body robot moving in IR3 .
Clearly, some of the hypotheses of the canonical problem may not be satisﬁed in applications. For example, the assumption that the robot is the only
object in motion in the workspace rules out the relevant case of moving obstacles (e.g., other robots). Advance knowledge of obstacle geometry and placement is another strong assumption: especially in unstructured environments,
which are not purposely designed to accommodate robots, the robot itself is
typically in charge of detecting obstacles by means of its sensors, and the planning problem must therefore be solved on-line during the motion. Moreover, as
shown in Chap. 11, the free-ﬂying robot hypothesis does not hold in nonholonomic mechanical systems, which cannot move along arbitrary paths in the
workspace. Finally, manipulation and assembly problems are excluded from
the canonical formulation since they invariably involve contacts between rigid

12.2 Conﬁguration Space

525

bodies. As a matter of fact, all the above assumptions are introduced in order
to reduce motion planning to the purely geometrical — but still quite diﬃcult
— problem of generating a collision-free path. However, many methods that
successfully solve this simpliﬁed version of the problem lend themselves to an
extension to more general versions.
The notion of conﬁguration space is essential to obtain a more convenient
formulation of the canonical motion planning problem, as well as to envisage
approaches to its solution.

12.2 Conﬁguration Space
A very eﬀective scheme for motion planning is obtained by representing the
robot as a mobile point in an appropriate space, where the images of the
workspace obstacles are also reported. To this end, it is natural to refer to the
generalized coordinates of the mechanical system, whose value identiﬁes the
conﬁguration of the robot (see Sect. B.4). This associates to each posture of the
latter a point in the conﬁguration space C, i.e., the set of all the conﬁgurations
that the robot can assume.
Generalized coordinates of robots are essentially of two types. Cartesian
coordinates are used to describe the position of selected points on the links of
the kinematic chain and take value in Euclidean spaces. Angular coordinates
are used to represent the orientations of the bodies; independently from the
adopted representation (rotation matrices, Euler angles, quaternions), they
take values in SO(m) (m = 2, 3), the special orthonormal group of real (m×m)
matrices with orthonormal columns and determinant equal to 1 (see Sect. 2.2).
It is well known that a minimal parameterization of SO(m) requires m(m −
1)/2 parameters. The conﬁguration space of a robot is then obtained in general
as a Cartesian product of these spaces.
Some examples of conﬁguration spaces are presented below:
• The conﬁguration of a polygonal mobile robot in W = IR2 is described
by the position of a representative point on the body (e.g., a vertex) and
by the orientation of the polygon, both expressed with respect to a ﬁxed
reference frame. The conﬁguration space C is then IR2 × SO(2), whose
dimension is n = 3.
• For a polyhedral mobile robot in W = IR3 , the conﬁguration space C is
IR3 × SO(3), whose dimension is n = 6.
• For a ﬁxed-base planar manipulator with n revolute joints, the conﬁguration space is a subset of (IR2 × SO(2))n . The dimension of C equals the
dimension of (IR2 × SO(2))n minus the number of constraints due to the
presence of the joints, i.e., 3n−2n = n. In fact, in a planar kinematic chain,
each joint imposes two holonomic constraints on the following body.
• For a ﬁxed-base spatial manipulator with n revolute joints, the conﬁguration space is a subset of (IR3 ×SO(3))n . Since in this case each joint imposes
ﬁve constraints on the following body, the dimension of C is 6n − 5n = n.

526

12 Motion Planning

R1

a

1$

7
R4
R\
(

R4
1$

R1
R\

Rd

Rd

Fig. 12.1. The conﬁguration space of a 2R manipulator; left: a locally valid representation as a subset of IR2 , right: a topologically correct representation as a
two-dimensional torus

• For a unicycle-like vehicle with a trailer in IR2 , the conﬁguration space is
a subset of (IR2 × SO(2)) × (IR2 × SO(2)). If the trailer is connected to the
unicycle by a revolute joint, the conﬁguration of the robot can be described
by the position and orientation of the unicycle and the orientation of the
trailer. The dimension of C is therefore n = 4.
If n is the dimension of C, a conﬁguration in C can be described by a
vector q ∈ IRn . However, this description is only valid locally: the geometric
structure of the conﬁguration space C is in general more complex than that
of a Euclidean space, as shown in the following example.

Example 12.1
Consider the planar manipulator with two revolute joints (2R manipulator) of
Fig. 2.14. The conﬁguration space has dimension 2, and may be locally represented
by IR2 , or more precisely by its subset
Q = {q = (q1 , q2 ) : q1 ∈ [0, 2π), q2 ∈ [0, 2π)}.
This guarantees that the representation is injective, i.e., that a single value of q
exists for each manipulator posture. However, this representation is not topologically
correct: for example, the conﬁgurations denoted as q A and q B in Fig. 12.1, left, which
correspond to manipulator postures that are ‘close’ in the workspace W, appear to
be ‘far’ in Q. To take this into account, one should ‘fold’ the square Q onto itself
(so as to make opposite sides meet) in sequence along its two axes. This procedure
generates a ring, properly called torus, which can be visualized as a two-dimensional
surface immersed in IR3 (Fig. 12.1, right). The correct expression of this space is
SO(2) × SO(2).

12.2 Conﬁguration Space

527

When the conﬁguration of a robot (either articulated or mobile) includes
angular generalized coordinates, its conﬁguration space is properly described
as an n-dimensional manifold , i.e., a space in which a neighbourhood of a
point can be put in correspondence with IRn through a continuous bijective
function whose inverse is also continuous (a homeomorphism).
12.2.1 Distance
Having discussed the nature of the robot conﬁguration space, it is useful to
deﬁne a distance function in C. In fact, the planning methods that will be
discussed in the following make use of this notion.
Given a conﬁguration q, let B(q) be the subset of the workspace W occupied by the robot B, and p(q) be the position in W of a point p on B. Intuition
suggests that the distance between two conﬁgurations q 1 and q 2 should go
to zero when the two regions B(q 1 ) and B(q 2 ) tend to coincide. A deﬁnition
that satisﬁes this property is
d1 (q 1 , q 2 ) = max p(q 1 ) − p(q 2 ),
p∈B

(12.1)

where  ·  denotes the Euclidean distance in W = IRN . In other words, the
distance between two conﬁgurations in C is the maximum displacement in W
they induce on a point, as the point moves all over the robot.
However, the use of function d1 is cumbersome, because it requires characterizing the volume occupied by the robot in the two conﬁgurations and the
computation of the maximum distance in W between corresponding points.
For algorithmic purposes, the simple Euclidean norm is often chosen as a
conﬁguration space distance:
d2 (q 1 , q 2 ) = q 1 − q 2 .

(12.2)

Nevertheless, one must keep in mind that this deﬁnition is appropriate only
when C is a Euclidean space. Going back to Example 12.1, it is easy to realize that, unlike d1 (q A , q B ), the Euclidean norm d2 (q A , q B ) does not represent
correctly the distance on the torus. A possible solution is to modify the deﬁnition of d2 by suitably computing diﬀerences of angular generalized coordinates
(see Problem 12.2).
12.2.2 Obstacles
In order to characterize paths that represent a solution to the canonical motion
planning problem — those that avoid collisions between the robot and the
workspace obstacles — it is necessary to build the ‘images’ of the obstacles in
the conﬁguration space of the robot.
In the following, it is assumed that the obstacles are closed (i.e., they
contain their boundaries) but not necessarily limited subsets of W. Given an

528

12 Motion Planning

obstacle Oi (i = 1, . . . , p) in W, its image in conﬁguration space C is called
C-obstacle and is deﬁned as
COi = {q ∈ C : B(q) ∩ Oi = ∅}.

(12.3)

In other words, COi is the subset of conﬁgurations that cause a collision
(including simple contacts) between the robot B and the obstacle Oi in the
workspace. The union of all C-obstacles
CO =

p
2

COi

(12.4)

i=1

deﬁnes the C-obstacle region, while its complement
2

p
Oi = ∅}
Cfree = C − CO = {q ∈ C : B(q) ∩

(12.5)

i=1

is the free conﬁguration space, that is, the subset of robot conﬁgurations that
do not cause collision with the obstacles. A path in conﬁguration space is
called free if it is entirely contained in Cfree .
Although C in itself is a connected space — given two arbitrary conﬁguration there exists a path that joins them — the free conﬁguration space Cfree
may not be connected as a consequence of occlusions due to C-obstacles. Note
also that the assumption of free-ﬂying robot in the canonical problem means
that the robot can follow any path in the free conﬁguration space Cfree .
It is now possible to give a more compact formulation of the canonical motion planning problem. Assume that the initial and ﬁnal posture of the robot
B in W are mapped to the corresponding conﬁgurations in C, respectively
called start conﬁguration q s and goal conﬁguration q g . Planning a collisionfree motion for the robot means then generating a safe path between q s and
q g if they belong to the same connected component of Cfree , and reporting a
failure otherwise.
12.2.3 Examples of Obstacles
In the following, the C-obstacle generation procedure is presented in some
representative cases. For the sake of simplicity, it is assumed that obstacles in
W are either polygonal or polyhedral.

Example 12.2
Consider the case of a point robot B. In this case, the conﬁguration of the robot
is described by the coordinates of point B in the workspace W = IRN and the
conﬁguration space C is a copy of W. Similarly, the C-obstacles are copies of the
obstacles in W.

12.2 Conﬁguration Space

y

529

q2
x

q1

Fig. 12.2. C-obstacles for a circular robot in IR2 ; left: the robot B, an obstacle Oi
and the growing procedure for building C-obstacles, right: the conﬁguration space C
and the C-obstacle COi
Example 12.3
If the robot is a sphere1 in W = IRN , its conﬁguration can be described by the
Cartesian coordinates of a representative point, e.g., its centre — note that the
orientation of the sphere is irrelevant for collision checking. Therefore, as in the
previous example, the conﬁguration space C is a copy of the workspace W. However,
the C-obstacles are no longer simple copies of the obstacles in W, and they must
be built through a growing procedure. In particular, the boundary of the C-obstacle
COi is the locus of conﬁgurations that put the robot is in contact with the obstacle
Oi , and it can be obtained as the surface described by the representative point as the
robot slides on the boundary of Oi . As a consequence, to build COi it is suﬃcient
to grow Oi isotropically by the radius of the robot. This procedure is shown in
Fig. 12.2 for the case N = 2 (circular robot in IR2 ); in this case, each C-obstacle is
a generalized polygon, i.e., a planar region whose boundary consists of line segments
and/or circular arcs. If the representative point of the robot is diﬀerent from the
centre of the sphere, the growing procedure is not isotropic.
Example 12.4
Consider now the case of a polyhedral robot that is free to translate (with a ﬁxed
orientation) in IRN . Its conﬁguration can be described by the Cartesian coordinates
of a representative point, for example a vertex of the polyhedron. Therefore, the
conﬁguration space C is again a copy of IRN . Again, a growing procedure must be
applied to the workspace obstacles to obtain their image in the conﬁguration space.
In particular, the boundary of the C-obstacle COi is the surface described by the
representative point of the robot when the robot B slides at a ﬁxed orientation on
the boundary of Oi . Figure 12.3 shows this procedure for the case N = 2 (polygonal
robot in IR2 ). The resulting shape of COi depends on the position of the representative point on the robot, but in any case the C-obstacle is itself a polyhedron. COi
has in general a larger number of vertices than Oi , and is a convex polyhedron provided that B and Oi are convex. Note also that, although the result of the growing
1

For simplicity, the term sphere will be used in Euclidean spaces of arbitrary
dimension n in place of n–sphere.

530

12 Motion Planning

B

Oi

y
x

q2
q1

Fig. 12.3. C-obstacles for a polygonal robot translating in IR2 ; left: the robot B, an
obstacle Oi and the growing procedure for building C-obstacles, right: the conﬁguration space C and the C-obstacle COi
procedure — and thus the shape of the C-obstacles — depends on the choice of
the representative point on the robot, all the obtained planning problems in conﬁguration space are equivalent. In particular, the existence of a solution for any of
them implied the existence of a solution for all the others. Moreover, to each path in
conﬁguration space that solves one of these problems corresponds a (diﬀerent) free
path in any of the others, to which the same motion of the robot in the workspace
is associated.
Example 12.5
For a polyhedral robot that can translate and rotate in IRN , the dimension of the
conﬁguration space is increased with respect to the previous example, because it is
also necessary to describe the orientation DOFs. For example, consider the case of a
polygon that can translate and rotate in IR2 . The conﬁguration of the robot can be
characterized by the Cartesian coordinates of a representative point (for example,
a vertex of the polygon) and an angular coordinate θ representing the orientation
of the polygon with respect to a ﬁxed reference frame. The conﬁguration space C is
then IR2 ×SO(2), which can be locally represented by IR3 . To build the image in C of
an obstacle Oi , one should in principle repeat the procedure illustrated in Fig. 12.3
for each possible value of the robot orientation θ. The C-obstacle COi is the volume
generated by ‘stacking’ (in the direction of the θ axis) all the constant-orientation
slices thus obtained.
Example 12.6
For a robot manipulator B made by rigid links B1 , . . . , Bn connected by joints, there
exist in principle two kinds of C-obstacles: those that represent the collision between
a body Bi and an obstacle Oj , and those accounting for self-collisions, i.e., interference between two links Bi and Bj of the kinematic chain. Even considering for
simplicity only the ﬁrst type, the procedure for building C-obstacles is much more
complicated than in the previous examples. In fact, to obtain the boundary of the
C-obstacle COi , it is necessary to identify through appropriate inverse kinematics
computations all the conﬁgurations that bring one or more links of the manipulator B in contact with Oi . Figure 12.4 shows the result of the C-obstacle building

12.2 Conﬁguration Space

531

Fig. 12.4. C-obstacles for a wire-frame 2R manipulator in two diﬀerent cases; left:
the robot and the obstacles in W = IR2 , right: the conﬁguration space C and the
C-obstacle region CO

procedure for a wire-frame 2R manipulator in two diﬀerent cases (self-collisions are
not considered); note how, in spite of the simple shape of the obstacles in W, the
proﬁle of the C-obstacles is quite complicated. For simplicity, the conﬁguration space
has been represented as a subset (a square) of IR2 ; however, to correctly visualize
the C-obstacles, one should keep in mind that the correct representation of C is a
two-dimensional torus, so that the upper/lower and left/right sides of the square
are actually coincident. Note also that in the ﬁrst case (top of Fig. 12.4) the images
of the two obstacles in the upper right corner of W merge in a single C-obstacle,
and the free conﬁguration space Cfree consists of a single connected component. In
the second case (bottom of Fig. 12.4) Cfree is instead partitioned into three disjoint
connected components.

532

12 Motion Planning

Whatever the nature of the robot, an algebraic model of the workspace
obstacles is needed (e.g., derived from a CAD model of the workspace) to
compute exactly the C-obstacle region CO. However, except for the most elementary cases, the procedures for generating CO are extremely complex. As
a consequence, it is often convenient to resort to approximate representations
of CO. A simple (although computationally intensive) way to build such a
representation is to extract conﬁguration samples from C using a regular grid,
compute the corresponding volume occupied by the robot via direct kinematics, and ﬁnally identify through collision checking 2 those samples that bring
the robot to collide with obstacles. These samples can be considered as a
discrete representation of CO, whose accuracy can be improved at will by
increasing the resolution of the grid in C.
Some of the motion planning methods that will be presented in this chapter
do not require an explicit computation of the C-obstacle region. In particular,
this is true for the probabilistic planners described in Sect. 12.5, and for the
technique based on artiﬁcial potential ﬁelds and control points discussed in
Sect. 12.7.

12.3 Planning via Retraction
The basic idea of motion planning via retraction is to represent the free conﬁguration space by means of a roadmap R ⊂ Cfree , i.e., a network of paths
that describe adequately the connectivity of Cfree . The solution of a particular
instance of a motion planning problem is then obtained by connecting (retracting) to the roadmap the start conﬁguration q s and the goal conﬁguration q g ,
and ﬁnding a path on R between the two connection points. Depending on
the type of roadmap, and on the retraction procedure, this general approach
leads to diﬀerent planning methods. One of these is described in the following, under the simplifying assumption that Cfree is a limited subset of C = IR2
and is polygonal , i.e., its boundary is entirely made of line segments.3 As the
boundary of Cfree coincides with the boundary of CO, this assumption implies
that the C-obstacle region is itself a polygonal subset of C.
For each conﬁguration q in Cfree , let its clearance be deﬁned as
γ(q) = min q − s,
s∈∂Cfree

(12.6)

where ∂Cfree is the boundary of Cfree . The clearance γ(q) represents the minimum Euclidean distance between the conﬁguration q and the C-obstacle re2

3

Many algorithms based on computational geometry techniques are available (in
the literature, but also implemented in software packages) to test collision in
IR2 o IR3 . The most eﬃcient, such as I-Collide and V-Collide, use a hierarchical
representation of geometric models of bodies, and can speed up collision checking
by re-using the results of previous checks in spatially similar situations.
According to the deﬁnition, a polygonal subset is not necessarily connected, and
may contain ‘holes’.

12.3 Planning via Retraction

R[[S/

wR[[S/DS

533

wR[[SXDS
R[[SX

Fig. 12.5. An example of generalized Voronoi diagram and the solution of a particular instance of the planning problem, obtained by retracting q s and q g on the
diagram. The solution path from q s to q g consists of the two dashed segments and
the thick portion of the diagram joining them

gion. Moreover, consider the set of points on the boundary of Cfree that are
neighbours of q:
N (q) = {s ∈ ∂Cfree : q − s = γ(q)},

(12.7)

i.e., the points on ∂Cfree that determine the value of the clearance for q. With
these deﬁnitions, the generalized 4 Voronoi diagram of Cfree is the locus of its
conﬁgurations having more than one neighbour:
V(Cfree ) = {q ∈ Cfree : card(N (q)) > 1},

(12.8)

in which card(·) denotes the cardinality of a set. Figure 12.5 shows an example
of generalized Voronoi diagram for a polygonal free conﬁguration space; note
how the connectivity of Cfree is well captured by the diagram.
It is easy to show that V(Cfree ) is made of elementary arcs that are either
rectilinear — each made of contiguous conﬁgurations whose clearance is due to
the same pair of edges or vertices — or parabolic — each made of contiguous
conﬁgurations whose clearance is determined by the same pair edge-vertex.
Therefore, one can build an analytical expression of V(Cfree ) starting from
the pair of features (side/side, side/vertex, vertex/vertex) that determine the
4

A proper Voronoi diagram is obtained in the particular case in which the Cobstacles are isolated points.

534

12 Motion Planning

V

N(q)
(q)

n(q)

q

r(q)

Fig. 12.6. The retraction procedure for connecting a generic conﬁguration q in Cfree
to V(Cfree )

appearance of each arc. From an abstract point of view, V(Cfree ) can be considered as a graph having elementary arcs of the diagram as arcs and endpoints
of arcs as nodes.
By construction, the generalized Voronoi diagram has the property of locally maximizing clearance, and is therefore a natural choice as a roadmap
of Cfree for planning motions characterized by a healthy safety margin with
respect to the possibility of collisions. To use V(Cfree ) as a roadmap, a retraction procedure must be deﬁned for connecting a generic conﬁguration in Cfree
to the diagram. To this end, consider the geometric
construction shown in

Fig. 12.6. Since q ∈
/ V(Cfree ), it is card N (q) = 1, i.e., there exists a single
point on the polygonal boundary of CO (either a vertex or a point on a side)
that determines the value of the clearance γ(q). The gradient ∇γ(q), which
identiﬁes the direction of steepest ascent for the clearance at the conﬁguration
q, is directed as the half-line originating in N (q) and passing through q. The
ﬁrst intersection of this half-line with V(Cfree ) deﬁnes r(q), i.e., the connection point of q to the generalized Voronoi diagram. To guarantee that r(·) is
continuous, it is convenient to extend its domain of deﬁnition to all Cfree by
letting r(q) = q if q ∈ V(Cfree ).
From a topological viewpoint, the function r(·) deﬁned above is actually
an example of retraction of Cfree on V(Cfree ), i.e., a continuous surjective map
from Cfree to V(Cfree ) such that its restriction to V(Cfree ) is the identity map.
In view of its deﬁnition, r(·) preserves the connectivity of Cfree , in the sense
that q and r(q) — as well as the segment joining them — always lie in the
same connected component of Cfree . This property is particularly important,
because it is possible to show that, given a generic retraction ρ of Cfree on
a connectivity-preserving roadmap R, there exists a free path between two
conﬁgurations q s and q g if and only if there exists a path on R between ρ(q s )
and ρ(q g ). As a consequence, the problem of planning a path in Cfree reduces
to the problem of planning a path on its retraction R = ρ(Cfree ).

12.3 Planning via Retraction

535

Given the start and goal conﬁgurations q s and q g , the motion planning
method via retraction goes through the following steps (see Fig. 12.5):
1. Build the generalized Voronoi diagram V(Cfree ).
2. Compute the retractions r(q s ) and r(q g ) on V(Cfree ).
3. Search V(Cfree ) for a sequence of consecutive arcs such that r(q s ) belongs
to the ﬁrst and r(q g ) to the last.
4. If the search is successful, replace the ﬁrst arc of the sequence with its
subarc originating in r(q s ) and the last arc of the sequence with its subarc
terminating in r(q g ), and provide as output the path consisting of the
line segment joining q s to r(q s ), the modiﬁed arc sequence, and the line
segment joining q g to r(q g ); otherwise, report a failure.
If simplicity of implementation is desired, the graph search5 required at
Step 3 can be performed using basic strategies such as breadth-ﬁrst or depthﬁrst search. On the other hand, if one wishes to identify the minimum-length
path (among those which can be produced by the method) between q s and
q g , each arc must be labelled with a cost equal to its actual length. The
minimum-cost path can then be computed with an informed algorithm —
i.e., an algorithm using a heuristic estimate of the minimum cost path from a
generic node to the goal, in this case the Euclidean distance — such as A .
Whatever the adopted search strategy, the above motion planning method
via retraction of Cfree on V(Cfree ) is complete, i.e., it is guaranteed to ﬁnd a
solution if one exists, and to report a failure otherwise. Its time complexity is a
function of the number v of vertices of the polygonal region Cfree , and depends
essentially on the construction of the generalized Voronoi diagram (Step 1),
on the retraction of q s and q g (Step 2), and on the search on the diagram
(Step 3). As for Step 1, the most eﬃcient algorithms can build V(Cfree ) in time
O(v log v). The retraction procedure requires O(v), mainly to compute N (q s )
and N (q g ). Finally, since it is possible to prove that V(Cfree ) has O(v) arcs,
the complexity of breadth-ﬁrst or depth-ﬁrst search would be O(v), whereas
A would require O(v log v). Altogether, the time complexity of the motion
planning method via retraction is O(v log v).
It should be noted that, once the generalized Voronoi diagram of Cfree
has been computed, it can be used again to solve quickly other instances
(queries) of the same motion planning problem, i.e., to generate collision-free
paths between diﬀerent start and goal conﬁgurations in the same Cfree . For
example, this is useful when a robot must repeatedly move between diﬀerent
postures in the same static workspace. The motion planning method based on
retraction can then be considered multiple-query. It is also possible to extend
the method to the case in which the C-obstacles are generalized polygons.

5

See Appendix E for a quick survey on graph search strategies and algorithm
complexity.

536

12 Motion Planning

12.4 Planning via Cell Decomposition
Assume that the free conﬁguration space Cfree can be decomposed in simplyshaped regions, called cells, with the following basic characteristics:
• Given two conﬁgurations belonging to the same cell, it is ‘easy’ to compute
a collision-free path that joins them.
• Given two adjacent cells — i.e., two cells having in common a portion of
their boundaries of non-zero measure — it is ‘easy’ to generate a collisionfree path going from one cell to the other.
Starting from one such cell decomposition of Cfree , it is easy to build the
associated connectivity graph. The nodes of this graph represent cells, while an
arc between two nodes indicates that the two corresponding cells are adjacent.
By searching the connectivity graph for a path from the cell containing the
start conﬁguration q s to the cell containing the goal conﬁguration q g , one
obtains (if it exists) a sequence of adjacent cells, called channel , from which
it is possible — in view of the above mentioned characteristics of cells — to
extract a path that joins q s to q g and is entirely contained in Cfree .
The general approach so far outlined generates diﬀerent motion planning
methods, essentially depending on the type of cells used for the decomposition.
In the following, two algorithms are described, respectively based on exact and
approximate cell decomposition of Cfree . As before, it is assumed that Cfree is
a limited polygonal subset of C = IR2 .
12.4.1 Exact Decomposition
When an exact decomposition is used, the free conﬁguration space is partitioned in a collection of cells whose union gives exactly Cfree . A typical choice
for cells are convex polygons. In fact, convexity guarantees that the line segments joining two conﬁgurations belonging to the same cell lies entirely in
the cell itself, and therefore in Cfree . Moreover, it is easy to travel safely between two adjacent cells by passing through the midpoint of the segment that
constitutes their common boundary. A simple way to decompose Cfree in a
collection of convex polygons is to use the sweep line algorithm, which turns
out to be useful in a number of computational geometry problems. In the
present setting, the application of this algorithm proceeds as follows.
Choose a line that is not parallel to any side of the boundary of Cfree , and
let it translate (‘sweep’) all over Cfree . Whenever the line passes through one
of the vertices of Cfree , consider the two segments (extensions) that originate
from the vertex, lie on the line and point in opposite directions, terminating
at the ﬁrst intersection with ∂Cfree . Every extension that lies in Cfree (except
for its endpoints) is part of the boundary of a cell; the rest of the boundary is
made of (part of) sides of ∂Cfree and possibly other extensions. This procedure
is illustrated in Fig. 12.7, where a vertical sweep line has been used; note that
some of the vertices of Cfree contribute to the decomposition with a single or no

12.4 Planning via Cell Decomposition

c3

c4

c5
c10

qs

c18
c11

c6

c7 c 8

c14
c15

c1
c3

c4

c2

c16

c18
c11

c9

c21
c22
c19

c12
c13
c14

c8
c6 c7
c1

c20
qg

c5
c10

c19

c12 c13 c17

c9
c2

537

c15

c16

c17 c20
c21 c22

Fig. 12.7. An example of trapezoidal decomposition via the sweep line algorithm
(above) and the associated connectivity graph (below ). Also shown is the solution
of a particular planning problem (cs = c3 and cg = c20 ), both as a channel in Cfree
and as a path on the graph

extension at all. The result is a special case of convex polygonal decomposition,
that is called trapezoidal because its cells are trapezoids — triangular cells, if
present, are regarded as degenerate trapezoids having one side of zero length.
After the decomposition of Cfree has been computed, it is possible to build
the associated connectivity graph C. This is the graph whose nodes are the
cells of the decomposition, while an arc exists between two nodes if the corresponding cells are adjacent, i.e., the intersection of their boundary is a line
segment of non-zero length; therefore, cells with side-vertex or vertex-vertex
contacts are not adjacent. At this point, it is necessary to identify the cells cs
and cg in the decomposition that respectively contain q s and q g , the start and
goal conﬁgurations for the considered planning problem. A graph search algorithm can then be used to ﬁnd a channel from cs to cg , i.e., a path on C that
joins the two corresponding nodes (see Fig. 12.7). From the channel, which is
a sequence of adjacent cells, one must extract a path in Cfree going from q s
to q g . Since the interior of the channel is contained in Cfree and the cells are
convex polygons, such extraction is straightforward. For example, one may
identify the midpoints of the segments that represent the common boundaries
between consecutive cells of the channel, and connect them through a broken
line starting in q s and ending in q g .

538

12 Motion Planning

Wrapping up, given the two conﬁgurations q s and q g , the motion planning
algorithm via exact cell decomposition is based on the following steps:
1.
2.
3.
4.

Compute a convex polygonal (e.g., trapezoidal) decomposition of Cfree .
Build the associated connectivity graph C.
Search C for a channel, i.e., a sequence of adjacent cells from cs to cg .
If a channel has been found, extract and provide as output a collision-free
path from q s to q g ; otherwise, report a failure.

As in motion planning via retraction, using a non-informed graph search
algorithm in Step 3 will result in a channel that is not optimal, in the sense
that all paths from q s to q g that can be extracted from the channel may be
longer than necessary. To compute eﬃcient paths, the use of A is advisable
as a search algorithm. To this end, one should build a modiﬁed connectivity
graph C  having as nodes q s , q g and all the midpoints of adjacency segments
between cells, and arcs joining nodes belonging to the same cell (note that
nodes on adjacency segments belong to two cells). Each arc is then labelled
with a cost equal to the distance between the nodes connected by the arc. If
the heuristic function is chosen as the distance between the current node and
q g , the use of A will produce the shortest path in C  , if a solution exists.
The motion planning method based on exact cell decomposition is complete. As for its time complexity, it depends mainly on the cell decomposition
and on the connectivity graph search. Using the sweep line algorithm, the
decomposition procedure (including the generation of the connectivity graph)
has complexity O(v log v), where v is the number of vertices of Cfree . Moreover, it can be shown that the connectivity graph C has O(v) arcs. Hence,
regardless of the adopted search strategy, the motion planning method based
on exact cell decomposition has complexity O(v log v).
Note the following facts:
• Any planner based on exact cell decomposition can be considered multiplequery. In fact, once computed, the connectivity graph associated with the
decomposition can be used to solve diﬀerent instances of the same motion
planning problem.
• The connectivity graph represents a network of channels, each implicitly
containing an inﬁnity of paths that traverse the channel and are topologically equivalent, i.e., diﬀer only for a continuous deformation. Therefore,
cell decomposition provides as output a structure that is more ﬂexible
than the roadmap used by retraction-based methods. This may be useful
to plan paths in the channel that are also admissible with respect to possible kinematic constraints, as well as to avoid unexpected obstacles during
the actual execution of the motion.
• The solution paths produced by the planning method based on exact cell
decomposition are broken lines. It is however possible to smooth the path
using curve ﬁtting techniques. In practice, one selects a suﬃcient number

12.4 Planning via Cell Decomposition

539

of intermediate points (via points) on the path, among which it is necessary to include q s and q g , and then interpolates them using functions
with an appropriate level of diﬀerentiability (e.g., polynomial functions of
suﬃciently high degree).
• The above method can be extended to the case in which Cfree is a limited
polyhedral subset of C = IR3 . In particular, the decomposition of Cfree can
obtained through the sweep plane algorithm, which produces polyhedral
cells. The common boundary between adjacent cells consists of trapezoid of
non-zero area. This boundary can be safely crossed, e.g., at the barycentre
of the trapezoid.
Finally, it should be mentioned that there exist in the literature methods
based on exact cell decomposition that can solve essentially any motion planning problem, regardless of the dimension of C and of the geometry of Cfree ,
which can also be non-polyhedral. However, the complexity of these planners
is prohibitive, being exponential in the dimension of C, and their importance
is therefore mainly theoretical.
12.4.2 Approximate Decomposition
In approximate decompositions of Cfree , disjoint cells of predeﬁned shape are
used; for example, when C = IR2 one may choose square or rectangular cells. In
general, the union of all cells will represent an approximation by defect of Cfree .
To achieve a reasonable trade-oﬀ between the accuracy of the approximation
and the eﬃciency of the decomposition procedure, a recursive algorithm is
typically used, which starts with a coarse grid whose resolution is locally
increased to adapt better to the geometry of Cfree . As in motion planning
methods based on exact decomposition, the connectivity graph associated
with the obtained approximate decomposition is searched for a channel, from
which a solution path can be extracted.
In the following, a motion planning method based on approximate decomposition is described for the case in which Cfree is a limited polygonal subset
of C = IR2 . Without loss of generality, it will be assumed that the ‘external’
boundary of Cfree is a square, and that square cells are therefore used. The
decomposition algorithm (Fig. 12.8) starts by dividing initially C into four
cells, that are classiﬁed according to the categories below:
• free cells, whose interior has no intersection with the C-obstacle region;
• occupied cells, entirely contained in the C-obstacle region;
• mixed cells, that are neither free nor occupied.
At this point, one builds the connectivity graph C associated with the
current level of decomposition: this is the graph having free and mixed cells
as nodes, and arcs that join nodes representing adjacent cells. Once the nodes
corresponding to the cells that contain q s and q g have been identiﬁed, C is
searched for a path between them, e.g., using the A algorithm. If such a path

540

12 Motion Planning

qs

qg

qs
qg

Fig. 12.8. An example of motion planning via approximate cell decomposition; left:
the assigned problem, right: the solution as a free channel (thick line)

does not exist, a failure is reported. If the path exists, it consists of a sequence
of cells that may be either all free (free channel ) or not (mixed channel ). In
the ﬁrst case, a solution to the motion planning problem has been found; in
particular, a conﬁguration space path can be easily extracted from the free
channel as in the method based on exact cell decomposition. Instead, if the
channel contains mixed cells, each of them is further divided into fourcells,
which are then classiﬁed as free, occupied or mixed. The algorithm proceeds
by iterating these steps, until a free channel going from q s to q g has been
found or a minimum admissible size has been reached for the cells. Figure 12.8
shows an example of application of this technique. Note that, at the resolution
level where the solution has been found, free and occupied cells represent an
approximation by defect of Cfree and CO, respectively. The missing part of C
is occupied by mixed cells (in gray).
At each iteration, the search algorithm on the connectivity graph can ﬁnd
a free channel going from q s to q g only if such a channel exists on the approximation of Cfree (i.e., on the free cells) at the current level of decomposition.
This motion planning method is therefore resolution complete, in the sense
that a suﬃcient reduction of the minimum admissible size for cells guarantees
that a solution is found whenever one exists.
A comparison between Figs. 12.7 and 12.8 clearly shows that, unlike what
happens in exact decomposition, the boundary of a cell in an approximate
decomposition does not correspond in general to a change in the spatial constraints imposed by obstacles. One of the consequences of this fact is that
the implementation of a motion planning method based on approximate decomposition is remarkably simpler, as it only requires a recursive division of
cells followed by a collision check between the cells and the C-obstacle region.
In particular, the ﬁrst can be realized using a data structure called quadtree.
This is a tree in which any internal node (i.e., a node that is not a leaf) has
exactly four child nodes. In the cell decomposition case, the tree is rooted at

12.5 Probabilistic Planning

541

C, i.e., the whole conﬁguration space. Lower level nodes represent cells that
are free, occupied or mixed; only the latter have children.
Another diﬀerence with respect to the method based on exact decomposition is that an approximate decomposition is intrinsically associated with
a particular instance of a planning problem, as the decomposition procedure
itself is guided by the search for a free channel between start and goal.
The planning method based on approximate decomposition is conceptually
applicable in conﬁguration spaces of arbitrary dimension. For example, in
IR3 it is possible to use an octree, a tree in which any internal node has
eight children. In IRn , the corresponding data structure is a 2n -tree. Since the
maximum number of leaves of a 2n -tree is 2np , where p is the depth (number
of levels) of the tree, the complexity of approximate cell decomposition —
and thus of the associated motion planning method — is exponential in the
dimension of C and in the maximal resolution of the decomposition. As a
consequence, this technique is eﬀective in practice only in conﬁguration spaces
of low dimension (typically, not larger than 4).

12.5 Probabilistic Planning
Probabilistic planners represent a class of methods of remarkable eﬃciency,
especially in problems involving high-dimensional conﬁguration spaces. They
belong to the general family of sampling-based methods, whose basic idea consists of determining a ﬁnite set of collision-free conﬁgurations that adequately
represent the connectivity of Cfree , and using these conﬁgurations to build a
roadmap that can be employed for solving motion planning problems. This
is realized by choosing at each iteration a sample conﬁguration and checking
if it entails a collision between the robot and the workspace obstacles. If the
answer is aﬃrmative, the sample is discarded. A conﬁguration that does not
cause a collision is instead added to the current roadmap and connected if
possible to other already stored conﬁgurations.
The above strategy is quite general and may lead to diﬀerent planning
methods depending on the speciﬁc design choices, and mainly on the criterion
for selecting the samples in C to be checked for collision. One may proceed
in a deterministic fashion, choosing the samples by means of a regular grid
that is applied to C. However, it is preferable to use a randomized approach,
in which the sample conﬁgurations are chosen according to some probability
distribution. In the following, two planners of this type are described.
12.5.1 PRM Method
The basic iteration of the PRM (Probabilistic Roadmap) method begins by
generating a random sample q rand of the conﬁguration space using a uniform
probability distribution in C. Then, q rand is tested for collision, by using kinematic and geometric relationships to compute the corresponding posture of

542

12 Motion Planning

qs
qg

Fig. 12.9. A PRM in a two-dimensional conﬁguration space (left) and its use for
solving a particular planning problem (right)

the robot and invoking an algorithm that can detect collisions (including contacts) between the latter and the obstacles. If q rand does not cause collisions,
it is added to the roadmap and connected (if possible) through free local paths
to suﬃciently ‘near’ conﬁgurations already in the roadmap. Usually, ‘nearness’ is deﬁned on the basis of the Euclidean distance in C, but it is possible
to use diﬀerent distance notions; for example, as mentioned in Sect. 12.2.1,
one may use a conﬁguration space distance notion induced by a distance in
the workspace. The generation of a free local path between q rand and a near
conﬁguration q near is delegated to a procedure known as local planner . A common choice is to throw a rectilinear path in C between q rand and q near and
test it for collision, for example by sampling the segment with suﬃcient resolution and checking the single samples for collision. If the local path causes
a collision, it is discarded and no direct connection between q rand and q near
appears in the roadmap.
The PRM incremental generation procedure stops when either a maximum
number of iterations has been reached, or the number of connected components in the roadmap becomes smaller than a given threshold. At this point,
one veriﬁes whether it is possible to solve the assigned motion planning problem by connecting q s and q g to the same connected component of the PRM
by free local paths. Figure 12.9 shows an example of PRM and the solution
to a particular problem. Note the presence of multiple connected components
of the PRM, one of which consists of a single conﬁguration.
If a solution cannot be found, the PRM can be improved by performing
more iterations of the basic procedure, or using special strategies aimed at
reducing the number of its connected components. For example, a possible
technique consists of trying to connect conﬁgurations that are close but belong
to diﬀerent components via more general (e.g., not rectilinear) local paths.
The main advantage of the PRM method is its remarkable speed in ﬁnding
a solution to motion planning problems, provided that the roadmap has been
suﬃciently developed. In this respect, it should be noted that new instances
of the same problem induce a potential enhancement of the PRM, which
improves with usage both in terms of connectivity and of time eﬃciency. The

12.5 Probabilistic Planning

543

PRM method is therefore intrinsically multiple-query. In high-dimensional
conﬁguration spaces, the time needed by this method to compute a solution
can be several orders of magnitude smaller than with the previously presented
techniques. Another aspect of the method that is worth mentioning is the
simplicity of implementation. In particular, note that the generation of Cobstacles is completely eliminated.
The downside of the PRM method is that it is only probabilistically complete, i.e., the probability of ﬁnding a solution to the planning problem when
one exists tends to 1 as the execution time tends to inﬁnity. This means that, if
no solution exists, the algorithm will run indeﬁnitely. In practice, a maximum
number of iterations is enforced so as to guarantee its termination.
A situation that is critical for the PRM method is the presence of narrow
passages in Cfree , such as the one shown in the upper-right quadrant of the
scene in Fig. 12.9. In fact, using a uniform distribution for generating q rand , the
probability of placing a sample in a certain region of Cfree is proportional to its
volume. As a consequence, depending on its size, it may be very unlikely that a
path crossing a narrow passage appears in the PRM within a reasonable time.
To alleviate this problem, the method can be modiﬁed by using non-uniform
probability distributions. For example, there exist strategies for generating
q rand that are biased towards those regions of C that contain fewer samples,
and therefore are more likely to contain close obstacles and the associated
narrow passages.
12.5.2 Bidirectional RRT Method
Single-query probabilistic methods are aimed at quickly solving a particular
instance of a motion planning problem. Unlike multiple-query planners such
as PRM, these techniques do not rely on the generation of a roadmap that
represents exhaustively the connectivity of the free conﬁguration space; in
fact, they tend to explore only a subset of Cfree that is relevant for solving the
problem at hand. This results in a further reduction of the time needed to
compute a solution.
An example of single-query probabilistic planner is the bidirectional RRT
method, which makes use of a data structure called RRT (Rapidly-exploring
Random Tree). The incremental expansion of an RRT, denoted by T in the
following, relies on a simple randomized procedure to be repeated at each
iteration (see Fig. 12.10). The ﬁrst step is the generation of a random conﬁguration q rand according to a uniform probability distribution in C (as in the
PRM method). Then, the conﬁguration q near in T that is closer to q rand is
found, and a new candidate conﬁguration q new is produced on the segment
joining q near to q rand at a predeﬁned distance δ from q near . A collision check
is then run to verify that both q new and the segment going from q near to q new
belong to Cfree . If this is the case, T is expanded by incorporating q new and
the segment joining it to q near . Note that q rand is not added to the tree, so

544

12 Motion Planning

qs

qs
Ts

qnear
&
q new

qrand

Tg

qg

qg
q new

Fig. 12.10. The bidirectional RRT method in a two-dimensional conﬁguration space
left: the randomized mechanism for expanding a tree, right: the extension procedure
for connecting the two trees

that it is not necessary to check whether it belongs to Cfree ; its only function
is to indicate a direction of expansion for T .
It is worth pointing out that the RRT expansion procedure, although quite
simple, results in a very eﬃcient ‘exploration’ of C. In fact, it may be shown
that the procedure for generating new candidate conﬁguration is intrinsically
biased towards those regions of Cfree that have not been visited yet. Moreover,
the probability that a generic conﬁguration of Cfree is added to the RRT tends
to 1 as the execution time tends to inﬁnity, provided that the conﬁguration
lies in the same connected component of Cfree where the RRT is rooted.
To speed up the search for a free path going from q s to q g , the bidirectional RRT method uses two trees Ts and Tg , respectively rooted at q s and
q g . At each iteration, both trees are expanded with the previously described
randomized mechanism. After a certain number of expansion steps, the algorithm enters a phase where it tries to connect the two trees by extending each
one of them towards the other. This is realized by generating a q new as an expansion of Ts , and trying to connect Tg to q new . To this end, one may modify
the above expansion procedure. In particular, once q new has been generated
from Ts , it acts as a q rand for Tg : one ﬁnds the closest conﬁguration q near in
Tg , and moves from q near trying to actually reach q rand = q new , hence with a
variable stepsize as opposed to a constant δ.
If the segment joining q near to q new is collision-free, the extension is complete and the two trees have been connected; otherwise the free portion of the
segment is extracted and added to Tg together with its endpoint. At this point,
Tg and Ts exchange their roles and the connection attempt is repeated. If this
is not successful within a certain number of iterations, one may conclude that
the two trees are still far apart and resume the expansion phase.
Like the PRM method, bidirectional RRT is probabilistically complete. A
number of variations can be made on the basic scheme. For example, rather
than using a constant δ for generating q new , one may deﬁne the stepsize as a
function of the available free space, possibly going as far as q rand (as in the
extension procedure). This greedy version of the method can be much more

12.5 Probabilistic Planning

545

eﬃcient if C contains extensive free regions. Moreover, RRT-based methods
can be adapted to robots that do not satisfy the free-ﬂying assumption, such
as robots that are subject to nonholonomic constraints.
Extension to nonholonomic robots
Consider now the motion planning problem for a nonholonomic mobile robot
whose kinematic model is expressed as (11.10). As seen in the previous chapter, admissible paths in conﬁguration space must satisfy constraint (11.40).
For example, in the case of a robot with unicycle kinematics, rectilinear paths
in conﬁguration space — such as those used in the RRT expansion to move
from q near to q new — are not admissible in general.
A simple yet general approach to the design of nonholonomic motion planning methods is to use motion primitives, i.e., a ﬁnite set of admissible local
paths in conﬁguration space, each produced by a speciﬁc choice of the velocity
inputs in the kinematic model. Admissible paths are generated as a concatenation of motion primitives. In the case of a unicycle robot, for example, the
following set of velocity inputs
v = v̄

ω = {−ω̄, 0, ω̄}

t ∈ [0, Δ]

(12.9)

results in three6 admissible local paths: the ﬁrst and the third are respectively
a left turn and a right turn along arcs of circle, while the second is a rectilinear
path (Fig. 12.11, left).
The expansion of an RRT for a nonholonomic mobile robot equipped with
a set of motion primitives is quite similar to the previously described procedure. The diﬀerence is that, once identiﬁed the conﬁguration q near on T that
is closest to q rand , the new conﬁguration q new is generated by applying the
motion primitives starting from q near and choosing one of the produced conﬁgurations, either randomly or as the closest to q rand . Clearly, q new and the
admissible local path joining q near to q new are subject to a collision test. Figure 12.11, right, shows an example of RRT — more precisely, its projection on
the workspace — for a unicycle equipped with the motion primitives (12.9).
Under suitable assumptions, it is possible to show that if the goal conﬁguration q g can be reached from the start conﬁguration q s through a collisionfree concatenation of motion primitives,7 the probability that q g is added to
6

7

Note that these particular motion primitives include neither backward motion
nor rotation on the spot. A unicycle with constant positive driving velocity v and
bounded steering velocity is called Dubins car in the literature.
This hypothesis, obviously necessary, implies that the choice of motion primitives
must be suﬃciently ‘rich’ to guarantee that the set of conﬁguration reachable
through concatenation is ‘dense’ enough with respect to Cfree . For example, the
unicycle with the motion primitives (12.9) with a variable time interval Δ can
reach any conﬁguration in Cfree , although not necessarily with a path that is
entirely contained in Cfree . This property is instead guaranteed if the Reeds–Shepp
curves given by (11.56) are used as motion primitives.

546

12 Motion Planning

Fig. 12.11. RRT-based motion planning for a unicycle; left: a set of motion primitives, right: an example of RRT

the tree T tends to 1 as the execution time tends to inﬁnity. To increase the
eﬃciency of the search, also in this case it is possible to devise a bidirectional
version of the method.

12.6 Planning via Artiﬁcial Potentials
All the methods so far presented are suitable for oﬀ-line motion planning,
because they require a priori knowledge of the geometry and the pose of the
obstacles in the robot workspace. This assumption is reasonable in many cases,
e.g., when an industrial manipulator is moving in a robotized cell. However, in
service robotics applications the robot must be able to plan its motion on-line,
i.e., using partial information on the workspace gathered during the motion
on the basis of sensor measurements.
An eﬀective paradigm for on-line planning relies on the use of artiﬁcial
potential ﬁelds. Essentially, the point that represents the robot in conﬁguration space moves under the inﬂuence of a potential ﬁeld U obtained as the
superposition of an attractive potential to the goal and a repulsive potential
from the C-obstacle region. Planning takes place in an incremental fashion:
at each robot conﬁguration q, the artiﬁcial force generated by the potential
is deﬁned as the negative gradient −∇U (q) of the potential, which indicates
the most promising direction of local motion.
12.6.1 Attractive Potential
The attractive potential is designed so as to guide the robot to the goal conﬁguration q g . To this end, one may use a paraboloid with vertex in q g :
Ua1 (q) =

1
1
ka eT (q)e(q) = ka e(q)2 ,
2
2

(12.10)

12.6 Planning via Artiﬁcial Potentials

547

Fig. 12.12. The shape of the paraboloidic attractive potential Ua1 (left) and of the
conical attractive potential Ua2 (right) in the case C = IR2 , for ka = 1

where ka > 0 and e = q g − q is the ‘error’ vector with respect to the goal
conﬁguration q g . This function is always positive and has a global minimum
in q g , where it is zero. The resulting attractive force is deﬁned as
f a1 (q) = −∇Ua1 (q) = ka e(q).

(12.11)

Hence, f a1 converges linearly to zero when the robot conﬁguration q tends to
the goal conﬁguration q g .
Alternatively, it is possible to deﬁne a conical attractive potential as
Ua2 (q) = ka e(q).

(12.12)

Also Ua2 is always positive, and zero in q g . The corresponding attractive force
is
e(q)
,
(12.13)
f a2 (q) = −∇Ua2 (q) = ka
e(q)
that is constant in modulus. This represents an advantage with respect to the
force f a1 generated by the paraboloidic attractive potential, which tends to
grow indeﬁnitely as the error vector increases in norm. On the other hand,
f a2 is indeﬁnite in q g . Figure 12.12 shows the shape of Ua1 and Ua2 in the
case C = IR2 , with ka = 1.
A choice that combines the advantages of the above two potentials is to
deﬁne the attractive potential as a conical surface away from the goal and
as a paraboloid in the vicinity of q g . In particular, by placing the transition
between the two potentials where e(q) = 1 (i.e., on the surface of the sphere
of unit radius centred in q g ) one obtains an attractive force that is continuous
for any q (see also Problem 12.9).
12.6.2 Repulsive Potential
The repulsive potential Ur is added to the attractive potential Ua to prevent
the robot from colliding with obstacles as it moves under the inﬂuence of the

548

12 Motion Planning

Fig. 12.13. The equipotential contours of the repulsive potential Ur in the range
of inﬂuence of a polygonal C-obstacle in C = IR2 , for kr = 1 and γ = 2

attractive force f a . In particular, the idea is to build a barrier potential in
the vicinity of the C-obstacle region, so as to repel the point that represents
the robot in C.
In the following, it will be assumed that the C-obstacle region has been
partitioned in convex components COi , i = 1, . . . , p. These components may
coincide with the C-obstacles themselves; this happens, for example, when the
robot B is a convex polygon (polyhedron) translating with a ﬁxed orientation
in IR2 (IR3 ) among convex polygonal (polyhedral) obstacles (see Sect. 12.2.2).
In the presence of non-convex C-obstacles, however, it is necessary to perform the decomposition in convex components before building the repulsive
potential.
For each convex component COi , deﬁne an associated repulsive potential
as
⎧
γ

1
1
⎪
⎨ kr,i
−
if ηi (q) ≤ η0,i
γ
ηi (q) η0,i
(12.14)
Ur,i (q) =
⎪
⎩
0
if ηi (q) > η0,i ,
where kr,i > 0, ηi (q) = min q  ∈COi q − q   is the distance of q from COi , η0,i
is the range of inﬂuence of COi and γ = 2, 3, . . .. The potential Ur,i is zero
outside and positive inside the range of inﬂuence η0,i and tends to inﬁnity as
the boundary of COi is approached, more abruptly as γ is increased (a typical
choice is γ = 2).
When C = IR2 and the convex component COi is polygonal, an equipotential contour of Ur,i (i.e., the locus of conﬁgurations q such that Ur,i has
a certain constant value) consists of rectilinear tracts that are parallel to the
sides of the polygon, connected by arcs of circle in correspondence of the vertices, as shown in Fig. 12.13. Note how the contours get closer to each other
in the proximity of the C-obstacle boundary, due to the hyperboloidic proﬁle

12.6 Planning via Artiﬁcial Potentials

549

of the potential. When C = IR3 and the convex component COi is polyhedral,
the equipotential surfaces of Ur,i are copies of the faces of COi , connected by
patches of cylindrical surfaces in correspondence of the edges and spherical
surfaces in correspondence of the vertices of COi .
The repulsive force resulting from Ur,i is
⎧
γ−1

⎪
kr,i
1
1
⎪
⎪
−
⎪
⎨ ηi2 (q) ηi (q) η0,i ∇ηi (q) if ηi (q) ≤ η0,i
f r,i (q) = −∇Ur,i (q) =
(12.15)
⎪
⎪
⎪
⎪
⎩
0
if ηi (q) > η0,i .
Denote by q m the conﬁguration of COi that is closer to q (q m is uniquely
determined in view of the convexity of COi ). The gradient vector ∇ηi (q),
which is orthogonal to the equipotential contour (or surface) passing through
q, is directed as the half-line originating from q m and passing through q. If
the boundary of COi is piecewise diﬀerentiable, function ηi is diﬀerentiable
everywhere in Cfree and f r,i is continuous in the same space.8
The aggregate repulsive potential is obtained by adding up the individual
potentials associated with the convex components of CO:
Ur (q) =

p


Ur,i (q).

(12.16)

i=1

If ηi (q g ) ≥ η0,i for i = 1, . . . , p (i.e., if the goal is placed outside the range of
inﬂuence of each obstacle component COi ), the value of the aggregate repulsive
ﬁeld Ur is zero in q g . In the following, it will be assumed that this is the case.
12.6.3 Total Potential
The total potential Ut is obtained by superposition of the attractive and the
aggregate repulsive potentials:
Ut (q) = Ua (q) + Ur (q).

(12.17)

This results in the force ﬁeld
f t (q) = −∇Ut (q) = f a (q) +

p


f r,i (q).

(12.18)

i=1
8

Note the relevance in this sense of the assumption that the component COi is
convex. If it were otherwise, there would exist conﬁgurations in Cfree for which q m
would not be uniquely deﬁned. In these conﬁgurations, belonging by deﬁnition to
the generalized Voronoi diagram V(Cfree ), function ηi would not be diﬀerentiable,
resulting in a discontinuous repulsive force. This might induce undesired eﬀects
on the planned path (for example, oscillations).

550

12 Motion Planning

Fig. 12.14. The total potential in C = IR2 obtained by superposition of a hyperboloidic attractive potential and a repulsive potential for a rectangular C-obstacle:
left: the equipotential contours, right: the resulting force ﬁeld

Ut clearly has a global minimum in q g , but there may also exist some
local minima where the force ﬁeld is zero. Considering for simplicity the case
C = IR2 , this happens in the ‘shadow zone’ of a C-obstacle when the repulsive
potential Ur,i has equipotential contours with lower curvature (e.g., segments)
than the attractive potential in the same area. See for example Fig. 12.14,
where a local minimum is clearly present ‘below’ the C-obstacle. A remarkable
exception is the case (sphere world ) in which all the convex components COi of
the C-obstacle region are spheres. In this situation, the total potential exhibits
isolated saddle points (where the force ﬁeld is still zero) but no local minima.
12.6.4 Planning Techniques
There are three diﬀerent approaches for planning collision-free motions on the
basis of a total artiﬁcial potential Ut and the associated force ﬁeld f t = −∇Ut .
They are brieﬂy discussed below:
1. The ﬁrst possibility is to let
τ = f t (q),

(12.19)

hence considering f t (q) as a vector of generalized forces that induce a
motion of the robot in accordance with its dynamic model.
2. The second method regards the robot as a unit point mass moving under
the inﬂuence of f t (q), as in
q̈ = f t (q).

(12.20)

3. The third possibility is to interpret the force ﬁeld f t (q) as a desired velocity
for the robot, by letting
q̇ = f t (q).
(12.21)

12.6 Planning via Artiﬁcial Potentials

551

In principle, one could use these three approaches for on-line as well as
oﬀ-line motion planning. In the ﬁrst case, (12.19) directly represents control inputs for the robot, whereas the implementation of (12.20) requires the
solution of the inverse dynamics problem, i.e., the substitution of q̈ in the
robot dynamic model to compute the generalized forces τ that realize such
accelerations. Equation (12.21) can instead be used on-line in a kinematic
control scheme, in particular to provide the reference inputs for the low-level
controllers that are in charge of reproducing such generalized velocities as accurately as possible. In any case, the artiﬁcial force ﬁeld f t represents, either
directly or indirectly, a true feedback control that guides the robot towards
the goal, while trying at the same time to avoid collisions with the workspace
obstacles that have been detected by the sensory system. To emphasize this
aspect, on-line motion generation based on artiﬁcial potentials is also referred
to as reactive planning.
In oﬀ-line motion planning, conﬁguration space paths are generated by
simulation, i.e., integrating numerically the robot dynamic model if (12.19) is
used, or directly by the diﬀerential equations (12.20) and (12.21).
In general, the use of (12.19) generates smoother paths, because with
this scheme the reactions to the presence of obstacles are naturally ‘ﬁltered’ through the robot dynamics. On the other hand, the strategy represented by (12.21) is faster in executing the motion corrections suggested by
the force ﬁeld f t , and may thus be considered safer. The characteristics of
scheme (12.20) are clearly intermediate between the other two. Another aspect to be considered is that using (12.21) guarantees (in the absence of local
minima) the asymptotic stability of q g (i.e., the robot reaches the goal with
zero velocity), whereas this is not true for the other two motion generation
strategies. To achieve asymptotic stability with (12.19) and (12.20), a damping
term proportional to the robot velocity q̇ must be added to f t .
In view of the above discussion, it is not surprising that the most common
choice is the simple numerical integration of (12.21) via the Euler method:
q k+1 = q k + T f t (q k ),

(12.22)

where q k and q k+1 represent respectively the current and the next robot conﬁguration, and T is the integration step. To improve the quality of the generated path, it is also possible to use a variable T , smaller when the modulus
of the force ﬁeld f t is larger (in the vicinity of obstacles) or smaller (close to
the destination q g ). Recalling that f t (q) = −∇Ut (q), Eq. (12.22) may be easily interpreted as a numerical implementation of the gradient method for the
minimization of Ut (q), often referred to as the algorithm of steepest descent.
12.6.5 The Local Minima Problem
Whatever technique is used to plan motions on the basis of artiﬁcial potentials,
local minima of Ut — where the total force ﬁeld f t is zero — represent a

552

12 Motion Planning

problem. For example, if (12.22) is used and the generated path enters the
basin of attraction of a local minimum, the planning process is bound to stop
there, without reaching the goal conﬁguration.9 Actually, the same problem
may occur also when (12.19) or (12.20) are used if the basin of attraction of the
local minimum is suﬃciently large. On the other hand, as noticed previously,
the total potential Ut obtained by the superposition of an attractive and a
repulsive potential invariably exhibits local minima, except for very particular
cases. This means that motion planning methods based on artiﬁcial potentials
are not complete in general, because it may happen that the goal conﬁguration
q g is not reached even though a solution exists.
Best-ﬁrst algorithm
A simple workaround for the local minima problem is to use a best-ﬁrst algorithm, which is formulated under the assumption that the free conﬁguration
space Cfree has been discretized using a regular grid. In general, the discretization procedure results in the loss of some boundary regions of Cfree , which are
not represented as free in the gridmap. Each free cell of the grid is assigned the
value of the total potential Ut computed at the centroid of the cell. Planning
proceeds by building a tree T rooted at the start conﬁguration q s . At each
iteration, the leaf with the minimum value of Ut is selected and its adjacent10
cells are examined. Those that are not already in T are added as children of
the considered leaf. Planning is successful when the cell containing the goal is
reached (in this case, the solution path is built by tracing back the arcs of the
tree from q g to q s ), whereas failure is reported when all the cells accessible
from q s have been explored without reaching q g .
The above best-ﬁrst algorithm evolves as a grid-discretized version of the
algorithm of steepest descent (12.22), until a cell is reached which represents
a minimum for the associated total potential. If it is a local minimum, the
algorithm visits (‘ﬁlls’) its entire basin of attraction and ﬁnally leaves it, reaching a point from which planning can continue. The resulting motion planning
method is resolution complete, because a solution is found only if one exists
on the gridmap that represents Cfree by defect. In general, increasing the grid
resolution may be necessary to recover the possibility of determining a solution. In any case, the time complexity of the basin ﬁlling procedure, which is
exponential in the dimension of C because such is the number of adjacent cells
to a given one, makes the best-ﬁrst algorithm applicable only in conﬁguration
spaces of low dimension (typically, not larger than 3).
9
10

The probability of reaching a saddle point instead is extremely low; besides, any
perturbation would allow the planner to exit such a point.
Various deﬁnitions of adjacency may be adopted. For example, in IR2 one may
use 1-adjacency or 2-adjacency. In the ﬁrst case, each cell c has four adjacent
cells, while in the second they are eight. The resulting paths will obviously reﬂect
this fact.

12.6 Planning via Artiﬁcial Potentials

553

A more eﬀective approach is to include in the best-ﬁrst method a randomized mechanism for evading local minima. In practice, one implements a
version of the best-ﬁrst algorithm in which the number of iterations aimed at
ﬁlling the basin of attraction of local minima is bounded. When the bound
is reached, a sequence of random steps (random walk ) is taken. This usually
allows the planner to evade the basin in a much shorter time than would
be needed by the complete ﬁlling procedure. As the probability of evading
the basin of attraction of a local minimum approaches 1 when the number
of random steps tends to inﬁnity, this randomized best-ﬁrst method is probabilistically complete (in addition to being resolution complete).
Navigation functions
Although the best-ﬁrst algorithm (in its basic or randomized version) represents a solution to the problem of local minima, it may generate paths that
are very ineﬃcient. In fact, with this strategy the robot will still enter (and
then evade) the basin of attraction of any local minimum located on its way
to goal. A more radical approach is based on the use of navigation functions,
i.e., artiﬁcial potentials that have no local minima. As already mentioned, if
the C-obstacles are spheres this property already holds for the potential Ut
deﬁned by superposition of an attractive and a repulsive ﬁeld. A ﬁrst possibility would then be to approximate by excess all C-obstacles with spheres,
and use the total potential Ut . Clearly, such an approximation may severely
reduce the free conﬁguration space Cfree , and even destroy its connectedness;
for example, imagine what would happen in the case depicted in Fig. 12.4.
In principle, a mathematically elegant way to deﬁne a navigation function
consists of building ﬁrst a diﬀerentiable homeomorphism (a diﬀeomorphism)
that maps the C-obstacle region to a collection of spheres, then generating a
classical total potential in the transformed space, and ﬁnally mapping it back
to the original conﬁguration space so as to obtain a potential free of local
minima. If the C-obstacles are star-shaped ,11 such a diﬀeomorphism actually
exists, and the procedure outlined above provides in fact a navigation function.
Another approach is to build the potential using harmonic functions, that are
the solutions of a particular diﬀerential equation that describes the physical
process of heat transmission or ﬂuid dynamics.
Generating a navigation function is, however, computationally cumbersome, and the associated planning methods are thus mainly of theoretical interest. A notable exception, at least in low-dimensional conﬁguration spaces,
is the numerical navigation function. This is a potential built on a gridmap
representation of Cfree by assigning value 0 to the cell containing q g , value 1
to its adjacent cells, value 2 to the unvisited cells among those adjacent to
11

A subset S ⊂ IRn is said to be star-shaped if it is homeomorphic to the closed
unit sphere in IRn and has a point p (centre) such that any other point in S may
be joined to p by a segment that is entirely contained in S.

554

12 Motion Planning
2

1

2

1

0

1

2

1

2

3

3

4

3

4

3

5

6

7

8

9

19

6

7

8

9

10

18

7

8

10 11

17

4

5

6

7

12

16

5

6

7

12 13

15

8

5

6

7

6

7

8

9

6

7

8

7

8

9

10 11 12 13 14 15

10 11 12 13 14

Fig. 12.15. An example of numerical navigation function in a simple twodimensional gridmap using 1-adjacency. Cells in gray denote a particular solution
path obtained by following from the start (cell 12) the steepest descent of the potential on the gridmap

cells with potential 1, and so on. To understand better the procedure, one
may visualize a wavefront that originates at q g and expands according to the
adopted adjacency deﬁnition (wavefront expansion algorithm). It is easy to
realize that the obtained potential is free of local minima, and therefore its
use in conjunction with the algorithm of steepest descent provides a motion
planning method that is complete in resolution (see Fig. 12.15).
Finally, it should be mentioned that the use of navigation functions is
limited to oﬀ-line motion planning, because their construction — be they
continuous or discrete in nature — requires the a priori knowledge of the geometry and pose of the workspace obstacles. As for on-line motion planning,
in which the obstacles are gradually reconstructed via sensor measurements
as the robot moves, the incremental construction of a total potential by superposition of attractive and repulsive ﬁelds represents a simple, often eﬀective
method to generate collision-free motions, even though completeness cannot
be claimed. In any case, motion planning based on artiﬁcial potentials belong to the single-query category, because the attractive component of the
potential depends on the goal conﬁguration q g .

12.7 The Robot Manipulator Case
Generating collision-free movements for robot manipulators is a particularly
important category of motion planning problems. In general, the computational complexity associated with this problem is substantial, due to the high
dimension of the conﬁguration space (typically n ≥ 4) and to the presence of
rotational DOFs (revolute joints).
It is sometimes possible to reduce the dimension of the conﬁguration space
C approximating by excess the size of the robot. For example, in a six-DOF
anthropomorphic manipulator one can replace the last three links of the kinematic chain (the spherical wrist) and the end-eﬀector with the volume they

12.7 The Robot Manipulator Case

555

‘sweep’ when the corresponding joints move across their whole available range.
The dimension of the conﬁguration space becomes three, as planning concerns
only the base, shoulder and elbow joints, while the wrist can move arbitrarily.
Clearly, this approximation is conservative, and hence acceptable only if the
aforementioned volume is small with respect to the workspace of the manipulator.
In the presence of rotational DOFs, the other complication is the shape
of the C-obstacles, which is complex even for simple workspace obstacles due
to the strong nonlinearity introduced by the manipulator inverse kinematics
(recall Fig. 12.4). Apart from the intrinsic diﬃculty of computing C-obstacles,
their non-polyhedral shape does not allow the application of the planning
methods presented in Sects. 12.3 and 12.4.
The most convenient choice for oﬀ-line planning is represented by probabilistic methods, which exhibit the best performance in high-dimensional
conﬁguration spaces. Moreover, as the computation of the C-obstacles is not
required, these planners are not aﬀected by their shape. However, it should be
noted that collision checking — which is an essential tool for probabilistic motion planning — becomes more onerous as the number of DOFs is increased.
For on-line planning, the best results are obtained by a suitable adaptation of the method based on artiﬁcial potentials. In particular, to avoid the
computation of C-obstacles and at the same time plan in a space of reduced
dimension, the potential is directly built in the workspace W = IRN rather
than in the conﬁguration space C, and acts on a set of control points located
on the manipulator. Among these is included a point that represents the endeﬀector (to which is assigned the goal of the motion planning problem) and at
least one point (possibly variable in time) for each body of the linkage. While
the attractive potential only inﬂuences the end-eﬀector representative point,
the repulsive potential acts on all control points. As a consequence, the artiﬁcial potentials used in this scheme are actually two: an attractive-repulsive
ﬁeld for the end-eﬀector, and a repulsive ﬁeld for the other control points
distributed on the manipulator links.
As before, diﬀerent approaches may be used to convert the force ﬁelds
generated by the artiﬁcial potentials to commands for the manipulator. Denote by pi (q), i = 1, . . . , P , the coordinates in W of the P control points
in correspondence of the conﬁguration q of the manipulator. In particular,
p1 , . . . , pP −1 are the control points located on the manipulator links, subject only to the repulsive potential Ur , while pP is the control point for the
end-eﬀector, which is subject to the total potential Ut = Ua + Ur .
A ﬁrst possibility is to impose to the robot joints the generalized forces
which would result from the combined action of the various force ﬁelds acting
on the control points in the workspace, according to
τ =−

P
−1

i=1

J Ti (q)∇Ur (pi ) − J TP (q)∇Ut (pP ),

(12.23)

556

12 Motion Planning
S
S

G

G

Fig. 12.16. Examples of motion planning via artiﬁcial potentials acting on control points for a planar 3R manipulator; left: planning is successful and leads to a
collision-free motion between the start S and the goal G, right: a failure is reported
because the manipulator is stuck at a force equilibrium

where J i (q), i = 1, . . . , P , denotes the Jacobian of the direct kinematics
function associated with the control point pi (q).
Alternatively, a purely kinematic planning scheme is obtained by letting
q̇ = −

P
−1


J Ti (q)∇Ur (pi ) − J TP (q)∇Ut (pP )

(12.24)

i=1

and feeding these joint velocities to the low-level control loops as reference
signals. Note that Eq. (12.24) represents a gradient-based minimization step
in the conﬁguration space C of a combined potential deﬁned in the workspace
W. In fact, a potential function acting on a control point in the workspace may
be seen as a composite function of q through the associated direct kinematics
relationship, and the Jacobian transpose J Ti (q), i = 1, . . . , P , maps a gradient
in W to a gradient in C. In formulae:

T 
T
∂U (pi (q))
∂U (pi ) ∂pi
∇q U (pi ) =
=
= J Ti (q)∇U (pi ),
∂q
∂pi ∂q
for i = 1, . . . , P .
The above two schemes can be considered respectively the transposition
of (12.19) and (12.21), and therefore they inherit the same characteristics.
In particular, when (12.23) is used the motion corrections prescribed by the
force ﬁelds are ﬁltered through the dynamic model of the manipulator, and
smoother movements can be expected. The kinematic scheme (12.24) instead
is faster in realizing such corrections.
Finally, it should be mentioned that the use of artiﬁcial potentials that are
deﬁned in the workspace may aggravate the local minima problem. In fact,
the various forces (either purely attractive or repulsive-attractive) acting on
the control points may neutralize each other at the joint level, blocking the

Problems

557

manipulator in a conﬁguration (force equilibrium) where no control point is at
a local minimum of the associated potential (see Fig. 12.16). As consequence,
it is always advisable to use workspace potential ﬁelds in conjunction with a
randomized best-ﬁrst algorithm.

Bibliography
In the last three decades, the literature on motion planning has grown considerably, and this area may now be considered as a scientiﬁc discipline in
itself. In the following, only a short list is given of the seminal works for the
material presented in this chapter.
The systematic use of the concept of conﬁguration space for motion planning was proposed in [138]. The method based on the retraction of the free
conﬁguration space on the generalized Voronoi diagram was originally introduced for a robot of circular shape in [170]. The technique based on trapezoidal
cell decomposition is described in [122], while [197] and [33] are among the general planning methods via decomposition mentioned at the end of Sect. 12.4.1.
The approach based on approximate cell decomposition was proposed in [138].
The PRM method for probabilistic planning was introduced in [107], while
the RRT method with its variants is described in [125].
The use of artiﬁcial potentials for on-line motion planning was pioneered
in [113]. The concept of navigation function was introduced in [185], while
its numerical version on a gridmap was described in [17], together with the
best-ﬁrst algorithm, also in its randomized version.
For other aspects of the motion planning problem that are merely hinted
at (nonholonomic motion planning) or simply ignored (managing uncertainty,
mobile obstacles) in this chapter, the reader can consult many excellent books,
going from the classical treatment in [122], through [123], to the most recent
texts [45, 145, 124].

Problems
12.1. Describe the nature (including the dimension) of the conﬁguration space
for a mobile manipulator consisting of a unicycle-like vehicle carrying a sixDOF anthropomorphic arm, providing a choice of generalized coordinates for
the system.
12.2. With reference to a 2R manipulator, modify the deﬁnition (12.2) of
conﬁguration space distance so as to take into account the fact that the manipulator posture does not change if the joint variables q1 and q2 are increased
(or decreased) by a multiple of 2π.
12.3. Consider a polygonal robot translating at a ﬁxed orientation in IR2
among polygonal obstacles. Build an example showing that the same Cobstacle region may correspond to robot and obstacles of diﬀerent shapes.

558

12 Motion Planning

12.4. With reference to the second workspace shown in Fig. 12.4, give the
numerical value of three conﬁgurations of the manipulator that lie in the three
connected components of Cfree . Moreover, sketch the manipulator posture for
each of these conﬁgurations.
12.5. Discuss the basic steps of an algorithm for computing the generalized
Voronoi diagram of a limited polygonal subset of IR2 . [Hint: a simple algorithm
is obtained by considering all the possible side-side, side-vertex and vertexvertex pairs, and generating the elementary arcs of the diagram on the basis
of the intersections of the associated equidistance contours.]
12.6. For the motion planning method via exact cell decomposition, give an
example in which the path obtained as a broken line joining the midpoints
of the common boundary of the channel cells goes through a vertex of the
C-obstacle region, and propose an alternative procedure for extracting a free
path from the channel. [Hint: build a situation in which the channel from cs
to cg contains a cell for which the entrance and the exit boundary lie on the
same side.]
12.7. Implement in a computer program the PRM method for a 2R planar
robot moving among circular workspace obstacles. The program receives as
input a geometrical description of the obstacles (centre and radius of each
obstacle) as well as the start and goal conﬁgurations, and terminates after
a maximum number of iterations. If a solution path has been found, it is
given as output together with the associated PRM; otherwise, a failure is
reported. Discuss the performance of the method with respect to the choice
of conﬁguration space distance (for example, (12.1) or (12.2)).
12.8. Implement in a computer program the RRT method for a circularshaped unicycle moving among square obstacles, with the motion primitives
deﬁned as in (12.9). The program receives as input the start and goal conﬁgurations, in addition to a geometrical description of the obstacles (centre and
side of each obstacle), and terminates after a maximum number of iterations.
If a solution path has been found, it is given as output together with the
associated RRT; otherwise, a failure is reported. Build a situation in which
the method cannot ﬁnd a solution because of the limitations inherent to the
chosen motion primitives.
12.9. For the case C = IR2 , build a continuously diﬀerentiable attractive potential having a paraboloidic proﬁle inside the circle of radius ρ and a conical
proﬁle outside. [Hint: modify the expression (12.12) of the conical potential
using a diﬀerent constant kb in place of ka , which already characterizes the
paraboloidic potential.]
12.10. For the case C = IR2 , prove that the total potential Ut may exhibit
a local minimum in areas where the equipotential contours of the repulsive
potential Ur have lower curvature than those of the attractive potential Ua .
[Hint: consider a polygonal C-obstacle and use a geometric construction.]

Problems

559

12.11. Consider a point robot moving in a planar workspace containing three
circular obstacles, respectively of radius 1, 2 and 3 and centre in (2, 1), (−1, 3)
and (1, −2). The goal is the origin of the workspace reference frame. Build the
total potential Ut resulting from the superposition of the attractive potential
Ua to the goal and the repulsive potential Ur from the obstacles, and derive
the corresponding artiﬁcial force ﬁeld. Moreover, compute the coordinates of
the saddle points of Ut .
12.12. Discuss the main issues arising from the application of the artiﬁcial
potential technique for planning on-line the motion of an omnidirectional circular robot. Assume that the robot is equipped with a rotating laser range
ﬁnder placed at its centre that measures the distance between the sensor and
the closest obstacles along each direction. If the distance is larger than the
maximum measurable range R, the sensor returns R as a reading. Sketch a
possible extension of the method to a unicycle-like mobile robot.
12.13. Implement in a computer program the motion planning method based
on the numerical navigation function. The program receives as input a twodimensional gridmap, in which some of the cells are labelled as ‘obstacles’, with
the speciﬁcation of the start and the goal cells. If the algorithm is successful,
a sequence of free cells from the start to the goal is provided as output. With
the aid of some examples, compare the average length of the solution paths
obtained using 1-adjacency and 2-adjacency to build the navigation function.

Appendices

A
Linear Algebra

Since modelling and control of robot manipulators requires an extensive use
of matrices and vectors as well as of matrix and vector operations, the goal
of this appendix is to provide a brush-up of linear algebra.

A.1 Deﬁnitions
A matrix of dimensions (m × n), with m and n positive integers, is an array
of elements aij arranged into m rows and n columns:
⎡

a11
⎢ a21
A = [aij ] i = 1, . . . , m = ⎢
⎣ ...

a12
a22
..
.

...
...
..
.

⎤
a1n
a2n ⎥
.
.. ⎥
. ⎦

am1

am2

...

amn

j = 1, . . . , n

(A.1)

If m = n, the matrix is said to be square; if m < n, the matrix has more
columns than rows; if m > n the matrix has more rows than columns. Further,
if n = 1, the notation (A.1) is used to represent a (column) vector a of
dimensions (m × 1);1 the elements ai are said to be vector components.
A square matrix A of dimensions (n × n) is said to be upper triangular if
aij = 0 for i > j:
⎤
⎡
a11 a12 . . . a1n
⎢ 0 a22 . . . a2n ⎥
;
A=⎢
..
.. ⎥
..
⎣ ...
.
.
. ⎦
0

0

...

ann

the matrix is said to be lower triangular if aij = 0 for i < j.
1

According to standard mathematical notation, small boldface is used to denote
vectors while capital boldface is used to denote matrices. Scalars are denoted by
roman characters.

564

A Linear Algebra

An (n × n) square matrix A is said to be diagonal if aij = 0 for i = j, i.e.,
⎡

a11
⎢ 0
A=⎢
⎣ ...
0

0
a22
..
.

...
...
..
.

0

...

⎤
0
0 ⎥
= diag{a11 , a22 , . . . , ann }.
.. ⎥
. ⎦
ann

If an (n × n) diagonal matrix has all unit elements on the diagonal (aii = 1),
the matrix is said to be identity and is denoted by I n .2 A matrix is said to be
null if all its elements are null and is denoted by O. The null column vector
is denoted by 0.
The transpose AT of a matrix A of dimensions (m × n) is the matrix of
dimensions (n×m) which is obtained from the original matrix by interchanging
its rows and columns:
⎡
⎤
a11 a21 . . . am1
⎢ a12 a22 . . . am2 ⎥
.
(A.2)
AT = ⎢
..
.. ⎥
..
⎣ ...
.
.
. ⎦
a1n

a2n

...

amn

The transpose of a column vector a is the row vector aT .
An (n × n) square matrix A is said to be symmetric if AT = A, and thus
aij = aji :
⎡
⎤
a11 a12 . . . a1n
⎢ a12 a22 . . . a2n ⎥
.
A=⎢
..
.. ⎥
..
⎣ ...
.
.
. ⎦
a1n

a2n

...

ann

An (n × n) square matrix A is said to be skew-symmetric if AT = −A, and
thus aij = −aji for i = j and aii = 0, leading to
⎡

0
⎢ −a12
A=⎢
⎣ ...
−a1n

a12
0
..
.

...
...
..
.

−a2n

...

⎤
a1n
a2n ⎥
.
.. ⎥
. ⎦
0

A partitioned matrix is a matrix whose elements are matrices (blocks) of
proper dimensions:
⎡

2

A11
⎢ A21
A=⎢
⎣ ...

A12
A22
..
.

...
...
..
.

⎤
A1n
A2n ⎥
.
.. ⎥
. ⎦

Am1

Am2

...

Amn

Subscript n is usually omitted if the dimensions are clear from the context.

A Linear Algebra

565

A partitioned matrix may be block-triangular or block-diagonal. Special partitions of a matrix are that by columns
A = [ a1
and that by rows

a2

...

an ]

⎡ aT ⎤
1

⎢ aT
⎢ 2
A=⎢
⎢ ..
⎣ .

⎥
⎥
⎥.
⎥
⎦

aTm
Given a square matrix A of dimensions (n × n), the algebraic complement
A(ij) of element aij is the matrix of dimensions ((n − 1) × (n − 1)) which is
obtained by eliminating row i and column j of matrix A.

A.2 Matrix Operations
The trace of an (n × n) square matrix A is the sum of the elements on the
diagonal:
n

aii .
(A.3)
Tr(A) =
i=1

Two matrices A and B of the same dimensions (m × n) are equal if aij =
bij . If A and B are two matrices of the same dimensions, their sum is the
matrix
C =A+B
(A.4)
whose elements are given by cij = aij + bij . The following properties hold:
A+O =A
A+B =B+A
(A + B) + C = A + (B + C).
Notice that two matrices of the same dimensions and partitioned in the same
way can be summed formally by operating on the blocks in the same position
and treating them like elements.
The product of a scalar α by an (m × n) matrix A is the matrix αA whose
elements are given by αaij . If A is an (n × n) diagonal matrix with all equal
elements on the diagonal (aii = a), it follows that A = aI n .
If A is a square matrix, one may write
A = As + Aa
where
As =

1
(A + AT )
2

(A.5)

(A.6)

566

A Linear Algebra

is a symmetric matrix representing the symmetric part of A, and
Aa =

1
(A − AT )
2

(A.7)

is a skew-symmetric matrix representing the skew-symmetric part of A.
The row-by-column product of a matrix A of dimensions (m × p) by a
matrix B of dimensions (p × n) is the matrix of dimensions (m × n)
C = AB
(A.8)
/p
whose elements are given by cij = k=1 aik bkj . The following properties hold:
A = AI p = I m A
A(BC) = (AB)C
A(B + C) = AB + AC
(A + B)C = AC + BC
(AB)T = B T AT .
Notice that, in general, AB = BA, and AB = O does not imply that A = O
or B = O; further, notice that AC = BC does not imply that A = B.
If an (m × p) matrix A and a (p × n) matrix B are partitioned in such a
way that the number of blocks for each row of A is equal to the number of
blocks for each column of B, and the blocks Aik and B kj have dimensions
compatible with product, the matrix product AB can be formally obtained by
operating by rows and columns on the blocks of proper position and treating
them like elements.
For an (n × n) square matrix A, the determinant of A is the scalar given
by the following expression, which holds ∀i = 1, . . . , n:
det(A) =

n



aij (−1)i+j det A(ij) .

(A.9)

j=1

The determinant can be computed according to any row i as in (A.9); the
same result is obtained by computing it according to any column j. If n = 1,
then det(a11 ) = a11 . The following property holds:
det(A) = det(AT ).
Moreover, interchanging two generic columns p and q of a matrix A yields


det [ a1 . . . ap . . . aq . . . an ] = −det [ a1 . . . aq . . . ap . . . an ] .
As a consequence, if a matrix has two equal columns (rows), then its determinant is null. Also, it is det(αA) = αn det(A).
Given an (m × n) matrix A, the determinant of the square block obtained
by selecting an equal number k of rows and columns is said to be k-order minor

A Linear Algebra

567

of matrix A. The minors obtained by taking the ﬁrst k rows and columns of
A are said to be principal minors.
If A and B are square matrices, then
det(AB) = det(A)det(B).

(A.10)

If A is an (n × n) triangular matrix (in particular diagonal), then
det(A) =

n
3

aii .

i=1

More generally, if A is block-triangular with m blocks Aii on the diagonal,
then
m
3
det(A) =
det(Aii ).
i=1

A square matrix A is said to be singular when det(A) = 0.
The rank "(A) of a matrix A of dimensions (m × n) is the maximum
integer r so that at least a non-null minor of order r exists. The following
properties hold:
"(A) ≤ min{m, n}
"(A) = "(AT )
"(AT A) = "(A)
"(AB) ≤ min{"(A), "(B)}.
A matrix so that "(A) = min{m, n} is said to be full-rank .
The adjoint of a square matrix A is the matrix
Adj A = [(−1)i+j det(A(ij) )]T

.

i = 1, . . . , n
j = 1, . . . , n

(A.11)

An (n × n) square matrix A is said to be invertible if a matrix A−1 exists,
termed inverse of A, so that
A−1 A = AA−1 = I n .
Since "(I n ) = n, an (n × n) square matrix A is invertible if and only if
"(A) = n, i.e., det(A) = 0 (nonsingular matrix). The inverse of A can be
computed as
1
Adj A.
(A.12)
A−1 =
det(A)
The following properties hold:
(A−1 )−1 = A
(AT )−1 = (A−1 )T .

568

A Linear Algebra

If the inverse of a square matrix is equal to its transpose
AT = A−1

(A.13)

then the matrix is said to be orthogonal ; in this case it is
AAT = AT A = I.

(A.14)

A square matrix A is said idempotent if
AA = A.

(A.15)

If A and B are invertible square matrices of the same dimensions, then
(AB)−1 = B −1 A−1 .

(A.16)

Given n square matrices Aii all invertible, the following expression holds:
−1
−1
diag{A11 , . . . , Ann }
= diag{A−1
11 , . . . , Ann }.
where diag{A11 , . . . , Ann } denotes the block-diagonal matrix.
If A and C are invertible square matrices of proper dimensions, the following expression holds:
(A + BCD)−1 = A−1 − A−1 B(DA−1 B + C −1 )−1 DA−1 ,
where the matrix DA−1 B + C −1 must be invertible.
If a block-partitioned matrix is invertible, then its inverse is given by the
general expression


−1  −1
A + EΔ−1 F −EΔ−1
A D
=
(A.17)
−Δ−1 F
Δ−1
C B
where Δ = B − CA−1 D, E = A−1 D and F = CA−1 , under the assumption
that the inverses of matrices A and Δ exist. In the case of a block-triangular
matrix, invertibility of the matrix requires invertibility of the blocks on the
diagonal. The following expressions hold:



−1
A−1
O
A O
=
−B −1 CA−1 B −1
C B


−1  −1
A
−A−1 DB −1
A D
=
.
O
B −1
O B
The derivative of an (m × n) matrix A(t), whose elements aij (t) are differentiable functions, is the matrix


d
d
aij (t)
.
(A.18)
Ȧ(t) = A(t) =
i = 1, . . . , m
dt
dt
j = 1, . . . , n

A Linear Algebra

569

If an (n × n) square matrix A(t) is so that "(A(t)) = n ∀t and its elements
aij (t) are diﬀerentiable functions, then the derivative of the inverse of A(t)
is given by
d −1
A (t) = −A−1 (t)Ȧ(t)A−1 (t).
(A.19)
dt
Given a scalar function f (x), endowed with partial derivatives with respect
to the elements xi of the (n × 1) vector x, the gradient of function f with
respect to vector x is the (n × 1) column vector

∇x f (x) =

∂f (x)
∂x



T
=

∂f (x)
∂x1

∂f (x)
∂x2

...

∂f (x)
∂xn

T
.

(A.20)

Further, if x(t) is a diﬀerentiable function with respect to t, then
∂f
d
ẋ = ∇Tx f (x)ẋ.
f˙(x) = f (x(t)) =
dt
∂x

(A.21)

Given a vector function g(x) of dimensions (m × 1), whose elements gi are
diﬀerentiable with respect to the vector x of dimensions (n × 1), the Jacobian
matrix (or simply Jacobian) of the function is deﬁned as the (m × n) matrix
⎡ ∂g (x) ⎤
1
⎢ ∂x ⎥
⎢
⎥
⎢ ∂g2 (x) ⎥
⎢
⎥
∂g(x) ⎢
= ⎢ ∂x ⎥
J g (x) =
⎥.
∂x
⎢
⎥
..
⎢
⎥
.
⎣
⎦
∂gm (x)
∂x

(A.22)

If x(t) is a diﬀerentiable function with respect to t, then
ġ(x) =

d
∂g
g(x(t)) =
ẋ = J g (x)ẋ.
dt
∂x

(A.23)

A.3 Vector Operations
Given n vectors xi of dimensions (m × 1), they are said to be linearly independent if the expression
k1 x1 + k2 x2 + . . . + kn xn = 0
holds true only when all the constants ki vanish. A necessary and suﬃcient
condition for the vectors x1 , x2 . . . , xn to be linearly independent is that the
matrix
A = [ x1 x2 . . . xn ]

570

A Linear Algebra

has rank n; this implies that a necessary condition for linear independence
is that n ≤ m. If instead "(A) = r < n, then only r vectors are linearly
independent and the remaining n − r vectors can be expressed as a linear
combination of the previous ones.
A system of vectors X is a vector space on the ﬁeld of real numbers IR if
the operations of sum of two vectors of X and product of a scalar by a vector
of X have values in X and the following properties hold:
x+y =y+x
∀x, y ∈ X
(x + y) + z = x + (y + z)
∀x, y, z ∈ X
∃0 ∈ X : x + 0 = x
∀x ∈ X
∀x ∈ X , ∃(−x) ∈ X : x + (−x) = 0
∀x ∈ X
1x = x
α(βx) = (αβ)x
∀α, β ∈ IR ∀x ∈ X
(α + β)x = αx + βx
∀α, β ∈ IR ∀x ∈ X
α(x + y) = αx + αy

∀α ∈ IR ∀x, y ∈ X .

The dimension of the space dim(X ) is the maximum number of linearly independent vectors x in the space. A set {x1 , x2 , . . . , xn } of linearly independent
vectors is a basis of vector space X , and each vector y in the space can be
uniquely expressed as a linear combination of vectors from the basis
y = c1 x1 + c2 x2 + . . . + cn xn ,

(A.24)

where the constants c1 , c2 , . . . , cn are said to be the components of the vector
y in the basis {x1 , x2 , . . . , xn }.
A subset Y of a vector space X is a subspace Y ⊆ X if it is a vector space
with the operations of vector sum and product of a scalar by a vector, i.e.,
αx + βy ∈ Y

∀α, β ∈ IR ∀x, y ∈ Y.

According to a geometric interpretation, a subspace is a hyperplane passing
by the origin (null element) of X .
The scalar product < x, y > of two vectors x and y of dimensions (m ×
1) is the scalar that is obtained by summing the products of the respective
components in a given basis
< x, y >= x1 y1 + x2 y2 + . . . + xm ym = xT y = y T x.

(A.25)

Two vectors are said to be orthogonal when their scalar product is null:
xT y = 0.
The norm of a vector can be deﬁned as
√
x = xT x.

(A.26)

(A.27)

A Linear Algebra

571

It is possible to show that both the triangle inequality
x + y ≤ x + y

(A.28)

and the Schwarz inequality
|xT y| ≤ x y.

(A.29)
T

hold. A unit vector x̂ is a vector whose norm is unity, i.e., x̂ x̂ = 1. Given a
vector x, its unit vector is obtained by dividing each component by its norm:
)=
x

1
x.
x

(A.30)

A typical example of vector space is the Euclidean space whose dimension is
3; in this case a basis is constituted by the unit vectors of a coordinate frame.
The vector product of two vectors x and y in the Euclidean space is the
vector
⎤
⎡
x2 y3 − x3 y2
(A.31)
x × y = ⎣ x3 y1 − x1 y3 ⎦ .
x1 y2 − x2 y1
The following properties hold:
x×x=0
x × y = −y × x
x × (y + z) = x × y + x × z.
The vector product of two vectors x and y can be expressed also as the
product of a matrix operator S(x) by the vector y. In fact, by introducing
the skew-symmetric matrix
⎡
⎤
0
−x3 x2
S(x) = ⎣ x3
(A.32)
0
−x1 ⎦
−x2 x1
0
obtained with the components of vector x, the vector product x × y is given
by
x × y = S(x)y = −S(y)x
(A.33)
as can be easily veriﬁed. Moreover, the following properties hold:
S(x)x = S T (x)x = 0
S(αx + βy) = αS(x) + βS(y).
Given three vectors x, y, z in the Euclidean space, the following expressions hold for the scalar triple products:
xT (y × z) = y T (z × x) = z T (x × y).

(A.34)

If any two vectors of three are equal, then the scalar triple product is null;
e.g.,
xT (x × y) = 0.

572

A Linear Algebra

A.4 Linear Transformation
Consider a vector space X of dimension n and a vector space Y of dimension
m with m ≤ n. The linear transformation (or linear map) between the vectors
x ∈ X and y ∈ Y can be deﬁned as
y = Ax

(A.35)

in terms of the matrix A of dimensions (m × n). The range space (or simply
range) of the transformation is the subspace
R(A) = {y : y = Ax, x ∈ X } ⊆ Y,

(A.36)

which is the subspace generated by the linearly independent columns of matrix
A taken as a basis of Y. It is easy to recognize that
"(A) = dim(R(A)).

(A.37)

On the other hand, the null space (or simply null) of the transformation is
the subspace
N (A) = {x : Ax = 0, x ∈ X } ⊆ X .
(A.38)
Given a matrix A of dimensions (m × n), the notable result holds:
"(A) + dim(N (A)) = n.

(A.39)

Therefore, if "(A) = r ≤ min{m, n}, then dim(R(A)) = r and dim(N (A)) =
n − r. It follows that if m < n, then N (A) = ∅ independently of the rank of
A; if m = n, then N (A) = ∅ only in the case of "(A) = r < m.
If x ∈ N (A) and y ∈ R(AT ), then y T x = 0, i.e., the vectors in the null
space of A are orthogonal to each vector in the range space of the transpose
of A. It can be shown that the set of vectors orthogonal to each vector of
the range space of AT coincides with the null space of A, whereas the set of
vectors orthogonal to each vector in the null space of AT coincides with the
range space of A. In symbols:
N (A) ≡ R⊥ (AT )

R(A) ≡ N ⊥ (AT )

(A.40)

where ⊥ denotes the orthogonal complement of a subspace.
If the matrix A in (A.35) is square and idempotent, the matrix represents
the projection of space X into a subspace.
A linear transformation allows the deﬁnition of the norm of a matrix A
induced by the norm deﬁned for a vector x as follows. In view of the property
Ax ≤ A x,

(A.41)

the norm of A can be deﬁned as
Ax
x=0 x

A = sup

(A.42)

A Linear Algebra

573

which can also be computed as
max Ax.
x

=1

A direct consequence of (A.41) is the property
AB ≤ A B.
A diﬀerent norm of a matrix is the Frobenius norm deﬁned as

1/2
AF = Tr(AT A)

(A.43)

(A.44)

A.5 Eigenvalues and Eigenvectors
Consider the linear transformation on a vector u established by an (n × n)
square matrix A. If the vector resulting from the transformation has the same
direction of u (with u = 0), then
Au = λu.

(A.45)

The equation in (A.45) can be rewritten in matrix form as
(λI − A)u = 0.

(A.46)

For the homogeneous system of equations in (A.46) to have a solution diﬀerent
from the trivial one u = 0, it must be
det(λI − A) = 0

(A.47)

which is termed a characteristic equation. Its solutions λ1 , . . . , λn are the
eigenvalues of matrix A; they coincide with the eigenvalues of matrix AT . On
the assumption of distinct eigenvalues, the n vectors ui satisfying the equation
(λi I − A)ui = 0

i = 1, . . . , n

(A.48)

are said to be the eigenvectors associated with the eigenvalues λi .
The matrix U formed by the column vectors ui is invertible and constitutes
a basis in the space of dimension n. Further, the similarity transformation
established by U
(A.49)
Λ = U −1 AU
4n
is so that Λ = diag{λ1 , . . . , λn }. It follows that det(A) = i=1 λi .
If the matrix A is symmetric, its eigenvalues are real and Λ can be written
as
(A.50)
Λ = U T AU ;
hence, the eigenvector matrix U is orthogonal.

574

A Linear Algebra

A.6 Bilinear Forms and Quadratic Forms
A bilinear form in the variables xi and yj is the scalar
B=

m 
n


aij xi yj

i=1 j=1

which can be written in matrix form
B(x, y) = xT Ay = y T AT x

(A.51)

where x = [ x1 x2 . . . xm ]T , y = [ y1 y2 . . . yn ]T , and A is the (m ×
n) matrix of the coeﬃcients aij representing the core of the form.
A special case of bilinear form is the quadratic form
Q(x) = xT Ax

(A.52)

where A is an (n × n) square matrix. Hence, for computation of (A.52), the
matrix A can be replaced with its symmetric part As given by (A.6). It follows
that if A is a skew-symmetric matrix, then
xT Ax = 0

∀x.

The quadratic form (A.52) is said to be positive deﬁnite if
xT Ax > 0 ∀x = 0

xT Ax = 0 x = 0.

(A.53)

The matrix A core of the form is also said to be positive deﬁnite. Analogously,
a quadratic form is said to be negative deﬁnite if it can be written as −Q(x) =
−xT Ax where Q(x) is positive deﬁnite.
A necessary condition for a square matrix to be positive deﬁnite is that
its elements on the diagonal are strictly positive. Further, in view of (A.50),
the eigenvalues of a positive deﬁnite matrix are all positive. If the eigenvalues
are not known, a necessary and suﬃcient condition for a symmetric matrix to
be positive deﬁnite is that its principal minors are strictly positive (Sylvester
criterion). It follows that a positive deﬁnite matrix is full-rank and thus it is
always invertible.
A symmetric positive deﬁnite matrix A can always be decomposed as
A = U T ΛU

(A.54)

where U is an orthogonal matrix of eigenvectors (U T U = I) and Λ is the
diagonal matrix of the eigenvalues of A.
Let λmin (A) and λmax (A) respectively denote the smallest and largest
eigenvalues of a positive deﬁnite matrix A (λmin , λmax > 0). Then, the
quadratic form in (A.52) satisﬁes the following inequality:
λmin (A)x2 ≤ xT Ax ≤ λmax (A)x2 .

(A.55)

A Linear Algebra

575

An (n × n) square matrix A is said to be positive semi-deﬁnite if
xT Ax ≥ 0

∀x.

(A.56)

This deﬁnition implies that "(A) = r < n, and thus r eigenvalues of A
are positive and n − r are null. Therefore, a positive semi-deﬁnite matrix A
has a null space of ﬁnite dimension, and speciﬁcally the form vanishes when
x ∈ N (A). A typical example of a positive semi-deﬁnite matrix is the matrix
A = H T H where H is an (m × n) matrix with m < n. In an analogous way,
a negative semi-deﬁnite matrix can be deﬁned.
Given the bilinear form in (A.51), the gradient of the form with respect
to x is given by

T
∂B(x, y)
= Ay,
(A.57)
∇x B(x, y) =
∂x
whereas the gradient of B with respect to y is given by

∇y B(x, y) =

∂B(x, y)
∂y

T
= AT x.

(A.58)

Given the quadratic form in (A.52) with A symmetric, the gradient of the
form with respect to x is given by

∇x Q(x) =

∂Q(x)
∂x

T
= 2Ax.

(A.59)

Further, if x and A are diﬀerentiable functions of t, then
Q̇(x) =

d
Q(x(t)) = 2xT Aẋ + xT Ȧx;
dt

(A.60)

if A is constant, then the second term obviously vanishes.

A.7 Pseudo-inverse
The inverse of a matrix can be deﬁned only when the matrix is square and
nonsingular. The inverse operation can be extended to the case of non-square
matrices. Consider a matrix A of dimensions (m × n) with "(A) = min{m, n}
If m < n, a right inverse of A can be deﬁned as the matrix Ar of dimensions (n × m) so that
AAr = I m .
If instead m > n, a left inverse of A can be deﬁned as the matrix Al of
dimensions (n × m) so that
Al A = I n .

576

A Linear Algebra

If A has more columns than rows (m < n) and has rank m, a special right
inverse is the matrix
(A.61)
A†r = AT (AAT )−1
which is termed right pseudo-inverse, since AA†r = I m . If W r is an (n × n)
positive deﬁnite matrix, a weighted right pseudo-inverse is given by
T
−1 T −1
.
A†r = W −1
r A (AW r A )

(A.62)

If A has more rows than columns (m > n) and has rank n, a special left
inverse is the matrix
(A.63)
A†l = (AT A)−1 AT
which is termed left pseudo-inverse, since A†l A = I n .3 If W l is an (m × m)
positive deﬁnite matrix, a weighted left pseudo-inverse is given by
A†l = (AT W l A)−1 AT W l .

(A.64)

The pseudo-inverse is very useful to invert a linear transformation y = Ax
with A a full-rank matrix. If A is a square nonsingular matrix, then obviously
x = A−1 y and then A†l = A†r = A−1 .
If A has more columns than rows (m < n) and has rank m, then the
solution x for a given y is not unique; it can be shown that the expression
x = A† y + (I − A† A)k,

(A.65)

†

with k an arbitrary (n × 1) vector and A as in (A.61), is a solution to the
system of linear equations established by (A.35). The term A† y ∈ N ⊥ (A) ≡
R(AT ) minimizes the norm of the solution x. The term (I − A† A)k is the
projection of k in N (A) and is termed homogeneous solution; as k varies,
all the solutions to the homogeneous equation system Ax = 0 associated
with (A.35) are generated.
On the other hand, if A has more rows than columns (m > n), the equation
in (A.35) has no solution; it can be shown that an approximate solution is given
by
(A.66)
x = A† y
where A† as in (A.63) minimizes y − Ax. If instead y ∈ R(A), then (A.66)
is a real solution.
Notice that the use of the weighted (left or right) pseudo-inverses in the
solution to the linear equation systems leads to analogous results where the
minimized norms are weighted according to the metrics deﬁned by matrices
W r and W l , respectively.
The results of this section can be easily extended to the case of (square
or nonsquare) matrices A not having full-rank. In particular, the expression (A.66) (with the pseudo-inverse computed by means of the singular value
decomposition of A) gives the minimum-norm vector among all those minimizing y − Ax.
3

Subscripts l and r are usually omitted whenever the use of a left or right pseudoinverse is clear from the context.

A Linear Algebra

577

A.8 Singular Value Decomposition
For a nonsquare matrix it is not possible to deﬁne eigenvalues. An extension
of the eigenvalue concept can be obtained by singular values. Given a matrix
A of dimensions (m × n), the matrix AT A has n nonnegative eigenvalues
λ1 ≥ λ2 ≥ . . . ≥ λn ≥ 0 (ordered from the largest to the smallest) which can
be expressed in the form
σi ≥ 0.

λi = σi2

The scalars σ1 ≥ σ2 ≥ . . . ≥ σn ≥ 0 are said to be the singular values of
matrix A. The singular value decomposition (SVD) of matrix A is given by
A = U ΣV T

(A.67)

where U is an (m × m) orthogonal matrix
U = [ u1

u2

...

um ] ,

(A.68)

vn ]

(A.69)

D = diag{σ1 , σ2 , . . . , σr }

(A.70)

V is an (n × n) orthogonal matrix
V = [ v1
and Σ is an (m × n) matrix


D O
Σ=
O O

v2

...

where σ1 ≥ σ2 ≥ . . . ≥ σr > 0. The number of non-null singular values is
equal to the rank r of matrix A.
The columns of U are the eigenvectors of the matrix AAT , whereas the
columns of V are the eigenvectors of the matrix AT A. In view of the partitions
of U and V in (A.68), (A.69), it is Av i = σi ui , for i = 1, . . . , r and Av i = 0,
for i = r + 1, . . . , n.
Singular value decomposition is useful for analysis of the linear transformation y = Ax established in (A.35). According to a geometric interpretation,
the matrix A transforms the unit sphere in IRn deﬁned by x = 1 into the set
of vectors y = Ax which deﬁne an ellipsoid of dimension r in IRm . The singular values are the lengths of the various axes of the ellipsoid. The condition
number of the matrix
σ1
κ=
σr
is related to the eccentricity of the ellipsoid and provides a measure of
ill-conditioning (κ
1) for numerical solution of the system established
by (A.35).
It is worth noticing that the numerical procedure of singular value decomposition is commonly adopted to compute the (right or left) pseudoinverse A† , even in the case of a matrix A not having full rank. In fact,
from (A.67), (A.70) it is
(A.71)
A† = V Σ † U T

578

A Linear Algebra

with
Σ† =



D†
O

O
O



D † = diag



1 1
1
, ,...,
σ1 σ2
σr

*
.

(A.72)

Bibliography
A reference text on linear algebra is [169]. For matrix computation see [88].
The properties of pseudo-inverse matrices are discussed in [24].

B
Rigid-body Mechanics

The goal of this appendix is to recall some fundamental concepts of rigid body
mechanics which are preliminary to the study of manipulator kinematics,
statics and dynamics.

B.1 Kinematics
A rigid body is a system characterized by the constraint that the distance
between any two points is always constant.
Consider a rigid body B moving with respect to an orthonormal reference
frame O–xyz of unit vectors x, y, z, called ﬁxed frame. The rigidity assumption allows the introduction of an orthonormal frame O –x y  z  attached to
the body, called moving frame, with respect to which the position of any point
of B is independent of time. Let x (t), y  (t), z  (t) be the unit vectors of the
moving frame expressed in the ﬁxed frame at time t.
The orientation of the moving frame O –x y  z  at time t with respect to
the ﬁxed frame O–xyz can be expressed by means of the orthogonal (3 × 3)
matrix
⎡ T
⎤
x (t)x y T (t)x z T (t)x
R(t) = ⎣ xT (t)y y T (t)y z T (t)y ⎦ ,
(B.1)
xT (t)z y T (t)z z T (t)z
which is termed rotation matrix deﬁned in the orthonormal special group
SO(3) of the (3 × 3) matrices with orthonormal columns and determinant
equal to 1. The columns of the matrix in (B.1) represent the components
of the unit vectors of the moving frame when expressed in the ﬁxed frame,
whereas the rows represent the components of the unit vectors of the ﬁxed
frame when expressed in the moving frame.
Let p be the constant position vector of a generic point P of B in the
moving frame O –x y  z  . The motion of P with respect to the ﬁxed frame
O–xyz is described by the equation
p(t) = pO (t) + R(t)p ,

(B.2)

580

B Rigid-body Mechanics

where pO (t) is the position vector of origin O of the moving frame with
respect to the ﬁxed frame.
Notice that a position vector is a bound vector since its line of application
and point of application are both prescribed, in addition to its direction; the
point of application typically coincides with the origin of a reference frame.
Therefore, to transform a bound vector from a frame to another, both translation and rotation between the two frames must be taken into account.
If the positions of the points of B in the moving frame are known, it follows
from (B.2) that the motion of each point of B with respect to the ﬁxed frame
is uniquely determined once the position of the origin and the orientation
of the moving frame with respect to the ﬁxed frame are speciﬁed in time.
The origin of the moving frame is determined by three scalar functions of
time. Since the orthonormality conditions impose six constraints on the nine
elements of matrix R(t), the orientation of the moving frame depends only
on three independent scalar functions, three being the minimum number of
parameters to represent SO(3).1
Therefore, a rigid body motion is described by arbitrarily specifying six
scalar functions of time, which describe the body pose (position + orientation).
The resulting rigid motions belong to the special Euclidean group SE(3) =
IR3 × SO(3).
The expression in (B.2) continues to hold if the position vector pO (t) of
the origin of the moving frame is replaced with the position vector of any
other point of B, i.e.,
p(t) = pQ (t) + R(t)(p − pQ )

(B.3)

where pQ (t) and pQ are the position vectors of a point Q of B in the ﬁxed
and moving frames, respectively.
In the following, for simplicity of notation, the dependence on the time
variable t will be dropped.
Diﬀerentiating (B.3) with respect to time gives the known velocity composition rule
ṗ = ṗQ + ω × (p − pQ ),
(B.4)
where ω is the angular velocity of rigid body B. Notice that ω is a free vector
since its point of application is not prescribed. To transform a free vector from
a frame to another, only rotation between the two frames must be taken into
account.
By recalling the deﬁnition of the skew-symmetric operator S(·) in (A.32),
the expression in (B.4) can be rewritten as
ṗ = ṗQ + S(ω)(p − pQ )
= ṗQ + S(ω)R(p − pQ ).
1

The minimum number of parameters represent a special orthonormal
group SO(m) is equal to m(m − 1)/2.

B Rigid-body Mechanics

581

Comparing this equation with the formal time derivative of (B.3) leads to the
result
Ṙ = S(ω)R.
(B.5)
In view of (B.4), the elementary displacement of a point P of the rigid body
B in the time interval (t, t + dt) is

dp = ṗdt = ṗQ + ω × (p − pQ ) dt
(B.6)
= dpQ + ωdt × (p − pQ ).
Diﬀerentiating (B.4) with respect to time yields the following expression
for acceleration:

p̈ = p̈Q + ω̇ × (p − pQ ) + ω × ω × (p − pQ ) .
(B.7)

B.2 Dynamics
Let ρdV be the mass of an elementary particle of a rigid body B, where ρ
denotes the density
% of the particle of volume dV . Also let VB be the body
volume and m = VB ρdV its total mass assumed to be constant. If p denotes
the position vector of the particle of mass ρdV in the frame O–xyz, the centre
of mass of B is deﬁned as the point C whose position vector is
"
1
pC =
pρdV .
(B.8)
m VB
In the case when B is the union of n distinct parts of mass m1 , . . . , mn and
centres of mass pC1 . . . pCn , the centre of mass of B can be computed as
1 
mi pCi
m i=1
n

pC =

/n
with m = i=1 mi .
Let r be a line passing by O and d(p) the distance from r of the particle
of B of mass ρdV and position vector p. The moment of inertia of body B
with respect to line r is deﬁned as the positive scalar
"
d2 (p)ρdV .
Ir =
VB

Let r denote the unit vector of line r; then, the moment of inertia of B with
respect to line r can be expressed as

"
T
T
Ir = r
S (p)S(p)ρdV r = r T I O r,
(B.9)
VB

582

B Rigid-body Mechanics

where S(·) is the skew-symmetric operator in (A.31), and the symmetric,
positive deﬁnite matrix
%
%
⎤
⎡%
(p2 + p2z )ρdV
− VB px py ρdV
− VB px pz ρdV
VB y
%
%
⎢
∗
(p2 + p2z )ρdV
− VB py pz ρdV ⎥
IO = ⎣
⎦
VB x
%
2
2
∗
∗
(p + py )ρdV
VB x
⎤
⎡
IOxx −IOxy −IOxz
⎥
⎢
IOyy −IOyz ⎦
=⎣ ∗
(B.10)
∗

∗

IOzz

is termed inertia tensor of body B relative to pole O.2 The (positive) elements
IOxx , IOyy , IOzz are the inertia moments with respect to three coordinate axes
of the reference frame, whereas the elements IOxy , IOxz , IOyz (of any sign)
are said to be products of inertia.
The expression of the inertia tensor of a rigid body B depends both on the
pole and the reference frame. If orientation of the reference frame with origin
at O is changed according to a rotation matrix R, the inertia tensor I O in
the new frame is related to I O by the relationship
I O = RI O RT .

(B.11)

The way an inertia tensor is transformed when the pole is changed can be
inferred by the following equation, also known as Steiner theorem or parallel
axis theorem:
(B.12)
I O = I C + mS T (pC )S(pC ),
where I C is the inertia tensor relative to the centre of mass of B, when expressed in a frame parallel to the frame with origin at O and with origin at
the centre of mass C.
Since the inertia tensor is a symmetric positive deﬁnite matrix, there always exists a reference frame in which the inertia tensor attains a diagonal
form; such a frame is said to be a principal frame (relative to pole O) and
its coordinate axes are said to be principal axes. In the case when pole O
coincides with the centre of mass, the frame is said to be a central frame and
its axes are said to be central axes.
Notice that if the rigid body is moving with respect to the reference frame
with origin at O, then the elements of the inertia tensor I O become a function of time. With respect to a pole and a reference frame attached to the
body (moving frame), instead, the elements of the inertia tensor represent six
structural constants of the body which are known once the pole and reference
frame have been speciﬁed.

2

The symbol ‘∗’ has been used to avoid rewriting the symmetric elements.

B Rigid-body Mechanics

583

Let ṗ be the velocity of a particle of B of elementary mass ρdV in frame
O–xyz. The linear momentum of body B is deﬁned as the vector
"
ṗρdV = mṗC .
(B.13)
l=
VB

Let Ω be any point in space and pΩ its position vector in frame O–xyz;
then, the angular momentum of body B relative to pole Ω is deﬁned as the
vector
"
ṗ × (pΩ − p)ρdV .
kΩ =
VB

The pole can be either ﬁxed or moving with respect to the reference frame.
The angular momentum of a rigid body has the following notable expression:
kΩ = I C ω + mṗC × (pΩ − pC ),

(B.14)

where I C is the inertia tensor relative to the centre of mass, when expressed
in a frame parallel to the reference frame with origin at the centre of mass.
The forces acting on a generic system of material particles can be distinguished into internal forces and external forces.
The internal forces, exerted by one part of the system on another, have
null linear and angular momentum and thus they do not inﬂuence rigid body
motion.
The external forces, exerted on the system by an agency outside the system, in the case of a rigid body B are distinguished into active forces and
reaction forces.
The active forces can be either concentrated forces or body forces. The
former are applied to speciﬁc points of B, whereas the latter act on all elementary particles of the body. An example of body force is the gravitational
force which, for any elementary particle of mass ρdV , is equal to g 0 ρdV where
g 0 is the gravity acceleration vector.
The reaction forces are those exerted because of surface contact between
two or more bodies. Such forces can be distributed on the contact surfaces or
they can be assumed to be concentrated.
For a rigid body B subject to gravitational force, as well as to active and
or reaction forces f 1 . . . f n concentrated at points p1 . . . pn , the resultant of
the external forces f and the resultant moment μΩ with respect to a pole Ω
are respectively
"
n
n


g 0 ρdV +
f i = mg 0 +
fi
(B.15)
f =
VB

"
μΩ =

VB

i=1

i=1

g 0 × (pΩ − p)ρdV +

= mg 0 × (pΩ − pC ) +

n


f i × (pΩ − pi )

i=1
n

i=1

f i × (pΩ − pi ).

(B.16)

584

B Rigid-body Mechanics

In the case when f and μΩ are known and it is desired to compute the
resultant moment with respect to a point Ω  other than Ω, the following
relation holds:
(B.17)
μΩ  = μΩ + f × (pΩ  − pΩ ).
Consider now a generic system of material particles subject to external
forces of resultant f and resultant moment μΩ . The motion of the system
in a frame O–xyz is established by the following fundamental principles of
dynamics (Newton laws of motion):
f = l̇
μΩ = k̇Ω

(B.18)
(B.19)

where Ω is a pole ﬁxed or coincident with the centre of mass C of the system.
These equations hold for any mechanical system and can be used even in the
case of variable mass. For a system with constant mass, computing the time
derivative of the momentum in (B.18) gives Newton equations of motion in
the form
(B.20)
f = mp̈C ,
where the quantity on the right-hand side represents the resultant of inertia
forces.
If, besides the assumption of constant mass, the assumption of rigid system
holds too, the expression in (B.14) of the angular momentum with (B.19) yield
Euler equations of motion in the form
μΩ = I Ω ω̇ + ω × (I Ω ω),

(B.21)

where the quantity on the right-hand side represents the resultant moment of
inertia forces.
For a system constituted by a set of rigid bodies, the external forces obviously do not include the reaction forces exerted between the bodies belonging
to the same system.

B.3 Work and Energy
Given a force f i applied at a point of position pi with respect to frame O–xyz,
the elementary work of the force f i on the displacement dpi = ṗi dt is deﬁned
as the scalar
dWi = f Ti dpi .
For a rigid body B subject to a system of forces of resultant f and resultant
moment μQ with respect to any point Q of B, the elementary work on the
rigid displacement (B.6) is given by
dW = (f T ṗQ + μTQ ω)dt = f T dpQ + μTQ ωdt.

(B.22)

B Rigid-body Mechanics

585

The kinetic energy of a body B is deﬁned as the scalar quantity
"
1
T =
ṗT ṗρdV
2 VB
which, for a rigid body, takes on the notable expression
T =

1
1
mṗTC ṗC + ω T I C ω
2
2

(B.23)

where I C is the inertia tensor relative to the centre of mass expressed in a
frame parallel to the reference frame with origin at the centre of mass.
A system of position forces, i.e., the forces depending only on the positions
of the points of application, is said to be conservative if the work done by each
force is independent of the trajectory described by the point of application of
the force but it depends only on the initial and ﬁnal positions of the point of
application. In this case, the elementary work of the system of forces is equal
to minus the total diﬀerential of a scalar function termed potential energy,
i.e.,
dW = −dU.
(B.24)
An example of a conservative system of forces on a rigid body is the gravitational force, with which is associated the potential energy
"
g T0 pρdV = −mg T0 pC .
(B.25)
U =−
VB

B.4 Constrained Systems
Consider a system Br of r rigid bodies and assume that all the elements of Br
can reach any position in space. In order to ﬁnd uniquely the position of all the
points of the system, it is necessary to assign a vector x = [ x1 . . . xp ]T
of 6r = p parameters, termed conﬁguration. These parameters are termed
Lagrange or generalized coordinates of the unconstrained system Br , and p
determines the number of degrees of freedom (DOFs).
Any limitation on the mobility of the system Br is termed constraint. A
constraint acting on Br is said to be holonomic if it is expressed by a system
of equations
h(x, t) = 0,
(B.26)
where h is a vector of dimensions (s × 1), with s < m. On the other hand,
a constraint in the form h(x, ẋ, t) = 0 which is nonintegrable is said to
be nonholonomic. For simplicity, only equality (or bilateral ) constraints are
considered. If the equations in (B.26) do not explicitly depend on time, the
constraint is said to be scleronomic.
On the assumption that h has continuous and continuously diﬀerentiable
components, and its Jacobian ∂h/∂x has full rank, the equations in (B.26)

586

B Rigid-body Mechanics

allow the elimination of s out of m coordinates of the system Br . With the
remaining n = m − s coordinates it is possible to determine uniquely the
conﬁgurations of Br satisfying the constraints (B.26). Such coordinates are
the Lagrange or generalized coordinates and n is the number of degrees of
freedom of the unconstrained system Br .3
The motion of a system Br with n DOFs and holonomic equality constraints can be described by equations of the form
x = x(q(t), t),

(B.27)

where q(t) = [ q1 (t) . . . qn (t) ]T is a vector of Lagrange coordinates.
The elementary displacement of system (B.27) relative to the interval (t, t+
dt) is deﬁned as
∂x(q, t)
∂x(q, t)
dt.
(B.28)
dx =
q̇dt +
∂q
∂t
The virtual displacement of system (B.27) at time t, relative to an increment
δλ, is deﬁned as the quantity
δx =

∂x(q, t)
δq.
∂q

(B.29)

The diﬀerence between the elementary displacement and the virtual displacement is that the former is relative to an actual motion of the system in an
interval (t, t + dt) which is consistent with the constraints, while the latter is
relative to an imaginary motion of the system when the constraints are made
invariant and equal to those at time t.
For a system with time-invariant constraints, the equations of motion
(B.27) become
x = x(q(t)),
(B.30)
and then, by setting δλ = dλ = λ̇dt, the virtual displacements (B.29) coincide
with the elementary displacements (B.28).
To the concept of virtual displacement can be associated that of virtual
work of a system of forces, by considering a virtual displacement instead of
an elementary displacement.
If external forces are distinguished into active forces and reaction forces, a
direct consequence of the principles of dynamics (B.18), (B.19) applied to the
system of rigid bodies Br is that, for each virtual displacement, the following
relation holds:
(B.31)
δWm + δWa + δWh = 0,
where δWm , δWa , δWh are the total virtual works done by the inertia, active,
reaction forces, respectively.
3

In general, the Lagrange coordinates of a constrained system have a local validity;
in certain cases, such as the joint variables of a manipulator, they can have a global
validity.

B Rigid-body Mechanics

587

In the case of frictionless equality constraints, reaction forces are exerted
orthogonally to the contact surfaces and the virtual work is always null. Hence,
(B.31) reduces to
(B.32)
δWm + δWa = 0.
For a steady system, inertia forces are identically null. Then the condition
for the equilibrium of system Br is that the virtual work of the active forces
is identically null on any virtual displacement, which gives the fundamental
equation of statics of a constrained system
δWa = 0

(B.33)

known as principle of virtual work . Expressing (B.33) in terms of the increment δλ of generalized coordinates leads to
δWa = ζ T δq = 0

(B.34)

where ζ denotes the (n × 1) vector of active generalized forces.
In the dynamic case, it is worth distinguishing active forces into conservative (that can be derived from a potential) and nonconservative. The virtual
work of conservative forces is given by
δWc = −

∂U
δq,
∂q

(B.35)

where U(λ) is the total potential energy of the system. The work of nonconservative forces can be expressed in the form
δWnc = ξ T δq,

(B.36)

where ξ denotes the vector of nonconservative generalized forces. It follows
that the vector of active generalized forces is

T
∂U
ζ =ξ−
.
(B.37)
∂q
Moreover, the work of inertia forces can be computed from the total kinetic
energy of system T as


∂T
d ∂T
δWm =
−
δq.
(B.38)
∂q
dt ∂ q̇
Substituting (B.35), (B.36), (B.38) into (B.32) and observing that (B.32) holds
true for any increment δλ leads to Lagrange equations

T 
T
∂L
d ∂L
−
= ξ,
(B.39)
dt ∂ q̇
∂q
where
L=T −U

(B.40)

588

B Rigid-body Mechanics

is the Lagrangian function of the system. The equations in (B.39) completely
describe the dynamic behaviour of an n-DOF system with holonomic equality
constraints.
The sum of kinetic and potential energy of a system with time-invariant
constraints is termed Hamiltonian function
H = T + U.

(B.41)

Conservation of energy dictates that the time derivative of the Hamiltonian
must balance the power generated by the nonconservative forces acting on the
system, i.e.,
dH
(B.42)
= ξ T q̇.
dt
In view of (B.37), (B.41), the equation in (B.42) becomes
dT
= ζ T q̇.
dt

(B.43)

Bibliography
The fundamental concepts of rigid-body mechanics and constrained systems
can be found in classical texts such as [87, 154, 224]. An authoritative reference
on rigid-body system dynamics is [187].

C
Feedback Control

As a premise to the study of manipulator decentralized control and centralized
control, the fundamental principles of feedback control of linear systems are
recalled, and an approach to the determination of control laws for nonlinear
systems based on the use of Lyapunov functions is presented.

C.1 Control of Single-input/Single-output Linear
Systems
According to classical automatic control theory of linear time-invariant singleinput/single-output systems, in order to servo the output y(t) of a system to
a reference r(t), it is worth adopting a negative feedback control structure.
This structure indeed allows the use of approximate mathematical models to
describe the input/output relationship of the system to control, since negative
feedback has a potential for reducing the eﬀects of system parameter variations
and nonmeasurable disturbance inputs d(t) on the output.
This structure can be represented in the domain of complex variable s as in
the block scheme of Fig. C.1, where G(s), H(s) and C(s) are the transfer functions of the system to control, the transducer and the controller, respectively.
From this scheme it is easy to derive
Y (s) = W (s)R(s) + WD (s)D(s),
where
W (s) =

C(s)G(s)
1 + C(s)G(s)H(s)

(C.1)

(C.2)

is the closed-loop input/output transfer function and
WD (s) =

G(s)
1 + C(s)G(s)H(s)

is the disturbance/output transfer function.

(C.3)

590

C Feedback Control

Fig. C.1. Feedback control structure

The goal of the controller design is to ﬁnd a control structure C(s) ensuring
that the output variable Y (s) tracks a reference input R(s). Further, the
controller should guarantee that the eﬀects of the disturbance input D(s) on
the output variable are suitably reduced. The goal is then twofold, namely,
reference tracking and disturbance rejection.
The basic problem for controller design consists of the determination of an
action C(s) which can make the system asymptotically stable. In the absence
of positive or null real part pole/zero and zero/pole cancellation in the openloop function F (s) = C(s)G(s)H(s), a necessary and suﬃcient condition for
asymptotic stability is that the poles of W (s) and WD (s) have all negative
real parts; such poles coincide with the zeros of the rational transfer function
1 + F (s). Testing for this condition can be performed by resorting to stability
criteria, thus avoiding computation of the function zeros.
Routh criterion allows the determination of the sign of the real parts of
the zeros of the function 1 + F (s) by constructing a table with the coeﬃcients
of the polynomial at the numerator of 1 + F (s) (characteristic polynomial ).
Routh criterion is easy to apply for testing stability of a feedback system,
but it does not provide a direct relationship between the open-loop function
and stability of the closed-loop system. It is then worth resorting to Nyquist
criterion which is based on the representation, in the complex plane, of the
open-loop transfer function F (s) evaluated in the domain of real angular frequency (s = jω, −∞ < ω < +∞).
Drawing of Nyquist plot and computation of the number of circles made by
the vector representing the complex number 1 + F (jω) when ω continuously
varies from −∞ to +∞ allows a test on whether or not the closed-loop system
is asymptotically stable. It is also possible to determine the number of positive,
null and negative real part roots of the characteristic polynomial, similarly to
application of Routh criterion. Nonetheless, Nyquist criterion is based on the
plot of the open-loop transfer function, and thus it allows the determination of
a direct relationship between this function and closed-loop system stability. It
is then possible from an examination of the Nyquist plot to draw suggestions
on the controller structure C(s) which ensures closed-loop system asymptotic
stability.

C Feedback Control

591

If the closed-loop system is asymptotically stable, the steady-state response
to a sinusoidal input r(t), with d(t) = 0, is sinusoidal, too. In this case, the
function W (s), evaluated for s = jω, is termed frequency response function;
the frequency response function of a feedback system can be assimilated to
that of a low-pass ﬁlter with the possible occurrence of a resonance peak inside
its bandwidth.
As regards the transducer, this should be chosen so that its bandwidth
is much greater than the feedback system bandwidth, in order to ensure
a nearly instantaneous response for any value of ω inside the bandwidth
of W (jω). Therefore, setting H(jω) ≈ H0 and assuming that the loop gain
1 in the same bandwidth, the expression in (C.1) for
|C(jω)G(jω)H0 |
s = jω can be approximated as
Y (jω) ≈

D(jω)
R(jω)
+
.
H0
C(jω)H0

Assuming R(jω) = H0 Yd (jω) leads to
Y (jω) ≈ Yd (jω) +

D(jω)
;
C(jω)H0

(C.4)

i.e., the output tracks the desired output Yd (jω) and the frequency components of the disturbance in the bandwidth of W (jω) produce an eﬀect on the
output which can be reduced by increasing |C(jω)H0 |. Furthermore, if the
disturbance input is a constant, the steady-state output is not inﬂuenced by
the disturbance as long as C(s) has at least a pole at the origin.
Therefore, a feedback control system is capable of establishing a proportional relationship between the desired output and the actual output, as evidenced by (C.4). This equation, however, requires that the frequency content
of the input (desired output) be inside the frequency range for which the loop
gain is much greater than unity.
The previous considerations show the advantage of including a proportional
action and an integral action in the controller C(s), leading to the transfer
function
1 + sTI
(C.5)
C(s) = KI
s
of a proportional-integral controller (PI); TI is the time constant of the integral
action and the quantity KI TI is called proportional sensitivity.
The adoption of a PI controller is eﬀective for low-frequency response of
the system, but it may involve a reduction of stability margins and/or a reduction of closed-loop system bandwidth. To avoid these drawbacks, a derivative
action can be added to the proportional and integral actions, leading to the
transfer function
1 + sTI + s2 TD TI
(C.6)
C(s) = KI
s
of a proportional-integral-derivative controller (PID); TD denotes the time
constant of the derivative action. Notice that physical realizability of (C.6)

592

C Feedback Control

demands the introduction of a high-frequency pole which little inﬂuences the
input/output relationship in the system bandwidth. The transfer function
in (C.6) is characterized by the presence of two zeros which provide a stabilizing action and an enlargement of the closed-loop system bandwidth. Bandwidth enlargement implies shorter response time of the system, in terms of
both variations of the reference signal and recovery action of the feedback
system to output variations induced by the disturbance input.
The parameters of the adopted control structure should be chosen so as
to satisfy requirements on the system behaviour at steady state and during
the transient. Classical tools to determine such parameters are the root locus
in the domain of the complex variable s or the Nichols chart in the domain
of the real angular frequency ω. The two tools are conceptually equivalent.
Their potential is diﬀerent in that root locus allows a control law to be found
which assigns the exact parameters of the closed-loop system time response,
whereas Nichols chart allows a controller to be speciﬁed which confers good
transient and steady-state behaviour to the system response.
A feedback system with strict requirements on the steady-state and transient behaviour, typically, has a response that can be assimilated to that of a
second-order system. In fact, even for closed-loop functions of greater order,
it is possible to identify a pair of complex conjugate poles whose real part
absolute value is smaller than the real part absolute values of the other poles.
Such a pair of poles is dominant in that its contribution to the transient response prevails over that of the other poles. It is then possible to approximate
the input/output relationship with the transfer function
W (s) =

kW
2ζs
s2
1+
+ 2
ωn
ωn

(C.7)

which has to be realized by a proper choice of the controller. Regarding
the values to assign to the parameters characterizing the transfer function
in (C.7), the following remarks are in order. The constant kW represents the
input/output steady-state gain, which is equal to 1/H0 if C(s)G(s)H0 has at
least a pole at the origin. The natural frequency ωn is the modulus of the
complex conjugate poles, whose real part is given by −ζωn where ζ is the
damping ratio of the pair of poles.
The inﬂuence of parameters ζ and ωn on the closed-loop frequency response can be evaluated in terms of the resonance peak magnitude
Mr =

1
2ζ

1 − ζ2

,

occurring at the resonant frequency
ωr = ωn

1 − 2ζ 2 ,

C Feedback Control

593

Fig. C.2. Feedback control structure with feedforward compensation

and of the 3 dB bandwidth
ω3 = ωn


1 − 2ζ 2 +

2 − 4ζ 2 + 4ζ 4 .

A step input is typically used to characterize the transient response in the
time domain. The inﬂuence of parameters ζ and ωn on the step response can
be evaluated in terms of the percentage of overshoot
s% = 100 exp(−πζ/ 1 − ζ 2 ),
of the rise time

1.8
ωn

tr ≈
and of the settling time within 1%
ts =

4.6
.
ζωn

The adoption of a feedforward compensation action represents a feasible
solution both for tracking a time-varying reference input and for enhancing
rejection of the eﬀects of a disturbance on the output. Consider the general
scheme in Fig. C.2. Let R(s) denote a given input reference and Dc (s) denote a computed estimate of the disturbance D(s); the introduction of the
feedforward action yields the input/output relationship


C(s)G(s)
F (s)G(s)
+
Y (s) =
R(s)
(C.8)
1 + C(s)G(s)H(s) 1 + C(s)G(s)H(s)

G(s)
D(s) − Dc (s) .
+
1 + C(s)G(s)H(s)
By assuming that the desired output is related to the reference input by a
constant factor Kd and regarding the transducer as an instantaneous system
(H(s) ≈ H0 = 1/Kd ) for the current operating conditions, the choice
F (s) =

Kd
G(s)

(C.9)

594

C Feedback Control

Fig. C.3. Feedback control structure with inverse model technique

yields the input/output relationship
Y (s) = Yd (s) +


G(s)
D(s) − Dc (s) .
1 + C(s)G(s)H0

(C.10)

If |C(jω)G(jω)H0 |
1, the eﬀect of the disturbance on the output is further
reduced by means of an accurate estimate of the disturbance.
Feedforward compensation technique may lead to a solution, termed inverse model control , illustrated in the scheme of Fig. C.3. It should be remarked, however, that such a solution is based on dynamics cancellation,
and thus it can be employed only for a minimum-phase system, i.e., a system
whose poles and zeros have all strictly negative real parts. Further, one should
consider physical realizability issues as well as eﬀects of parameter variations
which prevent perfect cancellation.

C.2 Control of Nonlinear Mechanical Systems
If the system to control does not satisfy the linearity property, the control
design problem becomes more complex. The fact that a system is qualiﬁed
as nonlinear , whenever linearity does not hold, leads to understanding how
it is not possible to resort to general techniques for control design, but it is
necessary to face the problem for each class of nonlinear systems which can
be deﬁned through imposition of special properties.
On the above premise, the control design problem of nonlinear systems
described by the dynamic model
H(x)ẍ + h(x, ẋ) = u

(C.11)

is considered, where [ xT ẋT ]T denotes the (2n × 1) state vector of the
system, u is the (n × 1) input vector, H(x) is an (n × n) positive deﬁnite
(and thus invertible) matrix depending on x, and h(x, ẋ) is an (n × 1) vector
depending on state. Several mechanical systems can be reduced to this class,
including manipulators with rigid links and joints.
The control law can be found through a nonlinear compensating action
obtained by choosing the following nonlinear state feedback law (inverse dynamics control):
)
5
u = H(x)v
+ h(x,
ẋ)
(C.12)

C Feedback Control

595

)
5
where H(x)
and h(x)
respectively denote the estimates of the terms H(x)
and h(x), computed on the basis of measures on the system state, and v is a
new control input to be deﬁned later. In general, it is
5
H(x)
= H(x) + ΔH(x)
)
h(x,
ẋ) = h(x, ẋ) + Δh(x, ẋ)

(C.13)
(C.14)

because of the unavoidable modelling approximations or as a consequence of
an intentional simpliﬁcation in the compensating action. Substituting (C.12)
into (C.11) and accounting for (C.13), (C.14) yields
ẍ = v + z(x, ẋ, v)
where

(C.15)


z(x, ẋ, v) = H −1 (x) ΔH(x)v + Δh(x, ẋ) .

If tracking of a trajectory (xd (t), ẋd (t), ẍd (t)) is desired, the tracking error
can be deﬁned as


xd − x
e=
(C.16)
ẋd − ẋ
and it is necessary to derive the error dynamics equation to study convergence
of the actual state to the desired one. To this end, the choice
v = ẍd + w(e),

(C.17)

substituted into (C.15), leads to the error equation
ė = F e − Gw(e) − Gz(e, xd , ẋd , ẍd ),

(C.18)

where the (2n × 2n) and (2n × n) matrices, respectively,


 
O I
O
F =
G=
O O
I
follow from the error deﬁnition in (C.16). Control law design consists of ﬁnding
the error function w(e) which makes (C.18) globally asymptotically stable,1
i.e.,
lim e(t) = 0.
t→∞

In the case of perfect nonlinear compensation (z(·) = 0), the simplest choice
of the control action is the linear one
w(e) = −K P (xd − x) − K D (ẋd − ẋ)
= [ −K P −K D ] e,
1

(C.19)

Global asymptotic stability is invoked to remark that the equilibrium state is
asymptotically stable for any perturbation.

596

C Feedback Control

where asymptotic stability of the error equation is ensured by choosing positive
deﬁnite matrices K P and K D . The error transient behaviour is determined
by the eigenvalues of the matrix


O
I
A=
(C.20)
−K P −K D
characterizing the error dynamics
ė = Ae.

(C.21)

If compensation is imperfect, then z(·) cannot be neglected and the error
equation in (C.18) takes on the general form
ė = f (e).

(C.22)

It may be worth choosing the control law w(e) as the sum of a nonlinear term
and a linear term of the kind in (C.19); in this case, the error equation can
be written as
ė = Ae + k(e),
(C.23)
where A is given by (C.20) and k(e) is available to make the system globally
asymptotically stable. The equations in (C.22), (C.23) express nonlinear differential equations of the error. To test for stability and obtain advise on the
choice of suitable control actions, one may resort to Lyapunov direct method
illustrated below.

C.3 Lyapunov Direct Method
The philosophy of the Lyapunov direct method is the same as that of most
methods used in control engineering to study stability, namely, testing for
stability without solving the diﬀerential equations describing the dynamic
system.
This method can be presented in short on the basis of the following reasoning. If it is possible to associate an energy-based description with a (linear
or nonlinear) autonomous dynamic system and, for each system state with the
exception of the equilibrium state, the time rate of such energy is negative,
then energy decreases along any system trajectory until it attains its minimum at the equilibrium state; this argument justiﬁes an intuitive concept of
stability.
With reference to (C.22), by setting f (0) = 0, the equilibrium state is
e = 0. A scalar function V (e) of the system state, continuous together with
its ﬁrst derivative, is deﬁned a Lyapunov function if the following properties
hold:
V (e) > 0

∀e = 0

C Feedback Control

V (e) = 0
V̇ (e) < 0

e=0
∀e = 0

V (e) → ∞

e → ∞.

597

The existence of such a function ensures global asymptotic stability of the equilibrium e = 0. In practice, the equilibrium e = 0 is globally asymptotically
stable if a positive deﬁnite, radially unbounded function V (e) is found so that
its time derivative along the system trajectories is negative deﬁnite.
If positive deﬁniteness of V (e) is realized by the adoption of a quadratic
form, i.e.,
(C.24)
V (e) = eT Qe
with Q a symmetric positive deﬁnite matrix, then in view of (C.22) it follows
V̇ (e) = 2eT Qf (e).

(C.25)

If f (e) is so as to render the function V̇ (e) negative deﬁnite, the function
V (e) is a Lyapunov function, since the choice (C.24) allows system global
asymptotic stability to be proved. If V̇ (e) in (C.25) is not negative deﬁnite
for the given V (e), nothing can be inferred on the stability of the system,
since the Lyapunov method gives only a suﬃcient condition. In such cases
one should resort to diﬀerent choices of V (e) in order to ﬁnd, if possible, a
negative deﬁnite V̇ (e).
In the case when the property of negative deﬁniteness does not hold, but
V̇ (e) is only negative semi-deﬁnite
V̇ (e) ≤ 0,
global asymptotic stability of the equilibrium state is ensured if the only system trajectory for which V̇ (e) is identically null (V̇ (e) ≡ 0) is the equilibrium
trajectory e ≡ 0 (a consequence of La Salle theorem).
Finally, consider the stability problem of the nonlinear system in the
form (C.23); under the assumption that k(0) = 0, it is easy to verify that
e = 0 is an equilibrium state for the system. The choice of a Lyapunov function candidate as in (C.24) leads to the following expression for its derivative:
V̇ (e) = eT (AT Q + QA)e + 2eT Qk(e).

(C.26)

AT Q + QA = −P ,

(C.27)

By setting
the expression in (C.26) becomes
V̇ (e) = −eT P e + 2eT Qk(e).

(C.28)

The matrix equation in (C.27) is said to be a Lyapunov equation; for any
choice of a symmetric positive deﬁnite matrix P , the solution matrix Q exists

598

C Feedback Control

and is symmetric positive deﬁnite if and only if the eigenvalues of A have
all negative real parts. Since matrix A in (C.20) veriﬁes such condition, it
is always possible to assign a positive deﬁnite matrix P and ﬁnd a positive
deﬁnite matrix solution Q to (C.27). It follows that the ﬁrst term on the
right-hand side of (C.28) is negative deﬁnite and the stability problem is
reduced to searching a control law so that k(e) renders the total V̇ (e) negative
(semi-)deﬁnite.
It should be underlined that La Salle theorem does not hold for timevarying systems (also termed non-autonomous) in the form
ė = f (e, t).
In this case, a conceptually analogous result which might be useful is the
following, typically referred to as Barbalat lemma — of which it is indeed a
consequence. Given a scalar function V (e, t) so that
1. V (e, t) is lower bounded
2. V̇ (e, t) ≤ 0
3. V̇ (e, t) is uniformly continuous
then it is lim t→∞ V̇ (e, t) = 0. Conditions 1 and 2 imply that V (e, t) has a
bounded limit for t → ∞. Since it is not easy to verify the property of uniform
continuity from the deﬁnition, Condition 3 is usually replaced by
3’. V̈ (e, t) is bounded
which is suﬃcient to guarantee validity of Condition 3. Barbalat lemma can
obviously be used for time-invariant (autonomous) dynamic systems as an
alternative to La Salle theorem, with respect to which some conditions are
relaxed; in particular, V (e) needs not necessarily be positive deﬁnite.

Bibliography
Linear systems analysis can be found in classical texts such as [61]. For the
control of these systems see [82, 171]. For the analysis of nonlinear systems
see [109]. Control of nonlinear mechanical systems is dealt with in [215].

D
Diﬀerential Geometry

The analysis of mechanical systems subject to nonholonomic constraints, such
as wheeled mobile robots, requires some basic concepts of diﬀerential geometry
and nonlinear controllability theory, that are brieﬂy recalled in this appendix.

D.1 Vector Fields and Lie Brackets
For simplicity, the case of vectors x ∈ IRn is considered. The tangent space
at x (intuitively, the space of velocities of trajectories passing through x) is
hence denoted by Tx (IRn ). The presented notions are however valid in the
more general case in which a diﬀerentiable manifold (i.e., a space that is
locally diﬀeomorphic to IRn ) is considered in place of a Euclidean space.
A vector ﬁeld g : IRn → Tx (IRn ) is a mapping that assigns to each point
x ∈ IRn a tangent vector g(x) ∈ Tx (IRn ). In the following it is always assumed
that vector ﬁelds are smooth, i.e., such that the associated mappings are of
class C ∞ .
If the vector ﬁeld g(x) is used to deﬁne a diﬀerential equation as in
ẋ = g(x),

(D.1)

g

the ﬂow φt (x) of g is the mapping that associates to each point x the value
at time t of the solution of (D.1) evolving from x at time 0, or
d g
g
φ (x) = g(φt (x)).
dt t

(D.2)

The family of mappings {φgt } is a one-parameter (i.e., t) group under the
composition operator
φgt1 ◦ φgt2 = φgt1 +t2 .
For example, for time-invariant linear systems it is g(x) = Ax and the ﬂow
is the linear operator φgt = eAt .

600

D Diﬀerential Geometry

?n

¦  /1

 1 > /d /1 H
/d
 /d

?1
/1

¦  /d

 /1

?d
Fig. D.1. The net displacement of system (D.4) under the input sequence (D.5) is
directed as the Lie bracket of the two vector ﬁelds g 1 and g 2

Given two vector ﬁelds g 1 and g 2 , the composition of their ﬂows is noncommutative in general:
g
g
g
g
φt 1 ◦ φs 2 = φs 2 ◦ φt 1 .

The vector ﬁeld [g 1 , g 2 ] deﬁned as
∂g 2
∂g 1
g 1 (x) −
g (x)
(D.3)
∂x
∂x 2
is called Lie bracket of g 1 and g 2 . The two vector ﬁeld g 1 and g 2 commute if
[g 1 , g 2 ] = 0.
The Lie bracket operation has an interesting interpretation. Consider the
driftless dynamic system
[g 1 , g 2 ](x) =

ẋ = g 1 (x)u1 + g 2 (x)u2

(D.4)

associated with the vector ﬁelds g 1 and g 2 . If the inputs u1 and u2 are never
active simultaneously, the solution of the diﬀerential equation (D.4) can be
obtained by composing the ﬂows of g 1 and g 2 . In particular, consider the
following input sequence:
⎧
u (t) = +1, u2 (t) = 0 t ∈ [0, ε)
⎪
⎨ 1
u1 (t) = 0, u2 (t) = +1 t ∈ [ε, 2ε)
u(t) =
(D.5)
u
⎪
⎩ 1 (t) = −1, u2 (t) = 0 t ∈ [2ε, 3ε)
u1 (t) = 0, u2 (t) = −1 t ∈ [3ε, 4ε),
where ε is an inﬁnitesimal time interval. The solution of (D.4) at time t = 4ε
can be obtained by following ﬁrst the ﬂow of g 1 , then of g 2 , then of −g 1 , and
ﬁnally of −g 2 (see Fig. D.1). By computing x(ε) through a series expansion
at x0 = x(0) along g 1 , then x(2ε) as a series expansion at x(ε) along g 2 , and
so on, one obtains
−g 2

−g

g

g

◦ φε 1 ◦ φε 2 ◦ φε 1 (x0 )


∂g 2
∂g 1
= x0 + ε2
g 1 (x0 ) −
g 2 (x0 ) + O(ε3 ).
∂x
∂x

x(4ε) = φε

D Diﬀerential Geometry

601

If g 1 and g 2 commute, the net displacement resulting from the input sequence (D.5) is zero.
The above expression shows that, at each point x, inﬁnitesimal motion
of the driftless system (D.4) is possible not only in the directions belonging
to the linear span of g 1 (x) and g 2 (x), but also in the direction of their Lie
bracket [g 1 , g 2 ](x). It can be proven that more complicated input sequences
can be used to generate motion in the direction of higher-order Lie brackets,
such as [g 1 , [g 1 , g 2 ]].
Similar constructive procedures can be given for systems with a drift 1
vector ﬁeld, such as the following:
ẋ = f (x) + g 1 (x)u1 + g 2 (x)u2 .

(D.6)

Using appropriate input sequences, it is possible to generate motion in the
direction of Lie brackets involving the vector ﬁeld f as well as g j , j = 1, 2.

Example D.1
For a single-input linear system
ẋ = A x + b u,
the drift and input vector ﬁelds are f (x) = Ax and g(x) = b, respectively. The
following Lie brackets:
−[f , g] = Ab
[f , [f , g]] = A2 b
− [f , [f , [f , g]]] = A3 b
..
.
represent well-known directions in which it is possible to move the system.

The Lie derivative of the scalar function α : IRn → IR along vector ﬁeld g
is deﬁned as
∂α
g(x).
(D.7)
Lg α(x) =
∂x
The following properties of Lie brackets are useful in computation:
[f , g] = −[g, f ]
[f , [g, h]] + [h, [f , g]] + [g, [h, f ]] = 0
[αf , βg] = αβ[f , g] + α(Lf β)g − β(Lg α)f
1

(skew-symmetry)
(Jacobi identity)
(chain rule)

This term emphasizes how the presence of f will in general force the system to
move (ẋ = 0) even in the absence of inputs.

602

D Diﬀerential Geometry

with α, β: IRn → IR. The vector space V(IRn ) of smooth vector ﬁelds on IRn ,
equipped with the Lie bracket operation, is a Lie algebra.
The distribution Δ associated with the m vector ﬁelds {g 1 , . . . , g m } is the
mapping that assigns to each point x ∈ IRn the subspace of Tx (IRn ) deﬁned
as
(D.8)
Δ(x) = span{g 1 (x), . . . , g m (x)}.
Often, a shorthand notation is used:
Δ = span{g 1 , . . . , g m }.
The distribution Δ is nonsingular if dim Δ(x) = r, with r constant for all
x. In this case, r is called the dimension of the distribution. Moreover, Δ is
called involutive if it is closed under the Lie bracket operation:
[g i , g j ] ∈ Δ

∀ g i , g j ∈ Δ.

The involutive closure Δ̄ of a distribution Δ is its closure under the Lie bracket
operation. Hence, Δ is involutive if and only if Δ̄ = Δ. Note that the distribution Δ = span{g} associated with a single vector ﬁeld is always involutive,
because [g, g](x) = 0.

Example D.2
The distribution


Δ = span{g 1 , g 2 } = span

cos x3
sin x3
0

 & '6
,

0
0
1

is nonsingular and has dimension 2. It is not involutive, because the Lie bracket


[g 1 , g 2 ](x) =

sin x3
−cos x3
0



is always linearly independent of g 1 (x) and g 2 (x). Its involutive closure is therefore
Δ̄ = span{g 1 , g 2 , [g 1 , g 2 ]}.

D Diﬀerential Geometry

603

D.2 Nonlinear Controllability
Consider a nonlinear dynamic system of the form
ẋ = f (x) +

m


g j (x)uj ,

(D.9)

j=1

that is called aﬃne in the inputs uj . The state x takes values in IRn , while
each component uj of the control input u ∈ IRm takes values in the class U
of piecewise-constant functions.
Denote by x(t, 0, x0 , u) the solution of (D.9) at time t ≥ 0, corresponding
to an input u: [0, t] → U and an initial condition x(0) = x0 . Such a solution
exists and is unique provided that the drift vector ﬁeld f and the input vector
ﬁelds g j are of class C ∞ . System (D.9) is said to be controllable if, for any
choice of x1 , x2 in IRn , there exists a time instant T and an input u: [0, T ] → U
such that x(T, 0, x1 , u) = x2 .
The accessibility algebra A of system (D.9) is the smallest subalgebra of
V(IRn ) that contains f , g 1 , . . . , g m . By deﬁnition, all the Lie brackets that can
be generated using these vector ﬁelds belong to A. The accessibility distribution ΔA of system (D.9) is deﬁned as
ΔA = span{v|v ∈ A}.

(D.10)

In other words, ΔA is the involutive closure of Δ = span{f , g 1 , . . . , g m }.
The computation of ΔA may be organized as an iterative procedure
ΔA = span {v|v ∈ Δi , ∀i ≥ 1} ,
with
Δ1 = Δ = span{f , g 1 , . . . , g m }
Δi = Δi−1 + span{[g, v]| g ∈ Δ1 , v ∈ Δi−1 },

i ≥ 2.

This procedure stops after κ steps, where κ is the smallest integer such that
Δκ+1 = Δκ = ΔA . This number is called the nonholonomy degree of the
system and is related to the ‘level’ of Lie brackets that must be included in
ΔA . Since dim ΔA ≤ n, it is κ ≤ n − m necessarily.
If system (D.9) is driftless
ẋ =

m


g i (x)ui ,

(D.11)

i=1

the accessibility distribution ΔA associated with vector ﬁelds g 1 , . . . , g m characterizes its controllability. In particular, system (D.11) is controllable if and
only if the following accessibility rank condition holds:
dim ΔA (x) = n.

(D.12)

604

D Diﬀerential Geometry

Note that for driftless systems the iterative procedure for building ΔA starts
with Δ1 = Δ = span{g 1 , . . . , g m }, and therefore κ ≤ n − m + 1.
For systems in the general form (D.9), condition (D.12) is only necessary
for controllability. There are, however, two notable exceptions:
• If system (D.11) is controllable, the system with drift obtained by performing a dynamic extension of (D.11)
ẋ =

m


g i (x)vi

(D.13)

i=1

v̇i = ui ,

i = 1, . . . , m,

(D.14)

i.e., by adding an integrator on each input channel, is also controllable.
• For a linear system
ẋ = Ax +

m


bj uj = Ax + Bu

j=1

(D.12) becomes
" ([ B

AB

A2 B

...

An−1 B ]) = n,

(D.15)

i.e., the well-known necessary and suﬃcient condition for controllability
due to Kalman.

Bibliography
The concepts brieﬂy recalled in this appendix can be studied in detail in
various tests of diﬀerential geometry [94, 20] and nonlinear control theory [104,
168, 195].

E
Graph Search Algorithms

This appendix summarizes some basic concepts on algorithm complexity and
graph search techniques that are useful in the study of motion planning.

E.1 Complexity
A major criterion for assessing the eﬃciency of an algorithm A is its running
time, i.e., the time needed for executing the algorithm in a computational
model capturing the most relevant characteristics of an actual elaboration
system. In practice, one is interested in estimating the running time as a
function of a single parameter n characterizing the size of the input within a
speciﬁc class of instances of the problem. In motion planning, this parameter
may be the dimension of the conﬁguration space, or the number of vertices of
the free conﬁguration space (if it is a polygonal subset).
In worst-case analysis, t(n) denotes the maximum running time of A in
correspondence of input instances of size n. Other kinds of analyses (e.g.,
average-case) are possible but they are less critical or general, requiring a
statistical knowledge of the input distribution that may not be available.
The exact functional expression of t(n) depends on the implementation of
the algorithm, and is of little practical interest because the running time in
the adopted computational model is only an approximation of the actual one.
More signiﬁcant is the asymptotic behaviour of t(n), i.e., the rate of growth
of t(n) with n. Denote by O(f (n)) the set of real functions g(n) such that
c1 f (n) ≤ g(n) ≤ c2 f (n)

∀ n ≥ n0 ,

with c1 , c2 and n0 positive constants. If the worst-case running time of A is
O(f (n)), i.e., if t(n) ∈ O(f (n)), the time complexity of A is said to be O(f (n)).
A very important class is represented by algorithms whose worst-case running time is asymptotically polynomial in the size of the input. In particular,
if t(n) ∈ O(np ), for some p ≥ 0, the algorithm is said to have polynomial time

606

E Graph Search Algorithms

complexity. If the asymptotic behaviour of the worst-case running time is not
polynomial, the time complexity of the algorithm is exponential . Note that
here ‘exponential’ actually means ‘not bounded by any polynomial function’.
The asymptotic behaviour of an algorithm with exponential time complexity is such that in the worst case it can only be applied to problems of ‘small’
size. However, there exist algorithms of exponential complexity that are very
eﬃcient on average, i.e., for the most frequent classes of input. A well known
example is the simplex algorithm for solving linear programming problems.
Similarly, there are algorithms with polynomial time complexity which are
ineﬃcient in practice because c1 , c2 or p are ‘large’.
The above concepts can be extended to inputs whose size is characterized
by more than one parameter, or to performance criteria diﬀerent from running
time. For example, the memory space required by an algorithm is another
important measure. The space complexity of an algorithm is said to be O(f (n))
if the memory space required for its execution is a function in O(f (n)).

E.2 Breadth-ﬁrst and Depth-ﬁrst Search
Let G = (N, A) be a graph consisting of a set N of nodes and a set A of arcs,
with cardinality n and a respectively. It is assumed that G is represented
by an adjacency list: to each node Ni is associated a list of nodes that are
connected to Ni by an arc. Consider the problem of searching G to ﬁnd a path
from a start node Ns to a goal node Ng . The simplest graph search strategies
are breadth-ﬁrst search (BFS) and depth-ﬁrst search (DFS). These are brieﬂy
described in the following with reference to an iterative implementation.
Breadth-ﬁrst search makes use of a queue — i.e., a FIFO (First In First
Out) data structure — of nodes called OPEN. Initially, OPEN contains only
the start node Ns , which is marked visited . All the other nodes in G are marked
unvisited . At each iteration the ﬁrst node in OPEN is extracted, and all its
unvisited adjacent nodes are marked visited and inserted in OPEN. The search
terminates when either Ng is inserted in OPEN or OPEN is empty (failure).
During the search, the algorithm maintains the BFS tree, which contains only
those arcs that have led to discovering unvisited nodes. This tree contains one
and only one path connecting the start node to each visited node, and hence
also a solution path from Ns to Ng , if it exists.
In depth-ﬁrst search, OPEN is a stack , i.e., a LIFO (Last In First Out)
data structure. Like in the breadth-ﬁrst case, it contains initially only the
start node Ns marked visited . When a node Nj is inserted in OPEN, the node
Ni which has determined its insertion is memorized. At each iteration, the
ﬁrst node in OPEN is extracted. If it is unvisited , it is marked visited and
the arc connecting Ni to Nj is inserted in the DFS tree. All unvisited nodes
that are adjacent to Nj are inserted in OPEN. The search terminates when
either Ng is inserted in OPEN or OPEN is empty (failure). Like in the BFS,
the DFS tree contains the solution path from Ns to Ng , if it exists.

E Graph Search Algorithms

607

Both breadth-ﬁrst and depth-ﬁrst search have time complexity O(a). Note
that BFS and DFS are actually traversal strategies, because they do not use
any information about the goal node; the graph is simply traversed until Ng
is marked visited . Both the algorithms are complete, i.e., they ﬁnd a solution
path if it exists and report failure otherwise.

E.3 A Algorithm
In many applications, the arcs of G are labelled with positive numbers called
weights. As a consequence, one may deﬁne the cost of a path on G as the
sum of the weights of its arcs. Consider the problem of connecting Ns to
Ng on G through a path of minimum cost, simply called minimum path. In
motion planning problems, for example, the nodes generally represent points
in conﬁguration space, and it is then natural to deﬁne the weight of an arc
as the length of the path that it represents. The minimum path is obviously
interesting because it is the shortest among those joining Ns to Ng on G.
A widely used strategy for determining the minimum path on a graph is
the A algorithm. A visits the nodes of G iteratively starting from Ns , storing
only the current minimum paths from Ns to the visited nodes in a tree T .
The algorithm employs a cost function f (Ni ) for each node Ni visited during
the search. This function, which is an estimate of the cost of the minimum
path that connects Ns to Ng passing through Ni , is computed as
f (Ni ) = g(Ni ) + h(Ni ),
where g(Ni ) is the cost of the path from Ns to Ni as stored in the current
tree T , and h(Ni ) is a heuristic estimate of the cost h (Ni ) of the minimum
path between Ni and Ng . While the value of g(Ni ) is uniquely determined by
the search, any choice of h(·) such that
∀Ni ∈ N : 0 ≤ h(Ni ) ≤ h (Ni )

(E.1)

is admissible. Condition (E.1) means that h(·) must not ‘overestimate’ the
cost of the minimum path from Ni to Ng .
In the following, a pseudocode description of A is given. For its understanding, some preliminary remarks are needed:
•
•
•
•

all the nodes are initially unvisited , except Ns which is visited ;
at the beginning, T contains only Ns ;
OPEN is a list of nodes that initially contains only Ns ;
Nbest is the node in OPEN with the minimum value of f (in particular, it
is the ﬁrst node if OPEN is sorted by increasing values of f );
• ADJ(Ni ) is the adjacency list of Ni ;
• c(Ni , Nj ) is the weight of the arc connecting Ni to Nj .

608

E Graph Search Algorithms

A algorithm
1
repeat
2
ﬁnd and extract Nbest from OPEN
3
if Nbest = Ng then exit
4
for each node Ni in ADJ(Nbest ) do
5
if Ni is unvisited then
6
add Ni to T with a pointer toward Nbest
7
insert Ni in OPEN; mark Ni visited
8
else if g(Nbest ) + c(Nbest , Ni ) < g(Ni ) then
9
redirect the pointer of Ni in T toward Nbest
10
if Ni is not in OPEN then
10
insert Ni in OPEN
10
else update f (Ni )
10
end if
11
end if
12 until OPEN is empty
Under condition (E.1), the A algorithm is complete. In particular, if the
algorithm terminates with an empty OPEN, there exists no path in G from
Ns to Ng (failure); otherwise, the tree T contains the minimum path from Ns
to Ng , which can be reconstructed by backtracking from Ng to Ns .
The A algorithm with the particular (admissible) choice h(Ni ) = 0, for
each node Ni , is equivalent to the Dijkstra algorithm. If the nodes in G represent points in a Euclidean space, an admissible heuristic is the Euclidean
distance between N and Ng . In fact, the length of the minimum path between
Ni and Ng is bounded below by the Euclidean distance.
The extraction of a node from OPEN and the visit of its adjacent nodes
is called node expansion. Given two admissible heuristic functions h1 and h2
such that h2 (Ni ) ≥ h1 (Ni ), for each node Ni in G, it is possible to prove
that each node in G expanded by A using h2 is also expanded using h1 . This
means that A equipped with the heuristic h2 is at least as eﬃcient as A
equipped with the heuristic h1 ; h2 is said to be more informed than h1 .
The A algorithm can be implemented with time complexity O(a log n).

Bibliography
The notions brieﬂy recalled in this appendix are explained in detail in various
texts on algorithm theory and artiﬁcial intelligence, such as [51, 189, 202].

References

1. C. Abdallah, D. Dawson, P. Dorato, M. Jamshidi, “Survey of robust control
for rigid robots,” IEEE Control Systems Magazine, vol. 11, no. 2, pp. 24–30,
1991.
2. M. Aicardi, G. Casalino, A. Bicchi, A. Balestrino, “Closed loop steering of
unicycle-like vehicles via Lyapunov techniques,” IEEE Robotics and Automation Magazine, vol. 2, no. 1, pp. 27–35, 1995.
3. J.S. Albus, H.G. McCain, R. Lumia, NASA/NBS Standard Reference Model
for Telerobot Control System Architecture (NASREM), NBS tech. note 1235,
Gaithersburg, MD, 1987.
4. C.H. An, C.G. Atkeson, J.M. Hollerbach, Model-Based Control of a Robot Manipulator , MIT Press, Cambridge, MA, 1988.
5. R.J. Anderson, M.W. Spong, “Hybrid impedance control of robotic manipulators,” IEEE Journal of Robotics and Automation, vol. 4, pp. 549–556, 1988.
6. J. Angeles, Spatial Kinematic Chains: Analysis, Synthesis, Optimization,
Springer-Verlag, Berlin, 1982.
7. S. Arimoto, F. Miyazaki, “Stability and robustness of PID feedback control
for robot manipulators of sensory capability,” in Robotics Research: The First
International Symposium, M. Brady, R. Paul (Eds.), MIT Press, Cambridge,
MA, pp. 783–799, 1984.
8. R.C. Arkin, Behavior-Based Robotics, MIT Press, Cambridge, MA, 1998.
9. B. Armstrong-Hélouvry, Control of Machines with Friction, Kluwer, Boston,
MA, 1991.
10. H. Asada, J.-J.E. Slotine, Robot Analysis and Control , Wiley, New York, 1986.
11. H. Asada, K. Youcef-Toumi, “Analysis and design of a direct-drive arm with a
ﬁve-bar-link parallel drive mechanism,” ASME Journal of Dynamic Systems,
Measurement, and Control , vol. 106, pp. 225–230, 1984.
12. H. Asada, K. Youcef-Toumi, Direct-Drive Robots, MIT Press, Cambridge, MA,
1987.
13. C.G. Atkeson, C.H. An, J.M. Hollerbach, ”Estimation of inertial parameters
of manipulator loads and links,” International Journal of Robotics Research,
vol. 5, no. 3, pp. 101–119, 1986.
14. J. Baillieul, “Kinematic programming alternatives for redundant manipulators,” Proc. 1985 IEEE International Conference on Robotics and Automation,
St. Louis, MO, pp. 722–728, 1985.

610

References

15. A. Balestrino, G. De Maria, L. Sciavicco, “An adaptive model following control
for robotic manipulators,” ASME Journal of Dynamic Systems, Measurement,
and Control , vol. 105, pp. 143–151, 1983.
16. A. Balestrino, G. De Maria, L. Sciavicco, B. Siciliano, “An algorithmic approach to coordinate transformation for robotic manipulators,” Advanced
Robotics, vol. 2, pp. 327–344, 1988.
17. J. Barraquand, J.-C. Latombe, “Robot motion planning: A distributed representation approach,” International Journal of Robotics Research, vol. 10,
pp. 628–649, 1991.
18. G. Bastin, G. Campion, B. D’Andréa-Novel, “Structural properties and classiﬁcation of kinematic and dynamic models of wheeled mobile robots,” IEEE
Transactions on Robotics and Automation, vol. 12, pp. 47–62, 1996.
19. A.K. Bejczy, Robot Arm Dynamics and Control , memo. TM 33-669, Jet Propulsion Laboratory, California Institute of Technology, 1974.
20. W.M. Boothby, An Introduction to Diﬀerentiable Manifolds and Riemannian
Geometry, Academic Press, Orlando, FL, 1986.
21. J. Borenstein, H.R. Everett, L. Feng, Navigating Mobile Robots: Systems and
Techniques, A K Peters, Wellesley, MA, 1996.
22. B.K.K. Bose, Modern Power Electronics and AC Drives, Prentice-Hall, Englewood Cliﬀs, NJ, 2001.
23. O. Bottema, B. Roth, Theoretical Kinematics, North Holland, Amsterdam,
1979.
24. T.L. Boullion, P.L. Odell, Generalized Inverse Matrices, Wiley, New York,
1971.
25. M. Brady, “Artiﬁcial intelligence and robotics,” Artiﬁcial Intelligence, vol. 26,
pp. 79–121, 1985.
26. M. Brady, J.M. Hollerbach, T.L. Johnson, T. Lozano-Pérez, M.T. Mason,
(Eds.), Robot Motion: Planning and Control, MIT Press, Cambridge, MA,
1982.
27. H. Bruyninckx, J. De Schutter, “Speciﬁcation of force-controlled actions in the
“task frame formalism” — A synthesis,” IEEE Transactions on Robotics and
Automation, vol. 12, pp. 581–589, 1996.
28. H. Bruyninckx, S. Dumey, S. Dutré, J. De Schutter, “Kinematic models for
model-based compliant motion in the presence of uncertainty,” International
Journal of Robotics Research, vol. 14, pp. 465–482, 1995.
29. F. Caccavale, P. Chiacchio, “Identiﬁcation of dynamic parameters and feedforward control for a conventional industrial manipulator,” Control Engineering
Practice, vol. 2, pp. 1039–1050, 1994.
30. F. Caccavale, C. Natale, B. Siciliano, L. Villani, “Resolved-acceleration control
of robot manipulators: A critical review with experiments,” Robotica, vol. 16,
pp. 565–573, 1998.
31. F. Caccavale, C. Natale, B. Siciliano, L. Villani, “Six-DOF impedance control based on angle/axis representations,” IEEE Transactions on Robotics and
Automation, vol. 15, pp. 289–300, 1999.
32. F. Caccavale, C. Natale, B. Siciliano, L. Villani, ”Robot impedance control
with nondiagonal stiﬀness,” IEEE Transactions on Automatic Control, vol. 44,
pp. 1943–1946, 1999.
33. J.F. Canny, The Complexity of Robot Motion Planning, MIT Press, Cambridge,
MA, 1988.

References

611

34. C. Canudas de Wit, H. Khennouf, C. Samson, O.J. Sørdalen, “Nonlinear control design for mobile robots,” in Recent Trends in Mobile Robots, Y.F. Zheng,
(Ed.), pp. 121–156, World Scientiﬁc Publisher, Singapore, 1993.
35. F. Chaumette, “Image moments: A general and useful set of features for visual
servoing,” IEEE Transactions on Robotics and Automation, vol. 21, pp. 11161127, 2005.
36. F. Chaumette, S. Hutchinson, “Visual servo control. Part I: Basic approaches,”
IEEE Robotics and Automation Magazine, vol. 13, no. 4, pp. 82–90, 2006.
37. P. Chiacchio, S. Chiaverini, L. Sciavicco, B. Siciliano, “Closed-loop inverse
kinematics schemes for constrained redundant manipulators with task space
augmentation and task priority strategy,” International Journal of Robotics
Research, vol. 10, pp. 410–425, 1991.
38. P. Chiacchio, S. Chiaverini, L. Sciavicco, B. Siciliano, “Inﬂuence of gravity
on the manipulability ellipsoid for robot arms,” ASME Journal of Dynamic
Systems, Measurement, and Control , vol. 114, pp. 723–727, 1992.
39. P. Chiacchio, F. Pierrot, L. Sciavicco, B. Siciliano, “Robust design of independent joint controllers with experimentation on a high-speed parallel robot,”
IEEE Transactions on Industrial Electronics, vol. 40, pp. 393–403, 1993.
40. S. Chiaverini, L. Sciavicco, “The parallel approach to force/position control of
robotic manipulators,” IEEE Transactions on Robotics and Automation, vol. 4,
pp. 361–373, 1993.
41. S. Chiaverini, B. Siciliano, “The unit quaternion: A useful tool for inverse
kinematics of robot manipulators,” Systems Analysis Modelling Simulation,
vol. 35, pp. 45–60, 1999.
42. S. Chiaverini, B. Siciliano, O. Egeland, “Review of the damped least-squares inverse kinematics with experiments on an industrial robot manipulator,” IEEE
Transactions on Control Systems Technology, vol. 2, pp. 123–134, 1994.
43. S. Chiaverini, B. Siciliano, L. Villani, “Force/position regulation of compliant robot manipulators,” IEEE Transactions on Automatic Control, vol. 39,
pp. 647–652, 1994.
44. S.L. Chiu, “Task compatibility of manipulator postures,” International Journal
of Robotics Research, vol. 7, no. 5, pp. 13–21, 1988.
45. H. Choset, K.M. Lynch, S. Hutchinson, G. Kantor, W. Burgard, L.E. Kavraki,
S. Thrun, Principles of Robot Motion: Theory, Algorithms, and Implementations, MIT Press, Cambridge, MA, 2005.
46. J.C.K. Chou, “Quaternion kinematic and dynamic diﬀerential equations. IEEE
Transactions on Robotics and Automation, vol. 8, pp. 53–64, 1992.
47. A.I. Comport, E. Marchand, M. Pressigout, F. Chaumette, “Real-time markerless tracking for augmented reality: The virtual visual servoing framework,”
IEEE Transactions on Visualization and Computer Graphics, vol. 12, pp. 615–
628, 2006.
48. P.I. Corke, Visual Control of Robots: High-Performance Visual Servoing, Research Studies Press, Taunton, UK, 1996.
49. P. Corke, S. Hutchinson, “A new partitioned approach to image-based visual servo control,” IEEE Transactions on Robotics and Automation, vol. 17,
pp. 507-515, 2001.
50. M. Corless, G. Leitmann, “Continuous state feedback guaranteeing uniform
ultimate boundedness for uncertain dynamic systems,” IEEE Transactions on
Automatic Control , vol. 26, pp. 1139–1144, 1981.

612

References

51. T.H. Cormen, C.E. Leiserson, R.L. Rivest, C. Stein, Introduction to Algorithms,
2nd ed., MIT Press, Cambridge, MA, 2001.
52. J.J. Craig, Adaptive Control of Mechanical Manipulators, Addison-Wesley,
Reading, MA, 1988.
53. J.J. Craig, Introduction to Robotics: Mechanics and Control, 3rd ed., Pearson
Prentice Hall, Upper Saddle River, NJ, 2004.
54. C. De Boor, A Practical Guide to Splines, Springer-Verlag, New York, 1978.
55. T.L. De Fazio, D.S. Seltzer, D.E. Whitney, “The instrumented Remote Center
of Compliance,” Industrial Robot, vol. 11, pp. 238–242, 1984.
56. A. De Luca, A Spline Generator for Robot Arms, tech. rep. RAL 68, Rensselaer Polytechnic Institute, Department of Electrical, Computer, and Systems
Engineering, 1986.
57. A. De Luca, C. Manes, “Modeling robots in contact with a dynamic environment,” IEEE Transactions on Robotics and Automation, vol. 10, pp. 542–548,
1994.
58. A. De Luca, G. Oriolo, C. Samson, “Feedback control of a nonholonomic carlike robot,” in Robot Motion Planning and Control , J.-P. Laumond, (Ed.),
Springer-Verlag, Berlin, Germany, 1998.
59. A. De Luca, G. Oriolo, B. Siciliano, “Robot redundancy resolution at the acceleration level,” Laboratory Robotics and Automation, vol. 4, pp. 97–106, 1992.
60. J. Denavit, R.S. Hartenberg, “A kinematic notation for lower-pair mechanisms
based on matrices,” ASME Journal of Applied Mechanics, vol. 22, pp. 215–221,
1955.
61. P.M. DeRusso, R.J. Roy, C.M. Close, A.A. Desrochers, State Variables for
Engineers, 2nd ed., Wiley, New York, 1998.
62. J. De Schutter, H. Bruyninckx, S. Dutré, J. De Geeter, J. Katupitiya, S. Demey, T. Lefebvre, “Estimating ﬁrst-order geometric parameters and monitoring
contact transitions during force-controlled compliant motions,” International
Journal of Robotics Research, vol. 18, pp. 1161–1184, 1999.
63. J. De Schutter, H. Bruyninckx, W.-H. Zhu, M.W. Spong, ”Force control: A
bird’s eye view,” in Control Problems in Robotics and Automation, B. Siciliano,
K.P. Valavanis, (Ed.), pp. 1–17, Springer-Verlag, London, UK, 1998.
64. J. De Schutter, H. Van Brussel, “Compliant robot motion I. A formalism
for specifying compliant motion tasks,” International Journal of Robotics Research, vol. 7, no. 4, pp. 3–17, 1988.
65. J. De Schutter, H. Van Brussel, “Compliant robot motion II. A control approach based on external control loops,” International Journal of Robotics
Research, vol. 7, no. 4, pp. 18–33, 1988.
66. K.L. Doty, C. Melchiorri, C. Bonivento, “A theory of generalized inverses applied to robotics,” International Journal of Robotics Research, vol. 12, pp. 1–19,
1993.
67. S. Dubowsky, D.T. DesForges, “The application of model referenced adaptive
control to robotic manipulators,” ASME Journal of Dynamic Systems, Measurement, and Control , vol. 101, pp. 193–200, 1979.
68. C. Edwards, L. Galloway, “A single-point calibration technique for a sixdegree–of–freedom articulated arm,” International Journal of Robotics Research, vol. 13, pp. 189–199, 1994.
69. O. Egeland, “Task-space tracking with redundant manipulators,” IEEE Journal of Robotics and Automation, vol. 3, pp. 471–475, 1987.

References

613

70. S.D. Eppinger, W.P. Seering, “Introduction to dynamic models for robot force
control,” IEEE Control Systems Magazine, vol. 7, no. 2, pp. 48–52, 1987.
71. B. Espiau, F. Chaumette, P. Rives, “A new approach to visual servoing in
robotics,” IEEE Transactions on Robotics and Automation, vol. 8, pp. 313–
326, 1992.
72. H.R. Everett, Sensors for Mobile Robots: Theory and Application, AK Peters,
Wellesley, MA, 1995.
73. G.E. Farin, Curves and Surfaces for CAGD: A Practical Guide, 5th ed., Morgan Kaufmann Publishers, San Francisco, CA, 2001.
74. E.D. Fasse, P.C. Breedveld, “Modelling of elastically coupled bodies: Parts I–
II”, ASME Journal of Dynamic Systems, Measurement, and Control , vol. 120,
pp. 496–506, 1998.
75. O. Faugeras, Three-Dimensional Computer Vision: A Geometric Viewpoint,
MIT Press, Boston, MA, 1993.
76. R. Featherstone, “Position and velocity transformations between robot endeﬀector coordinates and joint angles,” International Journal of Robotics Research, vol. 2, no. 2, pp. 35–45, 1983.
77. R. Featherstone, Robot Dynamics Algorithms, Kluwer, Boston, MA, 1987.
78. R. Featherstone, O. Khatib, “Load independence of the dynamically consistent
inverse of the Jacobian matrix,” International Journal of Robotics Research,
vol. 16, pp. 168–170, 1997.
79. J. Feddema, O. Mitchell, “Vision-guided servoing with feature-based trajectory
generation,” IEEE Transactions on Robotics and Automation, vol. 5, pp. 691–
700, 1989.
80. M. Fliess, J. Lévine, P. Martin, P. Rouchon, “Flatness and defect of nonlinear
systems: Introductory theory and examples,” International Journal of Control,
vol. 61, pp. 1327–1361, 1995.
81. J. Fraden, Handbook of Modern Sensors: Physics, Designs, and Applications,
Springer, New York, 2004.
82. G.F. Franklin, J.D. Powell, A. Emami-Naeini, Feedback Control of Dynamic
Systems, 5th ed., Prentice-Hall, Lebanon, IN, 2005.
83. E. Freund, “Fast nonlinear control with arbitrary pole-placement for industrial
robots and manipulators,” International Journal of Robotics Research, vol. 1,
no. 1, pp. 65–78, 1982.
84. L.-C. Fu, T.-L. Liao, “Globally stable robust tracking of nonlinear systems
using variable structure control with an application to a robotic manipulator,”
IEEE Transactions on Automatic Control, vol. 35, pp. 1345–1350, 1990.
85. M. Gautier, W. Khalil, “Direct calculation of minimum set of inertial parameters of serial robots,” IEEE Transactions on Robotics and Automation, vol. 6,
pp. 368–373, 1990.
86. A.A. Goldenberg, B. Benhabib, R.G. Fenton, “A complete generalized solution
to the inverse kinematics of robots,” IEEE Journal of Robotics and Automation, vol. 1, pp. 14–20, 1985.
87. H. Goldstein, C.P. Poole, J.L. Safko, Classical Mechanics, 3rd ed., AddisonWesley, Reading, MA, 2002.
88. G.H. Golub, C.F. Van Loan, Matrix Computations, 3rd ed., The Johns Hopkins
University Press, Baltimore, MD, 1996.
89. M.C. Good, L.M. Sweet, K.L. Strobel, “Dynamic models for control system
design of integrated robot and drive systems,” ASME Journal of Dynamic
Systems, Measurement, and Control , vol. 107, pp. 53–59, 1985.

614

References

90. D.M. Gorinevski, A.M. Formalsky, A.Yu. Schneider, Force Control of Robotics
Systems, CRC Press, Boca Raton, FL, 1997.
91. W.A. Gruver, B.I. Soroka, J.J. Craig, T.L. Turner, “Industrial robot programming languages: A comparative evaluation,” IEEE Transactions on Systems,
Man, and Cybernetics, vol. 14, pp. 565–570, 1984.
92. G. Hager, W. Chang, A. Morse, “Robot feedback control based on stereo vision: Towards calibration-free hand-eye coordination,” IEEE Control Systems
Magazine, vol. 15, no. 1, pp. 30–39, 1995.
93. R.M. Haralick, L.G. Shapiro, Computer and Robot Vision, vols. 1 & 2, AddisonWesley, Reading, MA, 1993.
94. S. Helgason, Diﬀerential Geometry and Symmetric Spaces, Academic Press,
New York, NY, 1962.
95. N. Hogan, “Impedance control: An approach to manipulation: Part I — Theory,” ASME Journal of Dynamic Systems, Measurement, and Control , vol. 107,
pp. 1–7, 1985.
96. J.M. Hollerbach, “A recursive Lagrangian formulation of manipulator dynamics
and a comparative study of dynamics formulation complexity,” IEEE Transactions on Systems, Man, and Cybernetics, vol. 10, pp. 730–736, 1980.
97. J.M. Hollerbach, “Dynamic scaling of manipulator trajectories,” ASME Journal of Dynamic Systems, Measurement, and Control , vol. 106, pp. 102–106,
1984.
98. J.M. Hollerbach, “A survey of kinematic calibration,” in The Robotics Review
1 , O. Khatib, J.J. Craig, and T. Lozano-Pérez (Eds.), MIT Press, Cambridge,
MA, pp. 207–242, 1989.
99. J.M. Hollerbach, G. Sahar, “Wrist-partitioned inverse kinematic accelerations
and manipulator dynamics,” International Journal of Robotics Research, vol. 2,
no. 4, pp. 61–76, 1983.
100. R. Horowitz, M. Tomizuka, “An adaptive control scheme for mechanical manipulators — Compensation of nonlinearity and decoupling control,” ASME
Journal of Dynamic Systems, Measurement, and Control, vol. 108, pp. 127–
135, 1986.
101. T.C.S. Hsia, T.A. Lasky, Z. Guo, “Robust independent joint controller design
for industrial robot manipulators,” IEEE Transactions on Industrial Electronics, vol. 38, pp. 21–25, 1991.
102. P. Hsu, J. Hauser, S. Sastry, “Dynamic control of redundant manipulators,”
Journal of Robotic Systems, vol. 6, pp. 133–148, 1989.
103. S. Hutchinson, G. Hager, P. Corke, “A tutorial on visual servo control,” IEEE
Transactions on Robotics and Automation, vol. 12, pp. 651–670, 1996.
104. A. Isidori, Nonlinear Control Systems, 3rd ed., Springer-Verlag, London, UK,
1995.
105. H. Kazerooni, P.K. Houpt, T.B. Sheridan, “Robust compliant motion of manipulators, Part I: The fundamental concepts of compliant motion,” IEEE Journal
of Robotics and Automation, vol. 2, pp. 83–92, 1986.
106. J.L. Jones, A.M. Flynn, Mobile Robots: Inspiration to Implementation, AK
Peters, Wellesley, MA, 1993.
107. L.E. Kavraki, P. Svestka, J.-C. Latombe, M.H. Overmars, “Probabilistic
roadmaps for path planning in high-dimensional conﬁguration spaces,” IEEE
Transactions on Robotics and Automation, vol. 12, pp. 566–580, 1996.

References

615

108. R. Kelly, R. Carelli, O. Nasisi, B. Kuchen, F. Reyes, “Stable visual servoing of
camera-in-hand robotic systems,” IEEE/ASME Transactions on Mechatronics,
vol. 5, pp. 39–48, 2000.
109. H.K. Khalil, Nonlinear Systems, Prentice-Hall, Englewood Cliﬀs, NJ, 2002.
110. W. Khalil, F. Bennis, “Symbolic calculation of the base inertial parameters
of closed-loop robots,” International Journal of Robotics Research, vol. 14,
pp. 112–128, 1995.
111. W. Khalil, E. Dombre, Modeling, Identiﬁcation and Control of Robots, Hermes
Penton Ltd, London, 2002.
112. W. Khalil, J.F. Kleinﬁnger, “Minimum operations and minimum parameters
of the dynamic model of tree structure robots,” IEEE Journal of Robotics and
Automation, vol. 3, pp. 517–526, 1987.
113. O. Khatib, “Real-time obstacle avoidance for manipulators and mobile robots,”
International Journal of Robotics Research, vol. 5, no. 1, pp. 90–98, 1986.
114. O. Khatib, “A uniﬁed approach to motion and force control of robot manipulators: The operational space formulation,” IEEE Journal of Robotics and
Automation, vol. 3, pp. 43–53, 1987.
115. P.K. Khosla, “Categorization of parameters in the dynamic robot model,”
IEEE Transactions on Robotics and Automation, vol. 5, pp. 261–268, 1989.
116. P.K. Khosla, T. Kanade, “Parameter identiﬁcation of robot dynamics,” in Proceedings of 24th IEEE Conference on Decision and Control, Fort Lauderdale,
FL, pp. 1754–1760, 1985.
117. P.K. Khosla, T. Kanade, “Experimental evaluation of nonlinear feedback
and feedforward control schemes for manipulators,” International Journal of
Robotics Research, vol. 7, no. 1, pp. 18–28, 1988.
118. C.A. Klein, C.H. Huang, “Review of pseudoinverse control for use with kinematically redundant manipulators,” IEEE Transactions on Systems, Man, and
Cybernetics, vol. 13, pp. 245–250, 1983.
119. D.E. Koditschek, “Natural motion for robot arms,” Proc. 23th IEEE Conference on Decision and Control , Las Vegas, NV, pp. 733–735, 1984.
120. A.J. Koivo, Fundamentals for Control of Robotic Manipulators, Wiley, New
York, 1989.
121. K. Kreutz, “On manipulator control by exact linearization,” IEEE Transactions on Automatic Control, vol. 34, pp. 763–767, 1989.
122. J.-C. Latombe, Robot Motion Planning, Kluwer, Boston, MA, 1991.
123. J.-P. Laumond, (Ed.), Robot Motion Planning and Control , Springer-Verlag,
Berlin, 1998.
124. S.M. LaValle, Planning Algorithms, Cambridge University Press, New York,
2006.
125. S.M. LaValle, J.J. Kuﬀner, “Rapidly-exploring random trees: Progress and
prospects,” in New Directions in Algorithmic and Computational Robotics,
B.R. Donald, K. Lynch, D. Rus, (Eds.), AK Peters, Wellesley, MA, pp. 293–
308, 2001.
126. M.B. Leahy, G.N. Saridis, “Compensation of industrial manipulator dynamics,” International Journal of Robotics Research, vol. 8, no. 4, pp. 73–84, 1989.
127. C.S.G. Lee, “Robot kinematics, dynamics and control,” IEEE Computer ,
vol. 15, no. 12, pp. 62–80, 1982.
128. W. Leonhard, Control of Electrical Drives, Springer-Verlag, New York, 2001.

616

References

129. A. Liégeois, “Automatic supervisory control of the conﬁguration and behavior
of multibody mechanisms,” IEEE Transactions on Systems, Man, and Cybernetics, vol. 7, pp. 868–871, 1977.
130. K.Y. Lim, M. Eslami, “Robust adaptive controller designs for robot manipulator systems,” IEEE Journal of Robotics and Automation, vol. 3, pp. 54–66,
1987.
131. C.S. Lin, P.R. Chang, J.Y.S. Luh, “Formulation and optimization of cubic
polynomial joint trajectories for industrial robots,” IEEE Transactions on Automatic Control , vol. 28, pp. 1066–1073, 1983.
132. S.K. Lin, “Singularity of a nonlinear feedback control scheme for robots,” IEEE
Transactions on Systems, Man, and Cybernetics, vol. 19, pp. 134–139, 1989.
133. H. Lipkin, J. Duﬀy, “Hybrid twist and wrench control for a robotic manipulator,” ASME Journal of Mechanism, Transmissions, and Automation Design,
vol. 110, pp. 138–144, 1988.
134. V. Lippiello, B. Siciliano, L. Villani, “Position-based visual servoing in industrial multirobot cells using a hybrid camera conﬁguration,” IEEE Transactions
on Robotics and Automation, vol. 23, pp. 73–86, 2007.
135. D.A. Lizárraga, “Obstructions to the existence of universal stabilizers for
smooth control systems,” Mathematics of Control, Signals, and Systems,
vol. 16, pp. 255–277, 2004.
136. J. Lončarić, “Normal forms of stiﬀness and compliance matrices,” IEEE Journal of Robotics and Automation, vol. 3, pp. 567–572, 1987.
137. T. Lozano-Pérez, “Automatic planning of manipulator transfer movements,”
IEEE Transactions on Systems, Man, and Cybernetics, vol. 11, pp. 681–698,
1981.
138. T. Lozano-Pérez, “Spatial planning: A conﬁguration space approach,” IEEE
Transactions on Computing, vol. 32, pp. 108–120, 1983.
139. T. Lozano-Pérez, “Robot programming,” Proceedings IEEE , vol. 71, pp. 821–
841, 1983.
140. T. Lozano-Pérez, M.T. Mason, R.H. Taylor, “Automatic synthesis of ﬁnemotion strategies for robots,” International Journal of Robotics Research,
vol. 3, no. 1, pp. 3–24, 1984.
141. J.Y.S. Luh, “Conventional controller design for industrial robots: A tutorial,”
IEEE Transactions on Systems, Man, and Cybernetics, vol. 13, pp. 298–316,
1983.
142. J.Y.S. Luh, M.W. Walker, R.P.C. Paul, “On-line computational scheme for
mechanical manipulators,” ASME Journal of Dynamic Systems, Measurement,
and Control , vol. 102, pp. 69–76, 1980.
143. J.Y.S. Luh, M.W. Walker, R.P.C. Paul, “Resolved-acceleration control of mechanical manipulators,” IEEE Transactions on Automatic Control, vol. 25,
pp. 468–474, 1980.
144. J.Y.S. Luh, Y.-F. Zheng, “Computation of input generalized forces for robots
with closed kinematic chain mechanisms,” IEEE Journal of Robotics and Automation vol. 1, pp. 95–103, 1985.
145. V.J. Lumelsky, Sensing, Intelligence, Motion: How Robots and Humans Move
in an Unstructured World , Wiley, Hoboken, NJ, 2006.
146. Y. Ma, S. Soatto, J. Kosecka, S. Sastry, An Invitation to 3-D Vision: From
Images to Geometric Models, Springer, New York, 2003.

References

617

147. A.A. Maciejewski, C.A. Klein, “Obstacle avoidance for kinematically redundant manipulators in dynamically varying environments,” International Journal of Robotics Research, vol. 4, no. 3, pp. 109–117, 1985.
148. E. Malis, F. Chaumette, S. Boudet, “2-1/2D visual servoing,” IEEE Transactions on Robotics and Automation, vol. 15, pp. 238–250, 1999.
149. B.R. Markiewicz, Analysis of the Computed Torque Drive Method and Comparison with Conventional Position Servo for a Computer-Controlled Manipulator , memo. TM 33-601, JPL, Pasadena, CA, 1973.
150. M.T. Mason, “Compliance and force control for computer controlled manipulators,” IEEE Transactions on Systems, Man, and Cybernetics, vol. 6, pp. 418–
432, 1981.
151. J.M. McCarthy, An Introduction to Theoretical Kinematics, MIT Press, Cambridge, MA, 1990.
152. N.H. McClamroch, D. Wang, “Feedback stabilization and tracking of constrained robots,” IEEE Transactions on Automatic Control, vol. 33, pp. 419–
426, 1988.
153. R.T. M’Closkey, R.M. Murray, “Exponential stabilization of driftless nonlinear
control systems using homogeneous feedback,” IEEE Transactions on Automatic Control , vol. 42, pp. 614–628, 1997.
154. L. Meirovitch, Dynamics and Control of Structures, Wiley, New York, 1990.
155. C. Melchiorri, Traiettorie per Azionamenti Elettrici, Progetto Leonardo,
Bologna, I, 2000.
156. N. Manring, Hydraulic Control Systems, Wiley, New York, 2005.
157. R. Middleton, G.C. Goodwin, “Adaptive computed torque control for rigid link
manipulators,” Systems & Control Letters, vol. 10, pp. 9–16, 1988.
158. R.R. Murphy, Introduction to AI Robotics, MIT Press, Cambridge, MA, 2000.
159. R.M. Murray, Z. Li, S.S. Sastry, A Mathematical Introduction to Robotic Manipulation, CRC Press, Boca Raton, CA. 1994.
160. Y. Nakamura, Advanced Robotics: Redundancy and Optimization, AddisonWesley, Reading, MA, 1991.
161. Y. Nakamura, H. Hanafusa, “Inverse kinematic solutions with singularity robustness for robot manipulator control,” ASME Journal of Dynamic Systems,
Measurement, and Control , vol. 108, pp. 163–171, 1986.
162. Y. Nakamura, H. Hanafusa, “Optimal redundancy control of robot manipulators,” International Journal of Robotics Research, vol. 6, no. 1, pp. 32–42,
1987.
163. Y. Nakamura, H. Hanafusa, T. Yoshikawa, “Task-priority based redundancy
control of robot manipulators,” International Journal of Robotics Research,
vol. 6, no. 2, pp. 3–15, 1987.
164. J.I. Neimark, F.A. Fufaev, Dynamics of Nonholonomic Systems, American
Mathematical Society, Providence, RI, 1972.
165. I. Nevins, D.E. Whitney, “The force vector assembler concept,” Proc. First
CISM-IFToMM Symposium on Theory and Practice of Robots and Manipulators, Udine, I, 1973.
166. F. Nicolò, J. Katende, “A robust MRAC for industrial robots,” Proc. 2nd
IASTED International Symposium on Robotics and Automation, pp. 162–171,
Lugano, Switzerland, 1983.
167. S. Nicosia, P. Tomei, “Model reference adaptive control algorithms for industrial robots,” Automatica, vol. 20, pp. 635–644, 1984.

618

References

168. H. Nijmeijer, A. van de Schaft, Nonlinear Dynamical Control Systems,
Springer-Verlag, Berlin, Germany, 1990.
169. B. Noble, Applied Linear Algebra, 3rd ed., Prentice-Hall, Englewood Cliﬀs, NJ,
1987.
170. C. O’Dúnlaing, C.K. Yap, “A retraction method for planning the motion of a
disc,” Journal of Algorithms, vol. 6, pp. 104–111, 1982.
171. K. Ogata, Modern Control Engineering, 4th ed., Prentice-Hall, Englewood
Cliﬀs, NJ, 2002.
172. D.E. Orin, R.B. McGhee, M. Vukobratović, G. Hartoch, “Kinematic and kinetic analysis of open-chain linkages utilizing Newton–Euler methods,” Mathematical Biosciences vol. 43, pp. 107–130, 1979.
173. D.E. Orin, W.W. Schrader, “Eﬃcient computation of the Jacobian for robot
manipulators,” International Journal of Robotics Research, vol. 3, no. 4,
pp. 66–75, 1984.
174. G. Oriolo, A. De Luca, M. Vendittelli, “WMR control via dynamic feedback linearization: Design, implementation and experimental validation,” IEEE Transactions on Control Systems Technology, vol. 10, pp. 835–852, 2002.
175. R. Ortega, M.W. Spong, “Adaptive motion control of rigid robots: A tutorial,”
Automatica, vol. 25, pp. 877–888, 1989.
176. T. Patterson, H. Lipkin, “Duality of constrained elastic manipulation,” Proc.
1991 IEEE International Conference on Robotics and Automation, pp. 2820–
2825, Sacramento, CA, 1991.
177. T. Patterson, H. Lipkin, “Structure of robot compliance,” ASME Journal of
Mechanical Design, vol. 115, pp. 576–580, 1993.
178. R.P. Paul, Modelling, Trajectory Calculation, and Servoing of a Computer Controlled Arm, memo. AIM 177, Stanford Artiﬁcial Intelligence Laboratory, 1972.
179. R.P. Paul, “Manipulator Cartesian path control,” IEEE Transactions on Systems, Man, and Cybernetics, vol. 9, pp. 702–711, 1979.
180. R.P. Paul, Robot Manipulators: Mathematics, Programming, and Control, MIT
Press, Cambridge, MA, 1981.
181. R.P. Paul, B.E. Shimano, G. Mayer, “Kinematic control equations for simple
manipulators,” IEEE Transactions on Systems, Man, and Cybernetics, vol. 11,
pp. 449–455, 1981.
182. R.P. Paul, H. Zhang, “Computationally eﬃcient kinematics for manipulators
with spherical wrists based on the homogeneous transformation representation,” International Journal of Robotics Research, vol. 5, no. 2, pp. 32–44,
1986.
183. D.L. Pieper, The Kinematics of Manipulators Under Computer Control
memo. AIM 72, Stanford Artiﬁcial Intelligence Laboratory, 1968.
184. M.H. Raibert, J.J. Craig, “Hybrid position/force control of manipulators,”
ASME Journal of Dynamic Systems, Measurement, and Control , vol. 103,
pp. 126–133, 1981.
185. E. Rimon, D.E. Koditschek, “The construction of analytic diﬀeomorphisms
for exact robot navigation on star worlds,” Proc. 1989 IEEE International
Conference on Robotics and Automation, Scottsdale, AZ, pp. 21–26, 1989.
186. E.I. Rivin, Mechanical Design of Robots, McGraw-Hill, New York, 1987.
187. R.E. Roberson, R. Schwertassek, Dynamics of Multibody Systems, SpringerVerlag, Berlin, Germany, 1988.
188. Z. Roth, B.W. Mooring, B. Ravani, “An overview of robot calibration,” IEEE
Journal of Robotics and Automation, vol. 3, pp. 377–386, 1987.

References

619

189. S. Russell, P. Norvig, Artiﬁcial Intelligence: A Modern Approach, 2nd ed.,
Prentice Hall, Englewood Cliﬀs, NJ, 2003.
190. J.K. Salisbury, “Active stiﬀness control of a manipulator in Cartesian coordinates,” Proc. 19th IEEE Conference on Decision and Control , pp. 95–100,
Albuquerque, NM, 1980.
191. J.K. Salisbury, J.J. Craig, “Articulated hands: Force control and kinematic
issues,” International Journal of Robotics Research, vol. 1, no. 1, pp. 4–17,
1982.
192. C. Samson, “Robust control of a class of nonlinear systems and applications
to robotics,” International Journal of Adaptive Control and Signal Processing,
vol. 1, pp. 49–68, 1987.
193. C. Samson, “Time-varying feedback stabilization of car-like wheeled mobile
robots,” International Journal of Robotics Research, vol. 12, no. 1, pp. 55–64,
1993.
194. C. Samson, M. Le Borgne, B. Espiau, Robot Control: The Task Function Approach, Clarendon Press, Oxford, UK, 1991.
195. S. Sastry, Nonlinear Systems: Analysis, Stability and Control , Springer-Verlag,
Berlin, Germany, 1999.
196. V.D. Scheinman, Design of a Computer Controlled Manipulator ,
memo. AIM 92, Stanford Artiﬁcial Intelligence Laboratory, 1969.
197. J.T. Schwartz, M. Sharir, “On the ‘piano movers’ problem: II. General techniques for computing topological properties of real algebraic manifolds,” Advances in Applied Mathematics, vol. 4, pp. 298–351, 1983.
198. L. Sciavicco, B. Siciliano, “Coordinate transformation: A solution algorithm for
one class of robots,” IEEE Transactions on Systems, Man, and Cybernetics,
vol. 16, pp. 550–559, 1986.
199. L. Sciavicco, B. Siciliano, “A solution algorithm to the inverse kinematic problem for redundant manipulators,” IEEE Journal of Robotics and Automation,
vol. 4, pp. 403–410, 1988.
200. L. Sciavicco, B. Siciliano, Modelling and Control of Robot Manipulators, 2nd
ed., Springer, London, UK, 2000.
201. L. Sciavicco, B. Siciliano, L. Villani, “Lagrange and Newton–Euler dynamic
modeling of a gear-driven rigid robot manipulator with inclusion of motor
inertia eﬀects,” Advanced Robotics, vol. 10, pp. 317–334, 1996.
202. R. Sedgewick, Algorithms, 2nd ed., Addison-Wesley, Reading, MA, 1988.
203. H. Seraji, “Conﬁguration control of redundant manipulators: Theory and
implementation,” IEEE Transactions on Robotics and Automation, vol. 5,
pp. 472–490, 1989.
204. S.W. Shepperd, S.W., “Quaternion from rotation matrix,” AIAA Journal of
Guidance and Control , vol. 1, pp. 223–224, 1978.
205. R. Shoureshi, M.E. Momot, M.D. Roesler, “Robust control for manipulators
with uncertain dynamics,” Automatica, vol. 26, pp. 353–359, 1990.
206. B. Siciliano, “Kinematic control of redundant robot manipulators: A tutorial,”
Journal of Intelligent and Robotic Systems, vol. 3, pp. 201–212, 1990.
207. B. Siciliano, “A closed-loop inverse kinematic scheme for on-line joint based
robot control,” Robotica, vol. 8, pp. 231–243, 1990.
208. B. Siciliano, J.-J.E. Slotine, “A general framework for managing multiple tasks
in highly redundant robotic systems,” Proc. 5th International Conference on
Advanced Robotics, Pisa, I, pp. 1211–1216, 1991.

620

References

209. B. Siciliano, L. Villani, Robot Force Control , Kluwer, Boston, MA, 2000.
210. R. Siegwart, I.R. Nourbakhsh, Introduction to Autonomous Mobile Robots,
MIT Press, Cambridge, MA, 2004.
211. D.B. Silver, “On the equivalence of Lagrangian and Newton–Euler dynamics
for manipulators,” International Journal of Robotics Research, vol. 1, no. 2,
pp. 60–70, 1982.
212. J.-J.E. Slotine, “The robust control of robot manipulators,” International
Jorunal of Robotics Research, vol. 4, no. 2, pp. 49–64, 1985.
213. J.-J.E. Slotine, “Putting physics in control — The example of robotics,” IEEE
Control Systems Magazine, vol. 8, no. 6, pp. 12–18, 1988.
214. J.-J.E. Slotine, W. Li, “On the adaptive control of robot manipulators,” International Journal of Robotics Research, vol. 6, no. 3, pp. 49–59, 1987.
215. J.-J.E. Slotine, W. Li, Applied Nonlinear Control , Prentice-Hall, Englewood
Cliﬀs, NJ, 1991.
216. M.W. Spong, “On the robust control of robot manipulators,” IEEE Transactions on Automatic Control, vol. 37, pp. 1782–1786, 1992.
217. M.W. Spong, S. Hutchinson, M. Vidyasagar, Robot Modeling and Control , Wiley, New York, 2006.
218. M.W. Spong, R. Ortega, R. Kelly, “Comments on “Adaptive manipulator
control: A case study”,” IEEE Transactions on Automatic Control, vol. 35,
pp. 761–762, 1990.
219. M.W. Spong, M. Vidyasagar, “Robust linear compensator design for nonlinear
robotic control,” IEEE Journal of Robotics and Automation, vol. 3, pp. 345–
351, 1987.
220. SRI International, Robot Design Handbook , G.B. Andeen, (Ed.), McGraw-Hill,
New York, 1988.
221. Y. Stepanenko, M. Vukobratović, “Dynamics of articulated open-chain active
mechanisms,” Mathematical Biosciences, vol. 28, pp. 137–170, 1976.
222. Y. Stepanenko, J. Yuan, “Robust adaptive control of a class of nonlinear mechanical systems with unbounded and fast varying uncertainties,” Automatica,
vol. 28, pp. 265–276, 1992.
223. S. Stramigioli, Modeling and IPC Control of Interactive Mechanical Systems
— A Coordinate Free Approach, Springer, London, UK, 2001.
224. K.R. Symon, Mechanics, 3rd ed., Addison-Wesley, Reading, MA, 1971.
225. K. Takase, R. Paul, E. Berg, “A structured approach to robot programming
and teaching,” IEEE Transactions on Systems, Man, and Cybernetics, vol. 11,
pp. 274–289, 1981.
226. M. Takegaki, S. Arimoto, “A new feedback method for dynamic control of manipulators,” ASME Journal of Dynamic Systems, Measurement, and Control ,
vol. 102, pp. 119–125, 1981.
227. T.-J. Tarn, A.K. Bejczy, X. Yun, Z. Li, “Eﬀect of motor dynamics on nonlinear
feedback robot arm control,” IEEE Transactions on Robotics and Automation,
vol. 7, pp. 114–122, 1991.
228. T.-J. Tarn, Y. Wu, N. Xi, A. Isidori, “Force regulation and contact transition
control,” IEEE Control Systems Magazine, vol. 16, no. 1, pp. 32–40, 1996.
229. R.H. Taylor, “Planning and execution of straight line manipulator trajectories,” IBM Journal of Research and Development, vol. 23, pp. 424–436, 1979.
230. R.H. Taylor, D.D. Grossman, “An integrated robot system architecture,” Proceedings IEEE , vol. 71, pp. 842–856, 1983.

References

621

231. S. Thrun, W. Burgard, D. Fox, Probabilistic Robotics, MIT Press, Cambridge,
MA, 2005.
232. L.W. Tsai, A.P. Morgan, “Solving the kinematics of the most general six- and
ﬁve-degree-of-freedom manipulators by continuation methods,” ASME Journal
of Mechanisms, Transmission, and Automation in Design, vol. 107, pp. 189–
200, 1985.
233. R. Tsai, “A versatile camera calibration technique for high accuracy 3-D machine vision metrology using oﬀ-the-shelf TV cameras and lenses,” IEEE Transactions on Robotics and Automation, vol. 3, pp. 323–344, 1987.
234. J.J.Uicker, “Dynamic force analysis of spatial linkages,” ASME Journal of Applied Mechanics, vol. 34, pp. 418–424, 1967.
235. L. Villani, C. Canudas de Wit, B. Brogliato, “An exponentially stable adaptive
control for force and position tracking of robot manipulators,” IEEE Transactions on Automatic Control, vol. 44, pp. 798–802, 1999.
236. M. Vukobratović, “Dynamics of active articulated mechanisms and synthesis
of artiﬁcial motion,” Mechanism and Machine Theory, vol. 13, pp. 1–56, 1978.
237. M.W. Walker, D.E. Orin, “Eﬃcient dynamic computer simulation of robotic
mechanisms,” ASME Journal of Dynamic Systems, Measurement, and Control ,
vol. 104, pp. 205–211, 1982.
238. C.W. Wampler, “Manipulator inverse kinematic solutions based on damped
least-squares solutions,” IEEE Transactions on Systems, Man, and Cybernetics, vol. 16, pp. 93–101, 1986.
239. L. Weiss, A. Sanderson, C. Neuman, “Dynamic sensor-based control of robots
with visual feedback,” IEEE Journal of Robotics and Automation, vol. 3,
pp. 404–417, 1987.
240. D.E. Whitney, “Resolved motion rate control of manipulators and human prostheses,” IEEE Transactions on Man-Machine Systems, vol. 10, pp. 47–53, 1969.
241. D.E. Whitney, “Force feedback control of manipulator ﬁne motions,” ASME
Journal of Dynamic Systems, Measurement, and Control, vol. 99, pp. 91–97,
1977.
242. D.E. Whitney, “Quasi-static assembly of compliantly supported rigid parts,”
ASME Journal of Dynamic Systems, Measurement, and Control , vol. 104,
pp. 65–77, 1982.
243. D.E. Whitney, “Historical perspective and state of the art in robot force control,” International Journal of Robotics Research, vol. 6, no. 1, pp. 3–14, 1987.
244. W. Wilson, C. Hulls, G. Bell, “Relative end-eﬀector control using Cartesian position based visual servoing,” IEEE Transactions on Robotics and Automation,
vol. 12, pp. 684–696, 1996.
245. T. Yoshikawa, “Manipulability of robotic mechanisms,” International Journal
of Robotics Research, vol. 4, no. 2, pp. 3–9, 1985.
246. T. Yoshikawa, “Dynamic manipulability ellipsoid of robot manipulators,” Journal of Robotic Systems, vol. 2, pp. 113–124, 1985.
247. T. Yoshikawa, “Dynamic hybrid position/force control of robot manipulators
— Description of hand constraints and calculation of joint driving force,” IEEE
Journal of Robotics and Automation, vol. 3, pp. 386–392, 1987.
248. T. Yoshikawa, Foundations of Robotics, MIT Press, Boston, MA, 1990.
249. T. Yoshikawa, T. Sugie, N. Tanaka, “Dynamic hybrid position/force control
of robot manipulators — Controller design and experiment,” IEEE Journal of
Robotics and Automation, vol. 4, pp. 699–705, 1988.

622

References

250. J.S.-C. Yuan, “Closed-loop manipulator control using quaternion feedback,”
IEEE Journal of Robotics and Automation, vol. 4, pp. 434–440, 1988.

Index

acceleration
feedback, 317
gravity, 255, 583
joint, 141, 256
link, 285
accessibility
loss, 471, 476
rank condition, 477, 603
accuracy, 87
actuator, 3, 191
algorithm
A , 607
best-ﬁrst, 552
complete, 535
complexity, 605
inverse kinematics, 132, 143
pose estimation, 427
probabilistically complete, 543
randomized best-ﬁrst, 553
resolution complete, 540
search, 606
steepest descent, 551
sweep line, 536
sweep plane, 539
wavefront expansion, 554
angle
and axis, 52, 139, 187
Euler, 48
architecture
control, 233, 237
functional, 233
hardware, 242
arm

anthropomorphic, 73, 96, 114
anthropomorphic with spherical
wrist, 77
parallelogram, 70
singularity, 119
spherical, 72, 95
three-link planar, 69, 91, 113
automation
ﬂexible, 17
industrial, 24
programmable, 16
rigid, 16
axis
and angle, 52
central, 582
joint, 62
principal, 582
Barbalat
lemma, 507, 512, 513, 598
bicycle
chained-form transformation, 485
ﬂat outputs, 491
front-wheel drive, 481
rear-wheel drive, 481
calibration
camera, 229, 440
kinematic, 88
matrix, 217
camera
calibration, 440
eye-in-hand, 409
eye-to-hand, 409

624

Index

ﬁxed conﬁguration, 409
hybrid conﬁguration, 409
mobile conﬁguration, 409
pan-tilt, 410
cell decomposition
approximate, 539
exact, 536
chained form, 482
ﬂat outputs, 492
transformation, 483
Christoﬀel
symbols, 258
collision checking, 532
compensation
decentralized feedforward, 319
feedforward, 593
feedforward computed torque, 324
gravity, 328, 345, 368, 446, 449
compliance
active, 367
control, 364, 367
matrix, 366
passive, 366
conﬁguration, 470, 525, 585
conﬁguration space
2R manipulator, 526
as a manifold, 527
distance, 527
free, 528
free path, 528
obstacles, 527
connectivity graph, 536, 537
constraint
artiﬁcial, 391
bilateral, 386, 585
epipolar, 434
frame, 391
holonomic, 385, 470, 585
Jacobian, 385
kinematic, 471
natural, 391
nonholonomic, 469, 585
Pfaﬃan, 471
pure rolling, 472
scleronomic, 585
unilateral, 386
control
adaptive, 338
admittance, 377

architecture, 233, 237
centralized, 327
comparison among schemes, 349, 453
compliance, 364, 367
decentralized, 309
force, 378
force with inner position loop, 379
force with inner velocity loop, 380
hybrid force/motion, 396
hybrid force/position, 403
hybrid force/velocity, 398, 402
impedance, 372
independent joint, 311
interaction, 363
inverse dynamics, 330, 347, 372, 487,
594
inverse model, 594
Jacobian inverse, 344
Jacobian transpose, 345
joint space, 305
kinematic, 134
linear systems, 589
motion, 303
operational space, 343, 364
parallel force/position, 381
PD with gravity compensation, 328,
345, 368
PI, 311, 322, 380, 591
PID, 322, 591
PIDD2 , 322
points, 555
position, 206, 312, 314, 317
resolved-velocity, 448
robust, 333
system, 3
unit vector, 337
velocity, 134, 314, 317, 502
vision-based, 408
voltage, 199
controllability
and nonholonomy, 477
condition, 477
system, 603
coordinate
generalized, 247, 296, 585
homogeneous, 56, 418
Lagrange, 585
transformation, 56

Index
degree
nonholonomy, 603
of freedom, 4, 585
Denavit–Hartenberg
convention, 61
parameters, 63, 69, 71, 72, 74, 75, 78,
79
diﬀerential ﬂatness, 491
displacement
elementary, 366, 368, 581, 586
virtual, 385, 586
distribution
accessibility, 603
dimension, 602
involutive, 602
involutive closure, 602
disturbance
compensation, 325
rejection, 207, 376, 590
drive
electric, 198
hydraulic, 202
with gear, 204
dynamic extension, 487
dynamic model
constrained mechanical system, 486
joint space, 257
linearity in the parameters, 259
notable properties, 257
operational space, 296
parallelogram arm, 277
parameter identiﬁcation, 280
reduced order, 402
skew-symmetry of matrix Ḃ − 2C,
257
two-link Cartesian arm, 264
two-link planar arm, 265
dynamics
direct, 298
fundamental principles, 584
inverse, 298, 330, 347
encoder
absolute, 210
incremental, 212, 517
end-eﬀector
force, 147
frame, 59
orientation, 187

625

pose, 58, 184
position, 184
energy
conservation, 588
conservation principle, 259
kinetic, 249
potential, 255, 585
environment
compliant, 389, 397
interaction, 363
programming, 238
rigid, 385, 401
structured, 15
unstructured, 25
epipolar
geometry, 433
line, 435
error
estimation, 430
force, 378
joint space, 328
operational space, 132, 345, 367, 445
orientation, 137
position, 137
tracking, 324
estimation
pose, 427
Euler
angles, 48, 137, 187
feedback
nonlinear, 594
position, 312
position and velocity, 314
position, velocity and acceleration,
317
ﬂat outputs, 491
force
active, 583, 586
centrifugal, 256
conservative, 585, 587
contact, 364
control, 378
controlled subspace, 387
Coriolis, 257
elementary work, 584
end-eﬀector, 147
error, 378
external, 583, 584

626

Index

generalized, 248, 587
gravity, 255, 583
internal, 583
nonconservative, 587
reaction, 385, 583, 586
resultant, 583
transformation, 151
form
bilinear, 574
negative deﬁnite, 574
positive deﬁnite, 574
quadratic, 574, 597
frame
attached, 40
base, 59
central, 582
compliant, 377
constraint, 391
current, 46
ﬁxed, 46, 579
moving, 579
principal, 582
rotation, 40
friction
Coulomb, 257
electric, 200
viscous, 257
Frobenius
norm, 421
theorem, 476
function
gradient, 569
Hamiltonian, 588
Lagrangian, 588
Lyapunov, 596
gear
reduction ratio, 205, 306
generator
torque-controlled, 200, 309
velocity-controlled, 200, 309
graph search, 606
A , 607
breadth-ﬁrst, 606
depth-ﬁrst, 606
gravity
acceleration, 255, 583
compensation, 328, 345, 368, 446, 449
force, 255, 583

Hamilton
principle of conservation of energy,
259
homography
planar, 420, 438
identiﬁcation
dynamic parameters, 280
kinematic parameters, 88
image
binary, 412
centroid, 416
feature parameters, 410
interpretation, 416
Jacobian, 424
moment, 416
processing, 410
segmentation, 411
impedance
active, 373
control, 372
mechanical, 373
passive, 374
inertia
ﬁrst moment, 262
matrix, 254
moment, 262, 581
product, 582
tensor, 251, 582
integrability
multiple kinematic constraints, 475,
477
single kinematic constraint, 473
interaction
control, 363
environment, 363
matrix, 424
inverse kinematics
algorithm, 132
anthropomorphic arm, 96
comparison among algorithms, 143
manipulator with spherical wrist, 94
second-order algorithm, 141
spherical arm, 95
spherical wrist, 99
three-link planar arm, 91
Jacobian
analytical, 128

Index
anthropomorphic arm, 114
computation, 111
constraint, 385
damped least-squares, 127
geometric, 105
image, 424
inverse, 133, 344
pseudo-inverse, 133
Stanford manipulator, 115
three-link planar arm, 113
transpose, 134, 345
joint
acceleration, 141, 256
actuating system, 191
axis, 62
prismatic, 4
revolute, 4
space, 84
torque, 147, 248
variable, 58, 248
kinematic chain
closed, 4, 65, 151
open, 4, 60
kinematics
anthropomorphic arm, 73
anthropomorphic arm with spherical
wrist, 77
diﬀerential, 105
direct, 58
DLR manipulator, 79
humanoid manipulator, 81
inverse, 90
inverse diﬀerential, 123
parallelogram arm, 70
spherical arm, 72, 95
spherical wrist, 75
Stanford manipulator, 76
three-link planar arm, 69
kineto-statics duality, 148
La Salle
theorem, 507, 597
Lagrange
coordinates, 585
equations, 587
formulation, 247, 292
function, 588
multipliers, 124, 485

627

level
action, 235
gray, 410
hierarchical, 234
primitive, 236
servo, 236
task, 235
Lie
bracket, 600
derivative, 601
link
acceleration, 285
centre of mass, 249
inertia, 251
velocity, 108
local
minima, 550, 551
planner, 542
Lyapunov
direct method, 596
equation, 597
function, 135, 328, 335, 340, 341, 345,
368, 431, 446, 449, 452, 506, 513,
596
manipulability
dynamic, 299
ellipsoid, 152
measure, 126, 153
manipulability ellipsoid
dynamic, 299
force, 156
velocity, 153
manipulator
anthropomorphic, 8
Cartesian, 4
cylindrical, 5
DLR, 79
end-eﬀector, 4
humanoid, 81
joint, 58
joints, 4
link, 58
links, 4
mechanical structure, 4
mobile, 14
parallel, 9
posture, 58
redundant, 4, 87, 124, 134, 142, 296

628

Index

SCARA, 7
spherical, 6
Stanford, 76, 115
with spherical wrist, 94
wrist, 4
matrix
adjoint, 567
algebraic complement, 565
block-partitioned, 564
calibration, 217, 229
compliance, 366
condition number, 577
damped least-squares, 127
damped least-squares inverse, 282
derivative, 568
determinant, 566
diagonal, 564
eigenvalues, 573
eigenvectors, 573
essential, 434
homogeneous transformation, 56
idempotent, 568
identity, 564
inertia, 254
interaction, 424
inverse, 567
Jacobian, 569
left pseudo-inverse, 90, 281, 386, 428,
431, 452, 576
minor, 566
negative deﬁnite, 574
negative semi-deﬁnite, 575
norm, 572
null, 564
operations, 565
orthogonal, 568, 579
positive deﬁnite, 255, 574, 582
positive semi-deﬁnite, 575
product, 566
product of scalar by, 565
projection, 389, 572
right pseudo-inverse, 125, 299, 576
rotation, 40, 579
selection, 389
singular value decomposition, 577
skew-symmetric, 257, 564
square, 563
stiﬀness, 366
sum, 565

symmetric, 251, 255, 564
trace, 565
transpose, 564
triangular, 563
mobile robot
car-like, 13, 482
control, 502
diﬀerential drive, 12, 479
dynamic model, 486
kinematic model, 476
legged, 11
mechanical structure, 10
omnidirectional, 13
path planning, 492
planning, 489
second-order kinematic model, 488
synchro drive, 12, 479
trajectory planning, 498
tricycle-like, 12, 482
wheeled, 10, 469
moment
image, 416
inertia, 262, 581
inertia ﬁrst, 262
resultant, 583
motion
constrained, 363, 384
control, 303
equations, 255
internal, 296
planning, 523
point-to-point, 163
primitives, 545
through a sequence of points, 168
motion planning
canonical problem, 523
multiple-query, 535
oﬀ-line, 524
on-line, 524
probabilistic, 541
query, 535
reactive, 551
sampling-based, 541
single-query, 543
via artiﬁcial potentials, 546
via cell decomposition, 536
via retraction, 532
motor
electric, 193

Index
hydraulic, 193
pneumatic, 193
navigation function, 553
Newton–Euler
equations, 584
formulation, 282, 292
recursive algorithm, 286
nonholonomy, 469
octree, 541
odometric localization, 514
operational
space, 84, 445
operator
Laplacian, 415
Roberts, 414
Sobel, 414
orientation
absolute, 436
end-eﬀector, 187
error, 137
minimal representation, 49
rigid body, 40
trajectory, 187
parameters
Denavit–Hartenberg, 63
dynamic, 259
extrinsic, 229, 440
intrinsic, 229, 440
uncertainty, 332, 444
path
circular, 183
geometrically admissible, 490
minimum, 607
primitive, 181
rectilinear, 182
plane
epipolar, 435
osculating, 181
points
feature, 417
path, 169
via, 186, 539
virtual, 173
polynomial
cubic, 164, 169
interpolating, 169

sequence, 170, 172, 175
Pontryagin
minimum principle, 499
pose
estimation, 418
regulation, 345
rigid body, 39
position
control, 206, 312
end-eﬀector, 184
feedback, 312, 314, 317
rigid body, 39
trajectory, 184
transducer, 210
posture
manipulator, 58
regulation, 328, 503, 512
potential
artiﬁcial, 546
attractive, 546
repulsive, 547
total, 549
power
ampliﬁer, 197
supply, 198
principle
conservation of energy, 259
virtual work, 147, 385, 587
PRM (Probabilistic Roadmap), 541
programming
environment, 238
language, 238
object-oriented, 242
robot-oriented, 241
teaching-by-showing, 240
quadtree, 540
range
sensor, 219
reciprocity, 387
redundancy
kinematic, 121
analysis, 121
kinematic, 87
resolution, 123, 298
Reeds–Shepp
curves, 501
regulation

629

630

Index

Cartesian, 511
discontinuous and/or time-varying,
514
pose, 345
posture, 328, 503, 512
Remote Centre of Compliance (RCC),
366
resolver, 213
retraction, 534
rigid body
angular momentum, 583
angular velocity, 580
inertia moment, 581
inertia product, 582
inertia tensor, 582
kinematics, 579
linear momentum, 583
mass, 581
orientation, 40
pose, 39, 580
position, 39
potential energy, 585
roadmap, 532
robot
applications, 18
ﬁeld, 26
industrial, 17
manipulator, 4
mobile, 10
origin, 1
service, 27
robotics
advanced, 25
deﬁnition, 2
fundamental laws, 2
industrial, 15
rotation
elementary, 41
instantaneous centre, 480
matrix, 40, 579
vector, 44
rotation matrix
composition, 45
derivative, 106
RRT (Rapidly-exploring Random Tree),
543
segmentation
binary, 412

image, 411
sensor
exteroceptive, 3, 215, 517
laser, 222
proprioceptive, 3, 209, 516
range, 219
shaft torque, 216
sonar, 219
vision, 225
wrist force, 216
servomotor
brushless DC, 194
electric, 193
hydraulic, 195
permanent-magnet DC, 194
simulation
force control, 382
hybrid visual servoing, 464
impedance control, 376
inverse dynamics, 269
inverse kinematics algorithms, 143
motion control schemes, 349
pose estimation, 432
regulation for mobile robots, 514
trajectory tracking for mobile robots,
508
visual control schemes, 453
visual servoing, 453
singularity
arm, 119
classiﬁcation, 116
decoupling, 117
kinematic, 116, 127
representation, 130
wrist, 119
space
conﬁguration, 470
joint, 83, 84, 162
null, 122, 149
operational, 83, 84, 296, 343
projection, 572
range, 122, 149, 572
vector, 570
work, 85
special group
Euclidean, 57, 580
orthonormal, 41, 49, 579
stability, 133, 135, 141, 328, 368, 446,
447, 452, 590, 595, 596

Index
statics, 147, 587
Steiner
theorem, 260, 582
stiﬀness
matrix, 366
tachometer, 214
torque
actuating, 257
computed, 324
controlled generator, 200
driving, 199, 203
friction, 257
joint, 147, 248
limit, 294
reaction, 199
sensor, 216
tracking
error, 504
reference, 590
trajectory, 503, 595
via input/output linearization, 507
via linear control, 505
via nonlinear control, 506
trajectory
dynamic scaling, 294
joint space, 162
operational space, 179
orientation, 187
planning, 161, 179
position, 184
tracking, 503
transducer
position, 210
velocity, 214
transformation
coordinate, 56
force, 151
homogeneous, 56
linear, 572
matrix, 56
perspective, 227
similarity, 573
velocity, 149
transmission, 192
triangulation, 435
unicycle
chained-form transformation, 484

631

dynamic model, 488
ﬂat outputs, 491
kinematic model, 478
minimum-time trajectories, 500
optimal trajectories, 499
second-order kinematic model, 489
unit quaternion, 54, 140
unit vector
approach, 59
binormal, 181
control, 337
normal, 59, 181
sliding, 59
tangent, 181
vector
basis, 570
bound, 580
column, 563
components, 570
feature, 418
ﬁeld, 599
homogeneous representation, 56
linear independence, 569
norm, 570
null, 564
operations, 569
product, 571
product of scalar by, 570
representation, 42
rotation, 44
scalar product, 570
scalar triple product, 571
space, 570
subspace, 570
sum, 570
unit, 571
velocity
controlled generator, 200
controlled subspace, 387
feedback, 314, 317
link, 108
transducer, 214
transformation, 149
trapezoidal proﬁle, 165
triangular proﬁle, 167
vision
sensor, 225
stereo, 409, 433

632

Index

visual servoing
hybrid, 460
image-based, 449
PD with gravity compensation, 446,
449
position-based, 445
resolved-velocity, 447, 451
Voronoi
generalized diagram, 533
wheel
caster, 11

ﬁxed, 11
Mecanum, 13
steerable, 11
work
elementary, 584
virtual, 147, 385, 586
workspace, 4, 14
wrist
force sensor, 216
singularity, 119
spherical, 75, 99

